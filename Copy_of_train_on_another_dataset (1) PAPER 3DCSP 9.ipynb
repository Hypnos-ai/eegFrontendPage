{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpOrblegVkBG",
        "outputId": "6bfba428-25f7-42bf-ed8a-edae2a203c48"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install mne\n",
        "!pip install gym\n",
        "!pip install markupsafe==1.1.1\n",
        "import tensorflow as tf\n",
        "print('TF VERSION',tf.__version__)\n",
        "\n",
        "# !pip uninstall tensorflow\n",
        "# !pip uninstall tensorflow-gpu\n",
        "# !pip install tensorflow==2.15.0\n",
        "# !pip install tensorflow-gpu==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI4DY3U_SpAD",
        "outputId": "8ef3baf3-e7cc-4d58-cb22-a0392124863e"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import mne\n",
        "import os\n",
        "import gym\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "class NDStandardScaler(TransformerMixin):\n",
        "    def __init__(self, **kwargs):\n",
        "        self._scaler = StandardScaler(copy=True, **kwargs)\n",
        "        self._orig_shape = None\n",
        "\n",
        "    def fit(self, X, **kwargs):\n",
        "        X = np.array(X)\n",
        "        # Save the original shape to reshape the flattened X later\n",
        "        # back to its original shape\n",
        "        if len(X.shape) > 1:\n",
        "            self._orig_shape = X.shape[1:]\n",
        "        X = self._flatten(X)\n",
        "        self._scaler.fit(X, **kwargs)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, **kwargs):\n",
        "        X = np.array(X)\n",
        "        X = self._flatten(X)\n",
        "        X = self._scaler.transform(X, **kwargs)\n",
        "        X = self._reshape(X)\n",
        "        return X\n",
        "\n",
        "    def _flatten(self, X):\n",
        "        # Reshape X to <= 2 dimensions\n",
        "        if len(X.shape) > 2:\n",
        "            n_dims = np.prod(self._orig_shape)\n",
        "            X = X.reshape(-1, n_dims)\n",
        "        return X\n",
        "\n",
        "    def _reshape(self, X):\n",
        "        # Reshape X back to it's original shape\n",
        "        if len(X.shape) >= 2:\n",
        "            X = X.reshape(-1, *self._orig_shape)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pywt\n",
        "import scipy.signal\n",
        "from scipy import stats\n",
        "def mean(x):\n",
        "    return np.mean(x, axis=-1).reshape(-1, 1)\n",
        "\n",
        "def stddev(x):\n",
        "    return np.std(x, axis=-1).reshape(-1, 1)\n",
        "\n",
        "def peaktopeak(x):\n",
        "    return np.ptp(x, axis=-1).reshape(-1, 1)\n",
        "\n",
        "def variance(x):\n",
        "    return np.var(x, axis=-1).reshape(-1, 1)\n",
        "\n",
        "def mini(x):\n",
        "    return np.min(x, axis=-1).reshape(-1, 1)\n",
        "\n",
        "def maxi(x):\n",
        "    return np.max(x, axis=-1).reshape(-1, 1)\n",
        "\n",
        "def argmini(x):\n",
        "    return np.argmin(x, axis=-1).reshape(-1, 1)\n",
        "\n",
        "def argmaxi(x):\n",
        "    return np.argmax(x, axis=-1).reshape(-1, 1)\n",
        "\n",
        "def rms(x):\n",
        "    return np.sqrt(np.mean(x**2, axis=-1)).reshape(-1, 1)\n",
        "\n",
        "def abs_diff_signal(x):\n",
        "    return np.sum(np.abs(np.diff(x, axis=-1)), axis=-1).reshape(-1, 1)\n",
        "\n",
        "def skewness(x):\n",
        "    return stats.skew(x, axis=-1).reshape(-1, 1)\n",
        "\n",
        "def kurtosis(x):\n",
        "    return stats.kurtosis(x, axis=-1).reshape(-1, 1)\n",
        "\n",
        "def concat_features(x):\n",
        "    features = np.concatenate(\n",
        "        (\n",
        "            peaktopeak(x),\n",
        "            rms(x),\n",
        "            abs_diff_signal(x),\n",
        "            skewness(x),\n",
        "            kurtosis(x),\n",
        "            variance(x),\n",
        "            mean(x),\n",
        "            stddev(x)\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "    return features\n",
        "\n",
        "def apply_cwt(data, scales, wavelet_name='morl'):\n",
        "    \"\"\"\n",
        "    Apply Continuous Wavelet Transform (CWT) to EEG data.\n",
        "\n",
        "    :param data: EEG data in CSP space with shape (components, timepoints)\n",
        "    :param scales: Scales for CWT\n",
        "    :param wavelet_name: Name of the mother wavelet for CWT\n",
        "    :return: CWT coefficients\n",
        "    \"\"\"\n",
        "    cwt_coeffs = np.array([pywt.cwt(data[i, :], scales, wavelet_name)[0] for i in range(data.shape[0])])\n",
        "    return cwt_coeffs\n",
        "\n",
        "    \n",
        "def featuresarray_load(data_array):\n",
        "    features = []\n",
        "    fs = 500\n",
        "    for d in data_array:\n",
        "        \n",
        "       \n",
        "        alpha = mne.filter.filter_data(d, sfreq=fs, l_freq=8, h_freq=12,verbose=False)\n",
        "        beta = mne.filter.filter_data(d, sfreq=fs, l_freq=12, h_freq=30,verbose=False)\n",
        "        \n",
        "        alph_ftrs = concat_features(alpha)\n",
        "        beta_ftrs = concat_features(beta)\n",
        "        \n",
        "        #nperseg = 256\n",
        "        \n",
        "        \n",
        "        _,p=scipy.signal.welch(beta, fs=fs,average='median',nfft = data_array.shape[2]//2)\n",
        "        _,p2=scipy.signal.welch(alpha, fs=fs,average='median',nfft = data_array.shape[2]//2)\n",
        "        \n",
        "\n",
        "        res = np.mean([alph_ftrs,beta_ftrs],axis=0)\n",
        "        #print('p',p.shape,res.shape)\n",
        "        res = np.concatenate((res,p,p2),axis=1)\n",
        "        #print(res.shape)\n",
        "        features.append(res)\n",
        "    return np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from mne.decoding import CSP\n",
        "import mne\n",
        "from mne.decoding import CSP\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "event_ids = [7,8,9,10]  \n",
        "event_id_to_label = {}\n",
        "for i in range(len(event_ids)):\n",
        "    event_id_to_label[i] = event_ids[i]\n",
        "print(event_id_to_label)\n",
        "path = 'data/A09T.gdf'\n",
        "raw = mne.io.read_raw_gdf(path, eog=['EOG-left', 'EOG-central', 'EOG-right'], preload=True)\n",
        "raw.drop_channels(['EOG-left', 'EOG-central', 'EOG-right'])\n",
        "ica = mne.preprocessing.ICA(n_components=len(raw.info['ch_names']), random_state=42, max_iter=1000)\n",
        "ica.fit(raw)\n",
        "ica.apply(raw)\n",
        "csp_filters = {} \n",
        "events = mne.events_from_annotations(raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import stft\n",
        "action_dict = {0:'left_hand',1:'right_hand',2:'foot',3:'tongue'}\n",
        "\n",
        "\n",
        "# Create epochs for all events\n",
        "all_epochs = mne.Epochs(raw, events[0], event_id=event_ids,  # No specific event_id filtering\n",
        "                        tmin=1, tmax=6, baseline=None, preload=True)\n",
        "all_epochs.pick_types(meg=False, eeg=True)\n",
        "\n",
        "X_all = all_epochs.get_data()\n",
        "event_ids_all = all_epochs.events[:, -1]\n",
        "print(X_all.shape,event_ids_all.shape)\n",
        "# sf = 250\n",
        "# for trial in range(1):\n",
        "#     for i in range(1):\n",
        "#         # Compute the STFT\n",
        "\n",
        "#         frequencies, times, Zxx = stft(X_all[trial,i,:], fs=sf, window='hann', nperseg=64, noverlap=0.5*64)\n",
        "\n",
        "#         # Plot the spectrogram\n",
        "#         plt.pcolormesh(times, frequencies, np.abs(Zxx), shading='gouraud')\n",
        "#         plt.title(f'STFT Spectrogram of EEG Signal: label={action_dict[event_ids_all[trial]-7]}, channel={i}')\n",
        "#         plt.ylabel('Frequency [Hz]')\n",
        "#         plt.xlabel('Time [sec]')\n",
        "#         plt.colorbar(label='Intensity')\n",
        "        \n",
        "#         plt.show()\n",
        "\n",
        "'''# Loop over each event ID for OvR CSP\n",
        "for event_id in event_ids:\n",
        "    # Generate binary labels: current class (1) vs rest (0)\n",
        "    y = (event_ids_all == event_id).astype(int)\n",
        "\n",
        "    # Check if both classes are present\n",
        "    if np.unique(y).size < 2:\n",
        "        print(f\"Skipping event_id {event_id} as it does not have two classes for CSP.\")\n",
        "        continue\n",
        "\n",
        "    print('y is:', y)  # Check the binary labels\n",
        "\n",
        "    # Apply CSP for the current binary classification\n",
        "    csp = CSP(n_components=4, norm_trace=False)\n",
        "    csp.fit(X_all, y)\n",
        "    csp_filters[event_id] = csp\n",
        "    X_csp = csp.transform(X_all)\n",
        "\n",
        "    # Append the features\n",
        "    combined_features.append(X_csp)\n",
        "\n",
        "combined_features = np.concatenate(combined_features, axis=1) if combined_features else np.array([])'''\n",
        "ncomp = 5\n",
        "csp_transformed_data = {}\n",
        "for event_id in event_ids:\n",
        "    y = (event_ids_all == event_id).astype(int)\n",
        "    if np.unique(y).size < 2:\n",
        "        print(f\"Skipping event_id {event_id}.\")\n",
        "        continue\n",
        "\n",
        "    csp = CSP(n_components=ncomp, norm_trace=False, transform_into='csp_space')\n",
        "    csp.fit(X_all, y)\n",
        "    csp_transformed_data[event_id] = csp.transform(X_all)\n",
        "\n",
        "print('CSP FILTERS DICT:',csp_transformed_data)\n",
        "# Combine CSP features for each trial based on its label\n",
        "n_trials = len(X_all)  # Number of trials\n",
        "n_components = ncomp       # Number of CSP components (assuming 3 for this example)\n",
        "n_time_points = csp_transformed_data[7].shape[2]   # Number of time points in the transformed CSP data\n",
        "\n",
        "# Initialize the combined_features array to hold CSP features for all trials\n",
        "combined_features = np.zeros((n_trials, n_components, n_time_points))\n",
        "\n",
        "# Loop through each trial and assign the CSP-transformed data\n",
        "for i, label in enumerate(event_ids_all):\n",
        "    # Fetch the CSP features for the current trial and class label\n",
        "    # Adjust the indexing based on how your labels and csp_transformed_data are structured\n",
        "    csp_features_for_label = csp_transformed_data.get(label, None)\n",
        "\n",
        "    # Check if the label exists in the dictionary and if the index is within bounds\n",
        "    if csp_features_for_label is not None and i < len(csp_features_for_label):\n",
        "        combined_features[i, :, :] = csp_features_for_label[i]\n",
        "\n",
        "print(combined_features.shape)\n",
        "\n",
        "print(combined_features.shape)\n",
        "y = np.zeros((X_all.shape[0], len(event_ids)))  \n",
        "\n",
        "for i, event_id in enumerate(event_ids):\n",
        "    binary_labels = (event_ids_all == event_id).astype(int)\n",
        "    y[:, i] = binary_labels  \n",
        "print(y)\n",
        "\n",
        "y_flattened = np.argmax(y, axis=1)\n",
        "print(y_flattened)\n",
        "print(y_flattened.tolist().count(0),y_flattened.tolist().count(1),y_flattened.tolist().count(2),y_flattened.tolist().count(3))\n",
        "# clf = Pipeline([('scaler',StandardScaler()),('SVC', SVC())])\n",
        "print('features shape: ',combined_features.shape,y_flattened.shape)\n",
        "# scores = cross_val_score(clf, combined_features, y_flattened, cv=10, scoring='accuracy')\n",
        "# print(\"Multiclass classification accuracy: %f\" % scores.mean())\n",
        "\n",
        "ftrs = featuresarray_load(combined_features)\n",
        "\n",
        "print('features shape: ',ftrs.shape,y_flattened.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(ftrs, y_flattened, train_size=0.85, random_state=42, stratify=y_flattened)\n",
        "scaler = NDStandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "from keras.layers import PReLU, Conv1D, Dropout, SpatialDropout1D, MaxPooling1D, GlobalMaxPooling1D, Layer, AveragePooling1D, LSTM, Reshape, BatchNormalization\n",
        "from keras.regularizers import l1_l2\n",
        "# model = Sequential([\n",
        "#     Dense(256, activation='relu'),\n",
        "#     Dense(128, activation='tanh'),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(len(event_ids), activation='softmax')  \n",
        "# ])\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "GLOBAL_SHAPE_LENGTH = ftrs.shape[2]\n",
        "model = Sequential([\n",
        "        Reshape((GLOBAL_SHAPE_LENGTH,ncomp)),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Conv1D(64, kernel_size=7),\n",
        "        PReLU(),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        SpatialDropout1D(0.1),\n",
        "\n",
        "        Conv1D(128, kernel_size=5),\n",
        "        BatchNormalization(),\n",
        "        PReLU(),\n",
        "        AveragePooling1D(pool_size=2),\n",
        "        SpatialDropout1D(0.1),\n",
        "\n",
        "        LSTM(128, activation='tanh', recurrent_regularizer=l1_l2(l1=0.01, l2=0.01),return_sequences=True),\n",
        "        BatchNormalization(),\n",
        "        GlobalMaxPooling1D(),\n",
        "        BatchNormalization(),\n",
        "        Dense(units=128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.1),\n",
        "        Dense(units=64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.1),\n",
        "        Dense(units=4, activation='softmax')\n",
        "    ])\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(combined_features, y_flattened, train_size=0.7, random_state=42, stratify=y)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=300, validation_split=0.1, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.evaluate(X_test,y_test)[1]*100.00)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
        "\n",
        "for _ in range(25):\n",
        "\n",
        "  sample_index_train = random.randint(0, len(X_train) - 1)\n",
        "  train_sample = X_train[sample_index_train]\n",
        "  train_label = y_train[sample_index_train]\n",
        "\n",
        "  match_indices = np.where(y_test == train_label)[0]\n",
        "  sample_index_test = random.choice(match_indices)\n",
        "  test_sample = X_test[sample_index_test]\n",
        "\n",
        "\n",
        "  feature_axis = np.arange(len(train_sample))\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(24, 6))\n",
        "\n",
        "  # Plot train sample\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(feature_axis, train_sample)\n",
        "  plt.title(f\"Train Sample {sample_index_train}, Label {y_train[sample_index_train]}\")\n",
        "  plt.xlabel(\"Feature Index\")\n",
        "  plt.ylabel(\"Feature Value\")\n",
        " \n",
        "  plt.grid(True)\n",
        "\n",
        "  # Plot test sample\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(feature_axis, test_sample)\n",
        "  plt.title(f\"Test Sample {sample_index_test}, Label {y_test[sample_index_test]}\")\n",
        "  plt.xlabel(\"Feature Index\")\n",
        "  plt.ylabel(\"Feature Value\")\n",
        " \n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "lst = y_train.tolist()\n",
        "print(lst.count(0),lst.count(1),lst.count(2),lst.count(3),GLOBAL_SHAPE_LENGTH)\n",
        "lst = y_test.tolist()\n",
        "print(lst.count(0),lst.count(1),lst.count(2),lst.count(3),GLOBAL_SHAPE_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBTySQPRIgiE",
        "outputId": "3ea3d41e-cc63-4c22-eea1-3f8875725d81"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class Plasticity(gym.Env):\n",
        "  #dataset=(X_train, y_train)\n",
        "    def __init__(self, images_per_episode=1, dataset=(X_train, y_train), random=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf,\n",
        "                                                shape=(GLOBAL_SHAPE_LENGTH,ncomp),\n",
        "                                                dtype=np.float32)\n",
        "        self.images_per_episode = images_per_episode\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.x, self.y = dataset\n",
        "        self.random = random\n",
        "        self.dataset_idx = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        done = False\n",
        "        reward = self.calculate_reward(action)\n",
        "\n",
        "        obs = self._next_obs()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if self.step_count >= self.images_per_episode:\n",
        "            done = True\n",
        "\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.step_count = 0\n",
        "\n",
        "        obs = self._next_obs()\n",
        "        return obs\n",
        "\n",
        "    def _next_obs(self):\n",
        "        if self.random:\n",
        "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
        "            self.expected_action = int(self.y[next_obs_idx])\n",
        "            obs = self.x[next_obs_idx]\n",
        "        else:\n",
        "            obs = self.x[self.dataset_idx]\n",
        "            self.expected_action = int(self.y[self.dataset_idx])\n",
        "\n",
        "            self.dataset_idx += 1\n",
        "            if self.dataset_idx >= len(self.x):\n",
        "                raise StopIteration()\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def calculate_reward(self, action):\n",
        "      if action == self.expected_action:\n",
        "          reward = 1\n",
        "      else:\n",
        "          reward = 0\n",
        "\n",
        "      return reward\n",
        "\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ-Ph6WjIpSz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten,Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "tf.compat.v1.experimental.output_all_intermediates(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN7Tbe6FIrES",
        "outputId": "9ecbeb57-1b86-421a-f19a-5e99556593c4"
      },
      "outputs": [],
      "source": [
        "env = Plasticity()\n",
        "states = env.observation_space.shape\n",
        "actions = env.action_space.n\n",
        "print(env.action_space)\n",
        "print(states,actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYwn-zUpIsXc"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv1D, Dropout, AveragePooling1D, SpatialDropout1D, MaxPooling1D, BatchNormalization, LSTM, Flatten, Dense, PReLU, Reshape\n",
        "from keras.models import Sequential\n",
        "from keras.backend import clear_session\n",
        "from keras.regularizers import l1_l2\n",
        "\n",
        "def build_model(states, actions):\n",
        "    model = Sequential([\n",
        "        Reshape((GLOBAL_SHAPE_LENGTH,ncomp,),input_shape=(1,ncomp,GLOBAL_SHAPE_LENGTH)),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Conv1D(64, kernel_size=7),\n",
        "        PReLU(),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        SpatialDropout1D(0.1),\n",
        "\n",
        "        Conv1D(128, kernel_size=5),\n",
        "        BatchNormalization(),\n",
        "        PReLU(),\n",
        "        AveragePooling1D(pool_size=2),\n",
        "        SpatialDropout1D(0.1),\n",
        "\n",
        "        LSTM(128, activation='tanh', recurrent_regularizer=l1_l2(l1=0.01, l2=0.01),return_sequences=True),\n",
        "        BatchNormalization(),\n",
        "        GlobalMaxPooling1D(),\n",
        "        BatchNormalization(),\n",
        "        Dense(units=128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.1),\n",
        "        Dense(units=64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.1),\n",
        "        Dense(units=actions, activation='linear')\n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGGX6HXlIuua",
        "outputId": "454633ac-3f15-4cc8-8458-0d7960f7aa10"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "#!pip install keras-rl2\n",
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from keras import __version__\n",
        "import tensorflow as tf\n",
        "\n",
        "# To reset all information and start fresh, clear the current Keras session:\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#with strategy.scope():\n",
        "model = build_model(states, actions)\n",
        "\n",
        "model.build(input_shape=(1,*states))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBv8l1MKIwD6"
      },
      "outputs": [],
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = EpsGreedyQPolicy(eps=1.0)\n",
        "    memory = SequentialMemory(limit=30000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-4)\n",
        "    return dqn, policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73cjkIeuIxpb",
        "outputId": "da470881-6501-4ca9-ebe6-4613e7943842"
      },
      "outputs": [],
      "source": [
        "dqn, policy = build_agent(model, actions)\n",
        "dqn.compile(tf.keras.optimizers.legacy.Adam(learning_rate=0.0005,decay=1e-3), metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck7ckYMYIy3y",
        "outputId": "3afff6b9-b732-449f-8847-3d762b332109"
      },
      "outputs": [],
      "source": [
        "from rl.callbacks import Callback\n",
        "\n",
        "#print(dqn.policy.eps)\n",
        "class LossHistory(Callback):\n",
        "    def __init__(self):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        self.losses.append(logs['metrics'][0])\n",
        "\n",
        "class LossHistory2(Callback):\n",
        "    def __init__(self):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        self.losses.append(logs['metrics'][1])\n",
        "\n",
        "class RewardHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.rewards = []\n",
        "\n",
        "    def on_episode_end(self, episode, logs={}):\n",
        "        self.rewards.append(logs['episode_reward'])\n",
        "\n",
        "class ExponentialDecayEpsilonCallback(Callback):\n",
        "    def __init__(self, initial_epsilon, min_epsilon, decay_rate, decay_steps):\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.decay_rate = decay_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.step_count = 0\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        self.step_count += 1\n",
        "        # if(self.epsilon <= 1e-10):\n",
        "        #     self.epsilon=0.5\n",
        "        self.epsilon = self.min_epsilon + (self.epsilon - self.min_epsilon) * np.exp(-self.step_count / self.decay_steps)\n",
        "        self.model.policy.eps = max(self.epsilon, self.min_epsilon)\n",
        "        \n",
        "        if(self.step_count % 100 == 0): print(f\" Epsilon: {self.epsilon}\")\n",
        "\n",
        "\n",
        "initial_epsilon = 1.0  \n",
        "min_epsilon = 0.0  \n",
        "decay_rate = 0.0001    \n",
        "decay_steps = 100000   \n",
        "\n",
        "epsilon_decay_cb = ExponentialDecayEpsilonCallback(initial_epsilon, min_epsilon, decay_rate, decay_steps)\n",
        "\n",
        "\n",
        "loss_history = LossHistory()\n",
        "loss_history2 = LossHistory2()\n",
        "reward_history = RewardHistory()\n",
        "\n",
        "\n",
        "#dqn.load_weights(prefix+'bciiv2a_dqn_weights.hdf5')\n",
        "dqn.fit(env, nb_steps=2500, callbacks=[loss_history,loss_history2,reward_history,epsilon_decay_cb], verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dqn.policy.eps = 0\n",
        "dqn.fit(env, nb_steps=250, callbacks=[loss_history,loss_history2,reward_history], verbose=1)\n",
        "dqn.fit(env, nb_steps=250, callbacks=[loss_history,loss_history2,reward_history], verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_-Qcn1XvAqM",
        "outputId": "d7a5723a-9cb3-4dea-8e58-11c1cbec7790"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "def dqn_eval(dqn_agent,d):\n",
        "    attempts, correct = 0, 0\n",
        "    labels = d[1]\n",
        "    eenv = Plasticity(dataset=d, random=False)\n",
        "    thing = 1\n",
        "    y_predFull = []\n",
        "    y_trueest = []\n",
        "    total_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "\n",
        "        while True:\n",
        "\n",
        "            if thing == 1:\n",
        "                obs = eenv.reset()\n",
        "                thing = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "\n",
        "                obs_reshaped = np.reshape(obs, (1,) + obs.shape)\n",
        "\n",
        "                q_values = dqn_agent.compute_q_values(obs_reshaped)\n",
        "                #print(q_values)\n",
        "\n",
        "                action = np.argmax(q_values)\n",
        "\n",
        "                #print('action: ', action, \" - \", labels[attempts])\n",
        "                y_predFull.append(action)\n",
        "                y_trueest.append(labels[attempts])\n",
        "\n",
        "                obs, rew, done, _ = eenv.step(action)\n",
        "                total_reward += rew\n",
        "                if done:\n",
        "                    attempts += 1\n",
        "\n",
        "    except StopIteration:\n",
        "        print()\n",
        "        print('Validation done...','total reward=',total_reward)\n",
        "\n",
        "        y_predFull = np.array(y_predFull)\n",
        "        y_trueest = np.array(y_trueest)\n",
        "\n",
        "        cm = confusion_matrix(y_trueest, y_predFull)\n",
        "        print(y_predFull)\n",
        "        print(y_trueest)\n",
        "\n",
        "        print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "        report = classification_report(y_trueest, y_predFull,digits=4)\n",
        "        print(\"Classification Report:\\n\", report)\n",
        "\n",
        "        report = classification_report(y_trueest, y_predFull,output_dict=True)\n",
        "\n",
        "        correct = sum(y_predFull == y_trueest)\n",
        "        attempts = len(y_trueest)\n",
        "        accuracy = (float(correct) / attempts)\n",
        "        print('Validation done...')\n",
        "        print('Accuracy: {:.2f}%'.format(accuracy*100))\n",
        "\n",
        "        F1 = report['macro avg']['f1-score']\n",
        "        precision = report['macro avg']['precision']\n",
        "        recall = report['macro avg']['recall']\n",
        "\n",
        "        print(\"F1 Score: {:.2f}%\".format(F1*100))\n",
        "        print(\"Precision: {:.2f}%\".format(precision*100))\n",
        "        print(\"Recall: {:.2f}%\".format(recall*100))\n",
        "\n",
        "    y_predFull = []\n",
        "    y_trueest = []\n",
        "    return accuracy, F1, precision, recall\n",
        "\n",
        "dqn_eval(dqn,d=(X_train,y_train))\n",
        "print('***************************************************************************************')\n",
        "dqn_eval(dqn,d=(X_test,y_test))\n",
        "print('***************************************************************************************')\n",
        "import math\n",
        "def dqn_eval2(thing,dataset):\n",
        "\n",
        "    n_splits = 10\n",
        "\n",
        "    mean_rewards_per_fold = []\n",
        "    acc_per_fold = []\n",
        "    f1_mean, p_mean, r_mean = [], [], []\n",
        "\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    # if flag:\n",
        "    #   train = X_train\n",
        "    #   test = y_train\n",
        "    # else:\n",
        "    #   train = X_test\n",
        "    #   test = y_test\n",
        "    train, test = dataset\n",
        "   \n",
        "    for train_index, test_index in skf.split(train,test):\n",
        "        xtr, xte = train[train_index],train[test_index]\n",
        "        ytr, yte = test[train_index], test[test_index]\n",
        "\n",
        "        test_env = Plasticity(dataset = (xte, yte),random=True)\n",
        "        \n",
        "        scores = dqn.test(test_env, nb_episodes=10, visualize=False, verbose=0)\n",
        "        acc, f1, p, r = dqn_eval(dqn,d=(xte, yte))\n",
        "        \n",
        "        mean_reward = np.mean(scores.history['episode_reward'])\n",
        "        mean_rewards_per_fold.append(mean_reward)\n",
        "        acc_per_fold.append(acc)\n",
        "        f1_mean.append(f1)\n",
        "        r_mean.append(r)\n",
        "        p_mean.append(p)\n",
        "        \n",
        "\n",
        "    overall_mean_reward = np.mean(mean_rewards_per_fold)\n",
        "\n",
        "    f1_avg, p_avg, r_avg = np.mean(f1_mean), np.mean(p_mean), np.mean(r_mean)\n",
        "    print(f\"Overall Mean Reward across all folds: {overall_mean_reward}\")\n",
        "    acc_avg = np.mean(acc_per_fold)\n",
        "    print(f\"Overall Mean Accuracy across all folds: {acc_avg * 100} %\")\n",
        "    print(f\"Overall Mean F1 across all folds: {f1_avg * 100} %\")\n",
        "    print(f\"Overall Mean Precision across all folds: {p_avg * 100} %\")\n",
        "    print(f\"Overall Mean Recall across all folds: {r_avg * 100} %\")\n",
        "\n",
        "def dqn_eval3(dqn_agent, dataset):\n",
        "\n",
        "    n_splits = 10\n",
        "\n",
        "    mean_rewards_per_fold = []\n",
        "    acc_per_fold = []\n",
        "    f1_mean, p_mean, r_mean = [], [], []\n",
        "    \n",
        "    train,test=dataset\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    \n",
        "    for train_index, test_index in skf.split(train,test):\n",
        "        xtr, xte = train[train_index],train[test_index]\n",
        "        ytr, yte = test[train_index], test[test_index]\n",
        "\n",
        "        test_env = Plasticity(dataset = (xte, yte),random=True)\n",
        "        \n",
        "        scores = dqn.test(test_env, nb_episodes=10, visualize=False, verbose=0)\n",
        "        acc, f1, p, r = dqn_eval(dqn,d=(xte, yte))\n",
        "        \n",
        "        mean_reward = np.mean(scores.history['episode_reward'])\n",
        "        mean_rewards_per_fold.append(mean_reward)\n",
        "        acc_per_fold.append(acc)\n",
        "        f1_mean.append(f1)\n",
        "        r_mean.append(r)\n",
        "        p_mean.append(p)\n",
        "        \n",
        "\n",
        "    overall_mean_reward = np.mean(mean_rewards_per_fold)\n",
        "\n",
        "    f1_avg, p_avg, r_avg = np.mean(f1_mean), np.mean(p_mean), np.mean(r_mean)\n",
        "    print(f\"Overall Mean Reward across all folds: {overall_mean_reward}\")\n",
        "    acc_avg = np.mean(acc_per_fold)\n",
        "    print(f\"Overall Mean Accuracy across all folds: {acc_avg * 100} %\")\n",
        "    print(f\"Overall Mean F1 across all folds: {f1_avg * 100} %\")\n",
        "    print(f\"Overall Mean Precision across all folds: {p_avg * 100} %\")\n",
        "    print(f\"Overall Mean Recall across all folds: {r_avg * 100} %\")\n",
        "features_array, label_array = np.concatenate((X_train,X_test)),np.concatenate((y_train,y_test))\n",
        "dqn_eval2(thing=True, dataset=(features_array,label_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Confusion matrix data\n",
        "conf_matrix = np.array([[10,  0,  0,  0],\n",
        " [ 0, 11,  0,  0],\n",
        " [ 0,  1, 10,  0],\n",
        " [ 0,  0,  0, 11]])\n",
        "\n",
        "# Normalize the confusion matrix row-wise for percentages\n",
        "row_sums = conf_matrix.sum(axis=1)\n",
        "conf_matrix_percent = conf_matrix / row_sums[:, np.newaxis] * 100\n",
        "\n",
        "# Create an annotation matrix for displaying counts and percentages\n",
        "annotations = np.empty_like(conf_matrix, dtype=object)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        annotations[i, j] = f\"{conf_matrix[i, j]}\\n{conf_matrix_percent[i, j]:.2f}%\"\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_percent, annot=annotations, fmt=\"\", cmap=\"Blues\", cbar=True,\n",
        "            xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix with Samples and Percentages')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z5i1pJFud0Hk",
        "outputId": "2e2f65b0-85e0-4366-fce7-866d3239f3c4"
      },
      "outputs": [],
      "source": [
        "device_name = '/device:GPU:0'\n",
        "#dqn_eval2(thing=True,flag=False)\n",
        "# print(np.mean(scores.history['episode_reward'])*100,'%')\n",
        "# plt.plot(scores.history['episode_reward'])\n",
        "# plt.title('Testing Rewards per Episode')\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Cumulative Reward')\n",
        "# plt.show()\n",
        "# print(np.mean(scores.history['episode_reward'])*100,'%')\n",
        "dqn.policy.eps = 0\n",
        "# train_new = np.load(file='/content/drive/MyDrive/a/bciiv2a_test_features_A02E.gdf.npy')\n",
        "# train_new_labels = train_dict['A02T.gdf']['labels']\n",
        "\n",
        "#dqn_eval(dqn,d=(X_test[len(X_test)-25:],y_test[len(y_test)-25:]))\n",
        "dqn_eval(dqn,d=(X_test,y_test))\n",
        "\n",
        "dqn.policy.eps = 0\n",
        "scores = dqn.test(Plasticity(images_per_episode=1,random=True,dataset=(X_test,y_test)), nb_episodes=10, visualize=False, action_repetition=1, verbose=1)\n",
        "print(np.mean(scores.history['episode_reward'])*100.,'%')\n",
        "\n",
        "\n",
        "def moving_average(data, window_size):\n",
        "    return [np.mean(data[i:i+window_size]) for i in range(len(data) - window_size + 1)]\n",
        "losses = loss_history.losses\n",
        "losses2 = loss_history2.losses\n",
        "smoothed_losses = moving_average(losses, window_size=50)\n",
        "print(smoothed_losses)\n",
        "plt.plot(smoothed_losses)\n",
        "plt.title('Smoothed Training Loss per Step')\n",
        "plt.xlabel('Step')\n",
        "plt.xlim(left=0)\n",
        "plt.ylabel('Smoothed Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "smoothed_losses2 = moving_average(losses2, window_size=70)\n",
        "print(smoothed_losses2)\n",
        "plt.plot(smoothed_losses2)\n",
        "plt.title('Smoothed Training Loss per Step')\n",
        "plt.xlabel('Step')\n",
        "plt.xlim(left=0)\n",
        "plt.ylabel('Smoothed Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# plt.plot(moving_average(reward_history.rewards, window_size=70))\n",
        "# plt.title('Training Rewards per Episode')\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Cumulative Reward')\n",
        "# plt.show()\n",
        "\n",
        "print(\"___________________________________________________________________________________________________________________________\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
