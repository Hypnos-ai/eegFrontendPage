{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxgwqMkYsUGV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.threading.set_intra_op_parallelism_threads(6)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1LpIQUxsUGc"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import mne\n",
    "import os\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz3o9rZcsUGe"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sNyv6YWsUGg"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import time\n",
    "\n",
    "dataset = []\n",
    "labels = []\n",
    "extension_dataset = []\n",
    "flexion_dataset = []\n",
    "rest_dataset = []\n",
    "for filename in os.listdir('formated_data'):\n",
    "    filepath = os.path.join('formated_data', filename)\n",
    "    data = scipy.io.loadmat(filepath)\n",
    "    data.pop('__header__')\n",
    "    data.pop('__version__')\n",
    "    data.pop('__globals__')\n",
    "    \n",
    "\n",
    "    data_arr = np.array(data['data'])\n",
    "    \n",
    "    data_arr = data_arr.T\n",
    "    if 'EXTENSION' in filename:\n",
    "        extension_dataset.append(data_arr)\n",
    "    if 'FLEXION' in filename:\n",
    "        flexion_dataset.append(data_arr)\n",
    "    if 'REST' in filename:\n",
    "        rest_dataset.append(data_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wP4DGsyJsUGh"
   },
   "outputs": [],
   "source": [
    "def read_data(dataset):\n",
    "    for i in range(len(dataset)):\n",
    "        #dataset[i] = np.resize(dataset[i],(7,16384))\n",
    "        d = dataset[i].astype(float)\n",
    "        dataset[i] = mne.filter.filter_data(d, l_freq=20, h_freq=450,sfreq=2000)\n",
    "        print(d.shape)\n",
    "\n",
    "    dataset = np.array(dataset)\n",
    "    print(dataset.shape)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8N1WiidMsUGk",
    "outputId": "e1073dcc-1f39-4a3e-fac9-6c96fcbdfaca"
   },
   "outputs": [],
   "source": [
    "\n",
    "#chan_names = ['Fp1','AF7','AF3','AFz','F7','F5','F3','F1','Fz','FT7','FC5','FC3','FC1','T7','C5','C3','C1','Cz','TP7','CP5','CP3','CP1','CPz','P7','P5','P3','P1','Pz','PO7','PO3','POz','Fp2','AF4','AF8','F2','F4','F6','F8','FC2','FC4','FC6','FT8','C2','C4','C6','T8','CP2','CP4','CP6','TP8','P2','P4','P6','P8','PO4','PO8','O1','Oz','O2','Iz']\n",
    "chan_names = ['ECR','FCR','ECU','FCU']\n",
    "info = mne.create_info(ch_names=chan_names, ch_types=['emg']*4, sfreq=2000)\n",
    "info.set_montage('standard_1020')\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58yzH7FDsUGl"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "extension_epochs = mne.EpochsArray(data=read_data(extension_dataset), info=info)\n",
    "flexion_epochs = mne.EpochsArray(data=read_data(flexion_dataset), info=info)\n",
    "rest_epochs = mne.EpochsArray(data=read_data(rest_dataset), info=info)\n",
    "# extension_epochs1 = mne.EpochsArray(data=(extension_dataset), info=info)\n",
    "# flexion_epochs1 = mne.EpochsArray(data=(flexion_dataset), info=info)\n",
    "# rest_epochs1 = mne.EpochsArray(data=(rest_dataset), info=info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2ldblRpsUGm",
    "outputId": "0ecb3766-fc4b-4ee3-88fc-a1e3b02a9e41"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from mne.time_frequency import tfr_morlet\n",
    "%matplotlib inline\n",
    "print(extension_epochs.get_data().shape)\n",
    "print(flexion_epochs.get_data().shape)\n",
    "print(rest_epochs.get_data().shape)\n",
    "tmp = np.vstack((extension_epochs.get_data(),flexion_epochs.get_data(),rest_epochs.get_data()))\n",
    "fs=2000\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"________________________________ \",i+1)\n",
    "    rand = random.randint(0,599)\n",
    "    if(0 <= rand <= 199):\n",
    "        print(\"extension\")\n",
    "    elif(200 <= rand <= 399):\n",
    "        print(\"flexion\")\n",
    "    else:\n",
    "        print(\"rest\")\n",
    "    \n",
    "    d = tmp[rand]\n",
    "    \n",
    "    t_evoked = mne.EvokedArray(d, info, tmin=0)\n",
    "    \n",
    "    \n",
    "    t_evoked.plot()\n",
    "    \n",
    "    print(\"________________________________ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "def min_max_scale_signal(signal, min_val=0, max_val=1):\n",
    "    \"\"\"\n",
    "    Scale the signal to a new range [min_val, max_val].\n",
    "\n",
    "    Parameters:\n",
    "    - signal: Input signal, a numpy array.\n",
    "    - min_val: Minimum value of the scaled range.\n",
    "    - max_val: Maximum value of the scaled range.\n",
    "\n",
    "    Returns:\n",
    "    - Scaled signal.\n",
    "    \"\"\"\n",
    "    min_signal = np.min(signal)\n",
    "    max_signal = np.max(signal)\n",
    "    scaled_signal = (signal - min_signal) / (max_signal - min_signal) * (max_val - min_val) + min_val\n",
    "    return scaled_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZ8Z0lcOsUGn",
    "outputId": "02073b45-7d47-4543-aca3-be3194e7a44f"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import mne\n",
    "import os\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "ext_list = []\n",
    "flex_list = []\n",
    "rest_list = []\n",
    "\n",
    "ext_list.append(extension_epochs.get_data())\n",
    "flex_list.append(flexion_epochs.get_data())\n",
    "rest_list.append(rest_epochs.get_data())\n",
    "extension_labels = [0 for i in range(extension_epochs.get_data().shape[0])]\n",
    "print(extension_labels)\n",
    "\n",
    "flexion_labels = [1 for i in range(flexion_epochs.get_data().shape[0])]\n",
    "print(flexion_labels)\n",
    "\n",
    "rest_labels = [2 for i in range(rest_epochs.get_data().shape[0])]\n",
    "print(rest_labels)\n",
    "\n",
    "label_list = extension_labels + flexion_labels + rest_labels\n",
    "\n",
    "extension_array = np.vstack(ext_list)\n",
    "flexion_array = np.vstack(flex_list)\n",
    "rest_array = np.vstack(rest_list)\n",
    "\n",
    "extension_array = min_max_scale_signal(extension_array,-1,1)\n",
    "flexion_array = min_max_scale_signal(flexion_array,-1,1)\n",
    "rest_array = min_max_scale_signal(rest_array,-1,1)\n",
    "\n",
    "label_array = np.hstack(label_list)\n",
    "\n",
    "print(label_array.shape)\n",
    "print(extension_array.shape,flexion_array.shape,rest_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0_jGaYmsUGo",
    "outputId": "8fed4ea4-baa9-4bfb-f33c-cddb43d525a5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pywt\n",
    "import scipy.signal\n",
    "from scipy import stats\n",
    "def mean(x):\n",
    "    return np.mean(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def stddev(x):\n",
    "    return np.std(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def peaktopeak(x):\n",
    "    return np.ptp(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def variance(x):\n",
    "    return np.var(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def mini(x):\n",
    "    return np.min(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def maxi(x):\n",
    "    return np.max(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def argmini(x):\n",
    "    return np.argmin(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def argmaxi(x):\n",
    "    return np.argmax(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def rms(x):\n",
    "    return np.sqrt(np.mean(x**2, axis=-1)).reshape(-1, 1)\n",
    "\n",
    "def abs_diff_signal(x):\n",
    "    return np.sum(np.abs(np.diff(x, axis=-1)), axis=-1).reshape(-1, 1)\n",
    "\n",
    "def skewness(x):\n",
    "    return stats.skew(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def kurtosis(x):\n",
    "    return stats.kurtosis(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def concat_features(x):\n",
    "    features = np.concatenate(\n",
    "        (\n",
    "            peaktopeak(x),\n",
    "            rms(x),\n",
    "            abs_diff_signal(x),\n",
    "            skewness(x),\n",
    "            kurtosis(x),\n",
    "            variance(x),\n",
    "            mean(x),\n",
    "            stddev(x)\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "    \n",
    "def featuresarray_load(data_array):\n",
    "    features = []\n",
    "    fs = 2000\n",
    "    for d in data_array:\n",
    "        \n",
    "        beta_ftrs = concat_features(d)\n",
    "        \n",
    "        _,p=scipy.signal.welch(d, fs=fs,average='median')\n",
    "        \n",
    "        res = np.concatenate((beta_ftrs,p),axis=1)\n",
    "        \n",
    "        features.append(res)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ext = featuresarray_load(extension_array)\n",
    "f_flex = featuresarray_load(flexion_array)\n",
    "f_rest = featuresarray_load(rest_array)\n",
    "\n",
    "np.save(\"ext_features_array.npy\",allow_pickle=False,arr=f_ext)\n",
    "np.save(\"flex_features_array.npy\",allow_pickle=False,arr=f_flex)\n",
    "np.save(\"rest_features_array.npy\",allow_pickle=False,arr=f_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VXS4FejsUGp"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "GVzyoGBisUGp",
    "outputId": "ca6587fe-7328-43dc-fd69-46e61fd64682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 4, 137)\n",
      "137\n",
      "(480, 4, 137)\n",
      "(120, 4, 137)\n",
      "(480,)\n",
      "(120,)\n",
      "160 160 160 137\n",
      "40 40 40 137\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, LayerNormalization, Reshape\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.backend import clear_session\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "# features_array = np.load(file=\"features_array3.npy\")\n",
    "# GLOBAL_SHAPE_LENGTH = features_array.shape[1]\n",
    "# scaler = StandardScaler()\n",
    "# features_array = scaler.fit_transform(features_array)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, LayerNormalization, Reshape\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.backend import clear_session\n",
    "\n",
    "e_features_array = np.load(file=\"ext_features_array.npy\")\n",
    "f_features_array = np.load(file=\"flex_features_array.npy\")\n",
    "r_features_array = np.load(file=\"rest_features_array.npy\")\n",
    "features_array = np.concatenate((e_features_array,f_features_array,r_features_array))\n",
    "\n",
    "\n",
    "print(features_array.shape)\n",
    "GLOBAL_SHAPE_LENGTH = features_array.shape[2]\n",
    "ncomp = features_array.shape[1]\n",
    "print(GLOBAL_SHAPE_LENGTH)\n",
    "\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_array, label_array, test_size=.2, random_state=42, shuffle=True, stratify=label_array)\n",
    "features_array  = scaler.fit_transform(features_array.reshape(-1, features_array.shape[-1])).reshape(features_array.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "lst = y_train.tolist()\n",
    "print(lst.count(0),lst.count(1),lst.count(2),GLOBAL_SHAPE_LENGTH)\n",
    "\n",
    "lst = y_test.tolist()\n",
    "print(lst.count(0),lst.count(1),lst.count(2),GLOBAL_SHAPE_LENGTH)\n",
    "\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for _ in range(25):\n",
    "#     sample_index_train = random.randint(0, len(X_train) - 1)\n",
    "#     train_sample = X_train[sample_index_train]\n",
    "#     train_label = y_train[sample_index_train]\n",
    "\n",
    "#     match_indices = np.where(y_test == train_label)[0]\n",
    "#     sample_index_test = random.choice(match_indices)\n",
    "#     test_sample = X_test[sample_index_test]\n",
    "\n",
    "#     time_axis = np.arange(train_sample.shape[1])  # Time points on the x-axis\n",
    "\n",
    "#     plt.figure(figsize=(24, 6))\n",
    "\n",
    "#     # Plot train sample\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     for channel in range(train_sample.shape[0]):\n",
    "#         plt.plot(time_axis, train_sample[channel, :], label=f'Channel {channel+1}')\n",
    "#     plt.title(f\"Train Sample {sample_index_train}, Label {train_label}\")\n",
    "#     plt.xlabel(\"Time Point\")\n",
    "#     plt.ylabel(\"Signal Amplitude\")\n",
    "#     plt.ylim((-1, 1))\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     # Plot test sample\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     for channel in range(test_sample.shape[0]):\n",
    "#         plt.plot(time_axis, test_sample[channel, :], label=f'Channel {channel+1}')\n",
    "#     plt.title(f\"Test Sample {sample_index_test}, Label {y_test[sample_index_test]}\")\n",
    "#     plt.xlabel(\"Time Point\")\n",
    "#     plt.ylabel(\"Signal Amplitude\")\n",
    "#     plt.ylim((-1, 1))\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Define a custom dataset class for handling the EMG data\n",
    "class EMGDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)  # Convert data to torch tensors\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Convert labels to torch tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return a tuple of (data, label) for each index\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "# Assuming X_train and y_train are numpy arrays\n",
    "# Create a custom dataset using the EMGDataset class\n",
    "train_dataset = EMGDataset(X_train, y_train)\n",
    "test_dataset = EMGDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader for the training and test sets\n",
    "batch_size = 32  # Set an appropriate batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 4*137\n",
    "num_hidden = 1000\n",
    "num_outputs = 3\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "beta = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import surrogate\n",
    "from torch import nn\n",
    "import snntorch as snn\n",
    "\n",
    "# Define the necessary parameters (ensure these match your model's requirements)\n",
    "num_inputs = 4 * 137  # Flattened input size: 4 channels x 137 time steps\n",
    "num_hidden = 1000     # Number of hidden units in the first fully connected layer\n",
    "num_outputs = 3       # Number of output classes (assuming 3 classes in the target labels)\n",
    "beta = 0.9            # Decay constant for the LIF neuron\n",
    "num_steps = 137       # Number of time steps (should match the temporal dimension of your data)\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)  # Input: 548 (4*137), Output: 1000 hidden units\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)  # Output: 3 classes\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input outside the loop for all samples in the batch\n",
    "        # Convert (batch_size, num_channels, num_timesteps) -> (batch_size, num_channels * num_timesteps)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input to shape: (batch_size, 548)\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        \n",
    "        # Record the final layer outputs\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # time-loop: Iterate through each time step\n",
    "        for step in range(num_steps):\n",
    "            # Pass through the first fully connected layer\n",
    "            x = x.flatten(1)\n",
    "            cur1 = self.fc1(x)  # Use the flattened input\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            # Pass through the second fully connected layer\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "          \n",
    "            # Store in list\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        # Stack the outputs over time to return the final tensor\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)  # Shape: (time_steps, batch_size, num_outputs)\n",
    "        \n",
    "# Load the network onto CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Train Loss: 9.703728675842285\n",
      "Iteration: 10 \t Train Loss: 1.741128921508789\n",
      "Iteration: 20 \t Train Loss: 0.9632488489151001\n",
      "Iteration: 30 \t Train Loss: 1.1436772346496582\n",
      "Iteration: 40 \t Train Loss: 1.0642805099487305\n",
      "Iteration: 50 \t Train Loss: 0.6866699457168579\n",
      "Iteration: 60 \t Train Loss: 0.4688836336135864\n",
      "Iteration: 70 \t Train Loss: 0.55772465467453\n",
      "Iteration: 80 \t Train Loss: 0.5328113436698914\n",
      "Iteration: 90 \t Train Loss: 0.3433271050453186\n",
      "Iteration: 100 \t Train Loss: 0.20605269074440002\n",
      "Iteration: 110 \t Train Loss: 0.3877933919429779\n",
      "Iteration: 120 \t Train Loss: 0.205989807844162\n",
      "Iteration: 130 \t Train Loss: 0.34619802236557007\n",
      "Iteration: 140 \t Train Loss: 0.41201362013816833\n",
      "Iteration: 150 \t Train Loss: 0.44649457931518555\n",
      "Iteration: 160 \t Train Loss: 0.41201990842819214\n",
      "Iteration: 170 \t Train Loss: 0.30911222100257874\n",
      "Iteration: 180 \t Train Loss: 0.2808725833892822\n",
      "Iteration: 190 \t Train Loss: 0.38753166794776917\n",
      "Iteration: 200 \t Train Loss: 0.30900537967681885\n",
      "Iteration: 210 \t Train Loss: 0.583638608455658\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "\n",
    "num_epochs = 15 # 60000 / 128 = 468\n",
    "counter = 0\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, _ = net(data)\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        loss_val = loss(spk_rec.sum(0), targets) # batch x num_out\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print train/test loss/accuracy\n",
    "        if counter % 10 == 0:\n",
    "            print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.6583333611488342\n"
     ]
    }
   ],
   "source": [
    "def measure_accuracy(model, dataloader):\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_length = 0\n",
    "    running_accuracy = 0\n",
    "\n",
    "    for data, targets in iter(dataloader):\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      # forward-pass\n",
    "      spk_rec, _ = model(data)\n",
    "      spike_count = spk_rec.sum(0) # batch x num_outputs\n",
    "      _, max_spike = spike_count.max(1)\n",
    "\n",
    "      # correct classes for one batch\n",
    "      num_correct = (max_spike == targets).sum()\n",
    "\n",
    "      # total accuracy\n",
    "      running_length += len(targets)\n",
    "      running_accuracy += num_correct\n",
    "    \n",
    "    accuracy = (running_accuracy / running_length)\n",
    "\n",
    "    return accuracy.item()\n",
    "print(f\"Test set accuracy: {measure_accuracy(net, test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Calculate the memory size of the model parameters\n",
    "model_size = sum(p.element_size() * p.numel() for p in net.parameters())\n",
    "print(f\"Model size: {model_size / 1024**2:.2f} MB\")  # Convert to MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3CebMTUsUGr"
   },
   "outputs": [],
   "source": [
    "class Plasticity(gym.Env):\n",
    "    def __init__(self, images_per_episode=1, dataset=(X_train, y_train), random=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1,\n",
    "                                                shape=(GLOBAL_SHAPE_LENGTH,ncomp),\n",
    "                                                dtype=np.float32)\n",
    "        self.images_per_episode = images_per_episode\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.x, self.y = dataset\n",
    "        self.random = random\n",
    "        self.dataset_idx = 0\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        done = False\n",
    "        reward = self.calculate_reward(action)\n",
    "\n",
    "        obs = self._next_obs()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.images_per_episode:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "\n",
    "        obs = self._next_obs()\n",
    "\n",
    "        return obs\n",
    "    \n",
    "    def calculate_reward(self, action):\n",
    "      if action == self.expected_action:\n",
    "          reward = 1\n",
    "          \n",
    "      else:\n",
    "          reward = 0\n",
    "\n",
    "      return reward\n",
    "    \n",
    "    def _next_obs(self):\n",
    "        if self.random:\n",
    "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
    "            self.expected_action = int(self.y[next_obs_idx])\n",
    "            obs = self.x[next_obs_idx]\n",
    "\n",
    "\n",
    "        else:\n",
    "            obs = self.x[self.dataset_idx]\n",
    "            self.expected_action = int(self.y[self.dataset_idx])\n",
    "\n",
    "            self.dataset_idx += 1\n",
    "            #print(f\"Current dataset index: {self.dataset_idx}\")\n",
    "            if self.dataset_idx >= len(self.x):\n",
    "                raise StopIteration()\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ln_pos7WsUGr"
   },
   "outputs": [],
   "source": [
    "env = Plasticity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbHgmMpzsUGr",
    "outputId": "ddff02df-46ec-42b0-eb65-97b2d966175a"
   },
   "outputs": [],
   "source": [
    "env.observation_space.sample()\n",
    "env.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duv9LGQ5sUGs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten,Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.compat.v1.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDFHkEuCsUGs",
    "outputId": "75299265-63f4-4e49-952d-0426fb5e1620"
   },
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKRJF-H3sUGs"
   },
   "outputs": [],
   "source": [
    "from keras.layers import PReLU, SpatialDropout1D, MaxPooling1D, GlobalMaxPooling1D, Layer, AveragePooling1D\n",
    "from keras.regularizers import l1_l2\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "import keras.backend as K\n",
    "# @keras.saving.register_keras_serializable()\n",
    "# class LightweightLSTMAttentionLayer(Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(LightweightLSTMAttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(name='att_weight',\n",
    "#                                  shape=(input_shape[-1], 1),\n",
    "#                                  initializer='glorot_uniform',\n",
    "#                                  trainable=True)\n",
    "#         self.b = self.add_weight(name='att_bias',\n",
    "#                                  shape=(input_shape[1], 1),\n",
    "#                                  initializer='zeros',\n",
    "#                                  trainable=True)\n",
    "#         super(LightweightLSTMAttentionLayer, self).build(input_shape)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         # Attention mechanism\n",
    "#         e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "#         e = K.squeeze(e, axis=-1)\n",
    "#         alpha = K.softmax(e)\n",
    "#         alpha = K.expand_dims(alpha, axis=-1)\n",
    "\n",
    "#         # Apply the attention\n",
    "#         context = x * alpha\n",
    "#         context = K.sum(context, axis=1)\n",
    "#         return context\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "def build_model(states, actions):\n",
    "    clear_session()\n",
    "    model = Sequential([\n",
    "        Reshape((GLOBAL_SHAPE_LENGTH,ncomp,),input_shape=(1,ncomp,GLOBAL_SHAPE_LENGTH)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv1D(64, kernel_size=7),\n",
    "        PReLU(),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        SpatialDropout1D(0.1),\n",
    "\n",
    "        Conv1D(128, kernel_size=5),\n",
    "        BatchNormalization(),\n",
    "        PReLU(),\n",
    "        AveragePooling1D(pool_size=2),\n",
    "        SpatialDropout1D(0.1),\n",
    "\n",
    "        LSTM(128, activation='tanh', recurrent_regularizer=l1_l2(l1=0.01, l2=0.01),return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        GlobalMaxPooling1D(),\n",
    "        BatchNormalization(),\n",
    "        Dense(units=128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(units=actions, activation='linear')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRSj6wCmsUGt",
    "outputId": "2811de91-e495-466d-fed9-cca13fc37953"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from keras import __version__\n",
    "model = build_model(states, actions)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrZzVOxxsUGt"
   },
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = EpsGreedyQPolicy(eps=0.1)\n",
    "    memory = SequentialMemory(limit=30000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-4)\n",
    "    return dqn, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7jDfPBisUGt",
    "outputId": "ea955ec6-ea99-46c2-cd06-989207863beb"
   },
   "outputs": [],
   "source": [
    "dqn, policy = build_agent(model, actions)\n",
    "dqn.compile(tf.keras.optimizers.legacy.Adam(learning_rate=0.0055,decay=1e-4), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeoVI_W8sUGu",
    "outputId": "ca1b7d66-0928-4ed7-9e68-e58a3f501796"
   },
   "outputs": [],
   "source": [
    "from rl.callbacks import Callback\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        self.losses.append(logs['metrics'][0])\n",
    "\n",
    "class LossHistory2(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        self.losses.append(logs['metrics'][1])\n",
    "\n",
    "class RewardHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.rewards = []\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        self.rewards.append(logs['episode_reward'])\n",
    "\n",
    "class ExponentialDecayEpsilonCallback(Callback):\n",
    "    def __init__(self, initial_epsilon, min_epsilon, decay_rate, decay_steps):\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.step_count = 0\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        self.step_count += 1\n",
    "        self.epsilon = self.min_epsilon + (self.epsilon - self.min_epsilon) * np.exp(-self.step_count / self.decay_steps)\n",
    "        self.model.policy.eps = max(self.epsilon, self.min_epsilon)\n",
    "        #print(f\"Step: {self.step_count}, New Epsilon: {self.epsilon}\")\n",
    "\n",
    "\n",
    "\n",
    "initial_epsilon = 1.0  \n",
    "min_epsilon = 0.0 \n",
    "decay_rate = 0.0001     \n",
    "decay_steps = 100000    \n",
    "epsilon_decay_cb = ExponentialDecayEpsilonCallback(initial_epsilon, min_epsilon, decay_rate, decay_steps)\n",
    "\n",
    "\n",
    "loss_history = LossHistory()\n",
    "loss_history2 = LossHistory2()\n",
    "reward_history = RewardHistory()\n",
    "\n",
    "\n",
    "dqn.fit(env, nb_steps=2000, callbacks=[loss_history,loss_history2,reward_history,epsilon_decay_cb], verbose=1)\n",
    "dqn.policy.eps = 0\n",
    "dqn.fit(env, nb_steps=200, callbacks=[loss_history,loss_history2,reward_history], verbose=1)\n",
    "dqn.fit(env, nb_steps=200, callbacks=[loss_history,loss_history2,reward_history], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPhaseLogger(Callback):\n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "    def on_action_end(self, action, logs={}):\n",
    "       \n",
    "        if not self.model.training and self.env.calculate_reward(action) > 0:\n",
    "            print(f\"Correct Action: {action}, Reward Gained\")\n",
    "        elif not self.model.training and self.env.calculate_reward(action) < 0:\n",
    "            print(f\"Wrong Action: {action}, Correct Action {self.env.y[self.env.dataset_idx-1]}, Penalty Incurred\")\n",
    "test_logger = TestPhaseLogger(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTCXwGHXsUGu",
    "outputId": "e0677694-3ee8-458c-fee9-b2185b165140"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from rl.policy import GreedyQPolicy\n",
    "dqn.policy = GreedyQPolicy()\n",
    "\n",
    "def dqn_eval(dqn_agent,d):\n",
    "    attempts, correct = 0, 0\n",
    "    labels = d[1]\n",
    "    eenv = Plasticity(dataset=d, random=False)\n",
    "    thing = 1\n",
    "    y_predFull = []\n",
    "    y_trueest = []\n",
    "    total_reward = 0\n",
    "    try:\n",
    "        while True:\n",
    "            if thing == 1:\n",
    "                obs = eenv.reset()\n",
    "                thing = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "\n",
    "                obs_reshaped = np.reshape(obs, (1,) + obs.shape)\n",
    "\n",
    "                q_values = dqn_agent.compute_q_values(obs_reshaped)\n",
    "                #print(q_values)\n",
    "\n",
    "                action = np.argmax(q_values)\n",
    "                \n",
    "                #print('action: ', action, \" - \", labels[attempts])\n",
    "                y_predFull.append(action)\n",
    "                y_trueest.append(labels[attempts])\n",
    "\n",
    "                obs, rew, done, _ = eenv.step(action)\n",
    "                if(rew == 2):\n",
    "                    print(f\"Correct Action: {action}, Reward Gained: {rew}\")\n",
    "                else:\n",
    "                    print(f\"Wrong Action: {action}, Correct Action {labels[attempts]}, Reward Gained: {rew}\")\n",
    "                total_reward += rew\n",
    "                if done:\n",
    "                    attempts += 1\n",
    "    except StopIteration:\n",
    "        print()\n",
    "        print('Validation done...','total reward=',total_reward+1)\n",
    "\n",
    "        y_predFull = np.array(y_predFull)\n",
    "        y_trueest = np.array(y_trueest)\n",
    "\n",
    "        cm = confusion_matrix(y_trueest, y_predFull)\n",
    "        print(y_predFull)\n",
    "        print(y_trueest)\n",
    "\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "        report = classification_report(y_trueest, y_predFull,digits=4)\n",
    "        print(\"Classification Report:\\n\", report)\n",
    "\n",
    "        report = classification_report(y_trueest, y_predFull,output_dict=True)\n",
    "\n",
    "        correct = sum(y_predFull == y_trueest)\n",
    "        attempts = len(y_trueest)\n",
    "        accuracy = (float(correct) / attempts)\n",
    "        print('Validation done...')\n",
    "        print('Accuracy: {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "        F1 = report['macro avg']['f1-score']\n",
    "        precision = report['macro avg']['precision']\n",
    "        recall = report['macro avg']['recall']\n",
    "\n",
    "        print(\"F1 Score: {:.2f}%\".format(F1*100))\n",
    "        print(\"Precision: {:.2f}%\".format(precision*100))\n",
    "        print(\"Recall: {:.2f}%\".format(recall*100))\n",
    "\n",
    "    y_predFull = []\n",
    "    y_trueest = []\n",
    "    return accuracy, F1, precision, recall\n",
    "\n",
    "dqn_eval(dqn,d=(X_train,y_train))\n",
    "print('***************************************************************************************')\n",
    "dqn_eval(dqn,d=(X_test,y_test))\n",
    "print('***************************************************************************************')\n",
    "\n",
    "def dqn_eval2(d,split=10):\n",
    "     \n",
    "    n_splits = split\n",
    "    mean_rewards_per_fold = []\n",
    "    acc_per_fold = []\n",
    "    f1_mean, p_mean, r_mean = [], [], []\n",
    "    # Loop over each fold\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    train, test = d\n",
    "    for train_index, test_index in skf.split(train,test):\n",
    "        xtr, xte = train[train_index],train[test_index]\n",
    "        ytr, yte = test[train_index], test[test_index]\n",
    "\n",
    "        test_env = Plasticity(dataset = (xte, yte),random=True)\n",
    "        \n",
    "        scores = dqn.test(test_env, nb_episodes=10, visualize=False, verbose=0)\n",
    "        acc, f1, p, r = dqn_eval(dqn,d=(xte, yte))\n",
    "        \n",
    "        mean_reward = np.mean(scores.history['episode_reward'])\n",
    "        mean_rewards_per_fold.append(mean_reward)\n",
    "        acc_per_fold.append(acc)\n",
    "        f1_mean.append(f1)\n",
    "        r_mean.append(r)\n",
    "        p_mean.append(p)\n",
    "   \n",
    "    overall_mean_reward = np.mean(mean_rewards_per_fold)\n",
    "\n",
    "    #print(acc_per_fold)\n",
    "    \n",
    "    f1_avg, p_avg, r_avg = np.mean(f1_mean), np.mean(p_mean), np.mean(r_mean)\n",
    "    #print(mean_rewards_per_fold)\n",
    "    print(f\"Overall Mean Reward across all folds: {overall_mean_reward * 100} %\")\n",
    "    acc_avg = np.mean(acc_per_fold)\n",
    "    print(f\"Overall Mean Accuracy across all folds: {acc_avg * 100} %\")\n",
    "    #print(f1_mean)\n",
    "    print(f\"Overall Mean F1 across all folds: {f1_avg * 100} %\")\n",
    "    #print(p_mean)\n",
    "    print(f\"Overall Mean Precision across all folds: {p_avg * 100} %\")\n",
    "    #print(r_mean)\n",
    "    print(f\"Overall Mean Recall across all folds: {r_avg * 100} %\")\n",
    "#dqn_eval2(d=(X_train,y_train))\n",
    "\n",
    "dqn_eval2(d=(features_array,label_array))\n",
    "#dqn_eval(dqn,d=(features_array,label_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dqn_eval(dqn,d=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6Y88fS5sUGv",
    "outputId": "b6587b9d-653d-4b44-dd1e-1daba16a9830"
   },
   "outputs": [],
   "source": [
    "scores = dqn.test(Plasticity(images_per_episode=1,random=True,dataset=(X_test,y_test)), callbacks=[test_logger],nb_episodes=30, visualize=False, action_repetition=1, verbose=1)\n",
    "print(np.mean(scores.history['episode_reward'])*100,'%')\n",
    "plt.plot(scores.history['episode_reward'])\n",
    "plt.title('Testing Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.show()\n",
    "print(np.mean(scores.history['episode_reward'])*100,'%')\n",
    "print(label_array,label_array.shape)\n",
    "\n",
    "dqn_eval(dqn,d=(features_array,label_array))\n",
    "def moving_average(data, window_size):\n",
    "    return [np.mean(data[i:i+window_size]) for i in range(len(data) - window_size + 1)]\n",
    "losses = loss_history.losses\n",
    "losses2 = loss_history2.losses\n",
    "smoothed_losses = moving_average(losses, window_size=50)\n",
    "print(smoothed_losses)\n",
    "plt.plot(smoothed_losses)\n",
    "plt.title('Smoothed Training Loss per Step')\n",
    "plt.xlabel('Step')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Smoothed Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "smoothed_losses2 = moving_average(losses2, window_size=70)\n",
    "print(smoothed_losses2)\n",
    "plt.plot(smoothed_losses2)\n",
    "plt.title('Smoothed Training Loss per Step')\n",
    "plt.xlabel('Step')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Smoothed Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(moving_average(reward_history.rewards, window_size=70))\n",
    "plt.title('Training Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.show()\n",
    "\n",
    "print(\"___________________________________________________________________________________________________________________________\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TheVirtualEnv",
   "language": "python",
   "name": "thevirtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
