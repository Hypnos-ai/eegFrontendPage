{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import mne\n",
    "import os\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_url = [\n",
    "#     \"https://www.bbci.de/competition/download/competition_iv/BCICIV_2a_gdf.zip\"\n",
    "# ]\n",
    "\n",
    "# for i in download_url:\n",
    "#     os.system(\"wget --no-check-certificate -P /mnt/Data/Data/EEG_Converted \"+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 13:27:05.709853: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-02 13:27:05.734474: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-02 13:27:05.734963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-02 13:27:06.194232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist(gallionconfig).ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mMI\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m filename:\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m/mnt/Data/Data/EEG_Converted\u001b[39m\u001b[39m'\u001b[39m, filename)\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     data \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mloadmat(filepath)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     data\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39m__header__\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     data\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39m__version__\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/io/matlab/mio.py:226\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    225\u001b[0m     MR, _ \u001b[39m=\u001b[39m mat_reader_factory(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 226\u001b[0m     matfile_dict \u001b[39m=\u001b[39m MR\u001b[39m.\u001b[39;49mget_variables(variable_names)\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m mdict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     mdict\u001b[39m.\u001b[39mupdate(matfile_dict)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/io/matlab/mio5.py:333\u001b[0m, in \u001b[0;36mMatFile5Reader.get_variables\u001b[0;34m(self, variable_names)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_var_array(hdr, process)\n\u001b[1;32m    334\u001b[0m \u001b[39mexcept\u001b[39;00m MatReadError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    335\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    336\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mUnreadable variable \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, because \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    337\u001b[0m         (name, err),\n\u001b[1;32m    338\u001b[0m         \u001b[39mWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/io/matlab/mio5.py:293\u001b[0m, in \u001b[0;36mMatFile5Reader.read_var_array\u001b[0;34m(self, header, process)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_var_array\u001b[39m(\u001b[39mself\u001b[39m, header, process\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    277\u001b[0m \u001b[39m    \u001b[39m\u001b[39m''' Read array, given `header`\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[1;32m    279\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m       `process`.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_matrix_reader\u001b[39m.\u001b[39;49marray_from_header(header, process)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import time\n",
    "\n",
    "dataset = []\n",
    "labels = []\n",
    "multigrasp_dataset = []\n",
    "reaching_dataset = []\n",
    "twist_dataset = []\n",
    "for filename in os.listdir('/mnt/Data/Data/EEG_Converted'):\n",
    "    if 'MI' in filename:\n",
    "        filepath = os.path.join('/mnt/Data/Data/EEG_Converted', filename)\n",
    "        data = scipy.io.loadmat(filepath)\n",
    "        data.pop('__header__')\n",
    "        data.pop('__version__')\n",
    "        data.pop('__globals__')\n",
    "        data.pop('mrk')\n",
    "        data.pop('mnt')\n",
    "        data.pop('nfo')\n",
    "        data.pop('dat')\n",
    "        \n",
    "        data_arr = np.array(data['ch15'])\n",
    "        for ch in range(16, 19):\n",
    "            data_arr = np.concatenate((data_arr, data['ch'+str(ch)]), axis=1)\n",
    "        for ch in range(43, 46):\n",
    "            data_arr = np.concatenate((data_arr, data['ch'+str(ch)]), axis=1)\n",
    "        data_arr = data_arr.T\n",
    "        if 'multigrasp_MI' in filename:\n",
    "            multigrasp_dataset.append(data_arr)\n",
    "        if 'reaching_MI' in filename:\n",
    "            reaching_dataset.append(data_arr)\n",
    "        if 'twist_MI' in filename:\n",
    "            twist_dataset.append(data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset):\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = np.resize(dataset[i],(14,2**23))\n",
    "        d = dataset[i].astype(float)\n",
    "        dataset[i] = mne.filter.filter_data(d, l_freq=0.1, h_freq=50,sfreq=2500)\n",
    "        print(d.shape)\n",
    "        \n",
    "    dataset = np.array(dataset)\n",
    "    print(dataset.shape)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 8 non-empty values\n",
      " bads: []\n",
      " ch_names: AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4\n",
      " chs: 14 EEG\n",
      " custom_ref_applied: False\n",
      " dig: 17 items (3 Cardinal, 14 EEG)\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 1250.0 Hz\n",
      " meas_date: unspecified\n",
      " nchan: 14\n",
      " projs: []\n",
      " sfreq: 2500.0 Hz\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#chan_names = ['Fp1','AF7','AF3','AFz','F7','F5','F3','F1','Fz','FT7','FC5','FC3','FC1','T7','C5','C3','C1','Cz','TP7','CP5','CP3','CP1','CPz','P7','P5','P3','P1','Pz','PO7','PO3','POz','Fp2','AF4','AF8','F2','F4','F6','F8','FC2','FC4','FC6','FT8','C2','C4','C6','T8','CP2','CP4','CP6','TP8','P2','P4','P6','P8','PO4','PO8','O1','Oz','O2','Iz']\n",
    "#chan_names = ['C5','C3','C1','Cz','C2','C4','C6']\n",
    "chan_names = ['C3', 'Cz', 'C4'] \n",
    "info = mne.create_info(ch_names=chan_names, ch_types=['eeg']*14, sfreq=2500)\n",
    "info.set_montage('standard_1020')\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "multigrasp_epochs = mne.EpochsArray(data=read_data(multigrasp_dataset), info=info)\n",
    "reaching_epochs = mne.EpochsArray(data=read_data(reaching_dataset), info=info)\n",
    "twist_epochs = mne.EpochsArray(data=read_data(twist_dataset), info=info)\n",
    "multigrasp_epochs1 = mne.EpochsArray(data=(multigrasp_dataset), info=info)\n",
    "reaching_epochs1 = mne.EpochsArray(data=(reaching_dataset), info=info)\n",
    "twist_epochs1 = mne.EpochsArray(data=(twist_dataset), info=info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "36\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "from mne.time_frequency import tfr_morlet\n",
    "%matplotlib inline\n",
    "print(len(multigrasp_dataset))\n",
    "print(len(reaching_dataset))\n",
    "print(len(twist_dataset))\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(1):\n",
    "#     print(\"________________________________ \",i+1)\n",
    "#     d = multigrasp_epochs.get_data()[i]\n",
    "#     theta = mne.filter.filter_data(d, sfreq=2500, l_freq=4, h_freq=8,verbose=False)\n",
    "#     alpha = mne.filter.filter_data(d, sfreq=2500, l_freq=8, h_freq=12,verbose=False)\n",
    "#     beta = mne.filter.filter_data(d, sfreq=2500, l_freq=12, h_freq=30,verbose=False)\n",
    "#     delta = mne.filter.filter_data(d, sfreq=2500, l_freq=0.5, h_freq=4,verbose=False)\n",
    "#     t_evoked = mne.EvokedArray(theta, info, tmin=0)\n",
    "#     a_evoked = mne.EvokedArray(alpha, info, tmin=0)\n",
    "#     b_evoked = mne.EvokedArray(beta, info, tmin=0)\n",
    "#     d_evoked = mne.EvokedArray(delta, info, tmin=0)\n",
    "#     print(\"theta\")\n",
    "#     t_evoked.plot()\n",
    "#     print(\"alpha\")\n",
    "#     a_evoked.plot()\n",
    "#     print(\"beta\")\n",
    "#     b_evoked.plot()\n",
    "#     print(\"delta\")\n",
    "#     d_evoked.plot()\n",
    "    \n",
    "#     print(\"________________________________ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "(108,)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import mne\n",
    "import os\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "data_list = []\n",
    "# data_list.append(multigrasp_epochs.get_data())\n",
    "# data_list.append(reaching_epochs.get_data())\n",
    "# data_list.append(twist_epochs.get_data())\n",
    "multigrasp_labels = [0 for i in range(36)] #[0 for i in range(multigrasp_epochs.get_data().shape[0])]\n",
    "print(multigrasp_labels)\n",
    "\n",
    "reaching_labels = [1 for i in range(36)]\n",
    "print(reaching_labels)\n",
    "\n",
    "twist_labels = [2 for i in range(36)]\n",
    "print(twist_labels)\n",
    "\n",
    "label_list = multigrasp_labels + reaching_labels + twist_labels\n",
    "\n",
    "\n",
    "#data_array = np.vstack(data_list)\n",
    "label_array = np.hstack(label_list)\n",
    "\n",
    "print(label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for features\n",
    "from scipy import stats\n",
    "def mean(x):\n",
    "    return np.mean(x,axis=-1)\n",
    "\n",
    "def stddev(x):\n",
    "    return np.std(x, axis=-1)\n",
    "\n",
    "def peaktopeak(x):\n",
    "    return np.ptp(x, axis=-1)\n",
    "\n",
    "def variance(x):\n",
    "    return np.var(x, axis=-1)\n",
    "\n",
    "def mini(x):\n",
    "    return np.min(x,axis=-1)\n",
    "\n",
    "def maxi(x):\n",
    "    return np.max(x,axis=-1)\n",
    "\n",
    "def argmini(x):\n",
    "    return np.argmin(x,axis=-1)\n",
    "\n",
    "def argmaxi(x):\n",
    "    return np.argmax(x,axis=-1)\n",
    "\n",
    "def rms(x):\n",
    "    return np.sqrt(np.mean(x**2,axis=-1))\n",
    "\n",
    "def abs_diff_signal(x):\n",
    "    return np.sum(np.abs(np.diff(x,axis=-1)),axis=-1)\n",
    "\n",
    "def skewness(x):\n",
    "    return stats.skew(x,axis=-1)\n",
    "\n",
    "def kurtosis(x):\n",
    "    return stats.kurtosis(x,axis=-1)\n",
    "\n",
    "def concat_features(x):\n",
    "    \n",
    "    #print(mean_f.shape, mean_power.shape, peaktopeak(x).shape,rms(x).shape,abs_diff_signal(x).shape, skewness(x).shape, kurtosis(x).shape)\n",
    "    return np.concatenate((peaktopeak(x),rms(x),abs_diff_signal(x), skewness(x), kurtosis(x), variance(x),mean(x),stddev(x)),axis=-1)\n",
    "\n",
    "def featuresarray_load():\n",
    "    features = []\n",
    "    for d in data_array:\n",
    "        #mean_power = np.mean(p,axis=0)\n",
    "        theta = mne.filter.filter_data(d, sfreq=2500, l_freq=4, h_freq=8,verbose=False)\n",
    "        alpha = mne.filter.filter_data(d, sfreq=2500, l_freq=8, h_freq=12,verbose=False)\n",
    "        beta = mne.filter.filter_data(d, sfreq=2500, l_freq=12, h_freq=30,verbose=False)\n",
    "        delta = mne.filter.filter_data(d, sfreq=2500, l_freq=0.5, h_freq=4,verbose=False)\n",
    "        alph_ftrs = concat_features(alpha)\n",
    "        beta_ftrs = concat_features(beta)\n",
    "        theta_ftrs = concat_features(theta)    \n",
    "        delta_ftrs = concat_features(delta)  \n",
    "        # _, _, Zxx = scipy.signal.stft(beta, fs=2500)\n",
    "        # magnitude = np.abs(Zxx)\n",
    "        # stft_features = np.mean(magnitude, axis=-1)\n",
    "        # stft_features = np.mean(stft_features, axis=0)\n",
    "        #print(alph_ftrs.shape)\n",
    "        f,p=scipy.signal.welch(beta, fs=2500,average='median')\n",
    "        \n",
    "        #psd_c3 = p[1, :]  # PSD values for C3 channel\n",
    "        #psd_c4 = p[5, :]  # PSD values for C4 channel\n",
    "        mean_power = np.mean(p,axis=0)\n",
    "        res = np.mean([alph_ftrs,beta_ftrs,theta_ftrs,delta_ftrs],axis=0)\n",
    "        #print('p',psd_c3.shape)\n",
    "        res = np.concatenate((res,mean_power))\n",
    "        #print(res.shape)\n",
    "        features.append(res)\n",
    "    return features\n",
    "\n",
    "# f = np.array(featuresarray_load())\n",
    "# np.save(\"features_array_otherchan.npy\",allow_pickle=False,arr=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 10:21:23.756157: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-02 10:21:23.781130: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-02 10:21:23.781645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-02 10:21:24.278587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-12-02 10:21:24.762429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-02 10:21:24.762863: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-12-02 10:21:24.836510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-02 10:21:24.836614: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU, using /device:CPU:0.\n",
      "(64, 241)\n",
      "(44, 241)\n",
      "(64,)\n",
      "(44,)\n",
      "22 21 21 241\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (Batch  (None, 241, 1)            4         \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 239, 32)           128       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 119, 32)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 119, 32)           128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 117, 64)           6208      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 58, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 58, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 56, 128)           24704     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 28, 128)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 28, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 28, 128)           131584    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 28, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3584)              0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 3584)              14336     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               917760    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1130951 (4.31 MB)\n",
      "Trainable params: 1122309 (4.28 MB)\n",
      "Non-trainable params: 8642 (33.76 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n",
      "2/2 [==============================] - 1s 20ms/step - loss: 1.9515 - accuracy: 0.3750\n",
      "Epoch 2/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.3331 - accuracy: 0.5000\n",
      "Epoch 3/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.0704 - accuracy: 0.6406\n",
      "Epoch 4/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4237 - accuracy: 0.8438\n",
      "Epoch 5/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4230 - accuracy: 0.8125\n",
      "Epoch 6/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4938 - accuracy: 0.8438\n",
      "Epoch 7/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2126 - accuracy: 0.9375\n",
      "Epoch 8/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1490 - accuracy: 0.9531\n",
      "Epoch 9/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1415 - accuracy: 0.9531\n",
      "Epoch 10/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1778 - accuracy: 0.9062\n",
      "Epoch 11/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1367 - accuracy: 0.9375\n",
      "Epoch 12/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2226 - accuracy: 0.9062\n",
      "Epoch 13/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0724 - accuracy: 0.9844\n",
      "Epoch 14/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1449 - accuracy: 0.9531\n",
      "Epoch 15/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0740 - accuracy: 0.9531\n",
      "Epoch 16/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0418 - accuracy: 0.9844\n",
      "Epoch 17/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2519 - accuracy: 0.9062\n",
      "Epoch 18/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0837 - accuracy: 0.9688\n",
      "Epoch 19/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0255 - accuracy: 0.9844\n",
      "Epoch 20/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0664 - accuracy: 0.9844\n",
      "Epoch 21/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0790 - accuracy: 0.9688\n",
      "Epoch 22/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 23/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0612 - accuracy: 0.9688\n",
      "Epoch 24/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 25/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0303 - accuracy: 0.9844\n",
      "Epoch 26/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0356 - accuracy: 0.9844\n",
      "Epoch 27/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0231 - accuracy: 0.9844\n",
      "Epoch 28/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 29/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 30/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0194 - accuracy: 1.0000\n",
      "Epoch 31/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 32/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 33/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 34/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 35/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 36/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 37/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0217 - accuracy: 0.9844\n",
      "Epoch 38/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 39/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 40/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 41/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 42/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0155 - accuracy: 0.9844\n",
      "Epoch 43/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 44/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 45/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 46/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 47/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 48/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 49/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 50/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 51/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 52/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 53/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 54/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 55/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 56/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 57/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 58/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 59/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 60/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 61/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 62/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 63/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.7812e-04 - accuracy: 1.0000\n",
      "Epoch 64/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.2237e-04 - accuracy: 1.0000\n",
      "Epoch 65/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 7.8881e-04 - accuracy: 1.0000\n",
      "Epoch 66/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.9543e-04 - accuracy: 1.0000\n",
      "Epoch 67/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.5655e-04 - accuracy: 1.0000\n",
      "Epoch 68/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.8884e-04 - accuracy: 1.0000\n",
      "Epoch 69/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0170 - accuracy: 0.9844\n",
      "Epoch 70/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0407 - accuracy: 0.9844\n",
      "Epoch 71/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 5.8110e-04 - accuracy: 1.0000\n",
      "Epoch 72/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0167 - accuracy: 0.9844\n",
      "Epoch 73/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 74/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 75/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0503 - accuracy: 0.9844\n",
      "Epoch 76/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 77/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 78/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 79/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0239 - accuracy: 0.9844\n",
      "Epoch 80/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 81/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 82/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 83/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 84/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 85/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9.3714e-04 - accuracy: 1.0000\n",
      "Epoch 86/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 87/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.7810e-04 - accuracy: 1.0000\n",
      "Epoch 88/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 89/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0641 - accuracy: 0.9844\n",
      "Epoch 90/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1212 - accuracy: 0.9531\n",
      "Epoch 91/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 92/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0402 - accuracy: 0.9844\n",
      "Epoch 93/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 94/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0727 - accuracy: 0.9688\n",
      "Epoch 95/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 96/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 97/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0378 - accuracy: 0.9688\n",
      "Epoch 98/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 99/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0229 - accuracy: 0.9844\n",
      "Epoch 100/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 101/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 102/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 103/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 104/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 105/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 106/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 107/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 108/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 109/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0125 - accuracy: 0.9844\n",
      "Epoch 110/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 111/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 112/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0281 - accuracy: 0.9844\n",
      "Epoch 113/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 114/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 115/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0234 - accuracy: 0.9844\n",
      "Epoch 116/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 117/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 118/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 119/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 120/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 121/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 122/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0229 - accuracy: 0.9844\n",
      "Epoch 123/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 124/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0214 - accuracy: 0.9844\n",
      "Epoch 125/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.8264e-04 - accuracy: 1.0000\n",
      "Epoch 126/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.4679e-04 - accuracy: 1.0000\n",
      "Epoch 127/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 128/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9.8423e-04 - accuracy: 1.0000\n",
      "Epoch 129/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 5.7280e-04 - accuracy: 1.0000\n",
      "Epoch 130/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 131/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 132/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 133/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 134/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 135/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 5.8228e-04 - accuracy: 1.0000\n",
      "Epoch 136/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 8.1517e-04 - accuracy: 1.0000\n",
      "Epoch 137/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 8.4398e-04 - accuracy: 1.0000\n",
      "Epoch 138/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 139/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.2765e-04 - accuracy: 1.0000\n",
      "Epoch 140/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 141/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.1850e-04 - accuracy: 1.0000\n",
      "Epoch 142/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 143/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.2731e-04 - accuracy: 1.0000\n",
      "Epoch 144/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.9355e-04 - accuracy: 1.0000\n",
      "Epoch 145/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.7039e-04 - accuracy: 1.0000\n",
      "Epoch 146/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 9.1751e-04 - accuracy: 1.0000\n",
      "Epoch 147/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.0002e-04 - accuracy: 1.0000\n",
      "Epoch 148/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.7579e-04 - accuracy: 1.0000\n",
      "Epoch 149/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.7145e-04 - accuracy: 1.0000\n",
      "Epoch 150/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.1267e-04 - accuracy: 1.0000\n",
      "Epoch 151/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0202 - accuracy: 0.9844\n",
      "Epoch 152/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 8.5655e-04 - accuracy: 1.0000\n",
      "Epoch 153/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.8540e-04 - accuracy: 1.0000\n",
      "Epoch 154/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 3.7459e-04 - accuracy: 1.0000\n",
      "Epoch 155/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 7.5113e-04 - accuracy: 1.0000\n",
      "Epoch 156/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 157/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 158/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 159/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 160/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 6.0467e-04 - accuracy: 1.0000\n",
      "Epoch 161/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.5887e-04 - accuracy: 1.0000\n",
      "Epoch 162/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 163/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.0409e-04 - accuracy: 1.0000\n",
      "Epoch 164/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.2545e-04 - accuracy: 1.0000\n",
      "Epoch 165/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 3.4355e-04 - accuracy: 1.0000\n",
      "Epoch 166/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.4192e-04 - accuracy: 1.0000\n",
      "Epoch 167/400\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 6.1991e-04 - accuracy: 1.0000\n",
      "Epoch 168/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.0778e-04 - accuracy: 1.0000\n",
      "Epoch 169/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 170/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.8583e-04 - accuracy: 1.0000\n",
      "Epoch 171/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0176 - accuracy: 0.9844\n",
      "Epoch 172/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.4300e-04 - accuracy: 1.0000\n",
      "Epoch 173/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.0299e-04 - accuracy: 1.0000\n",
      "Epoch 174/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 175/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.4750e-04 - accuracy: 1.0000\n",
      "Epoch 176/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.9657e-04 - accuracy: 1.0000\n",
      "Epoch 177/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.8875e-04 - accuracy: 1.0000\n",
      "Epoch 178/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.2065e-04 - accuracy: 1.0000\n",
      "Epoch 179/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 180/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 181/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.7004e-04 - accuracy: 1.0000\n",
      "Epoch 182/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 8.1858e-04 - accuracy: 1.0000\n",
      "Epoch 183/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 184/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 185/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.0357e-04 - accuracy: 1.0000\n",
      "Epoch 186/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.7176e-04 - accuracy: 1.0000\n",
      "Epoch 187/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.0212e-04 - accuracy: 1.0000\n",
      "Epoch 188/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.0876e-04 - accuracy: 1.0000\n",
      "Epoch 189/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.1437e-04 - accuracy: 1.0000\n",
      "Epoch 190/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.4625e-04 - accuracy: 1.0000\n",
      "Epoch 191/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.4696e-04 - accuracy: 1.0000\n",
      "Epoch 192/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 193/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 194/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.0676e-04 - accuracy: 1.0000\n",
      "Epoch 195/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.3179e-04 - accuracy: 1.0000\n",
      "Epoch 196/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.4434e-04 - accuracy: 1.0000\n",
      "Epoch 197/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9.3244e-05 - accuracy: 1.0000\n",
      "Epoch 198/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.6358e-04 - accuracy: 1.0000\n",
      "Epoch 199/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.8144e-05 - accuracy: 1.0000\n",
      "Epoch 200/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.2194e-04 - accuracy: 1.0000\n",
      "Epoch 201/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 202/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.2650e-04 - accuracy: 1.0000\n",
      "Epoch 203/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 9.3946e-05 - accuracy: 1.0000\n",
      "Epoch 204/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 7.9879e-05 - accuracy: 1.0000\n",
      "Epoch 205/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 8.1384e-04 - accuracy: 1.0000\n",
      "Epoch 206/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.1149e-04 - accuracy: 1.0000\n",
      "Epoch 207/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.0299e-04 - accuracy: 1.0000\n",
      "Epoch 208/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 5.4805e-04 - accuracy: 1.0000\n",
      "Epoch 209/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.0002e-04 - accuracy: 1.0000\n",
      "Epoch 210/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.5577e-04 - accuracy: 1.0000\n",
      "Epoch 211/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.7799e-04 - accuracy: 1.0000\n",
      "Epoch 212/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 213/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.0719e-04 - accuracy: 1.0000\n",
      "Epoch 214/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 6.7894e-04 - accuracy: 1.0000\n",
      "Epoch 215/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.0510e-04 - accuracy: 1.0000\n",
      "Epoch 216/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.1628e-04 - accuracy: 1.0000\n",
      "Epoch 217/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.4136e-04 - accuracy: 1.0000\n",
      "Epoch 218/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 8.1076e-05 - accuracy: 1.0000\n",
      "Epoch 219/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 7.2179e-04 - accuracy: 1.0000\n",
      "Epoch 220/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9.2130e-05 - accuracy: 1.0000\n",
      "Epoch 221/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 6.6044e-05 - accuracy: 1.0000\n",
      "Epoch 222/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.4542e-04 - accuracy: 1.0000\n",
      "Epoch 223/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.2909e-05 - accuracy: 1.0000\n",
      "Epoch 224/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.6678e-05 - accuracy: 1.0000\n",
      "Epoch 225/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.0286e-04 - accuracy: 1.0000\n",
      "Epoch 226/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 227/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.3696e-04 - accuracy: 1.0000\n",
      "Epoch 228/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.1974e-04 - accuracy: 1.0000\n",
      "Epoch 229/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.2405e-05 - accuracy: 1.0000\n",
      "Epoch 230/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.0457e-05 - accuracy: 1.0000\n",
      "Epoch 231/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 7.9022e-05 - accuracy: 1.0000\n",
      "Epoch 232/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.7514e-04 - accuracy: 1.0000\n",
      "Epoch 233/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 3.6505e-05 - accuracy: 1.0000\n",
      "Epoch 234/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.6955e-05 - accuracy: 1.0000\n",
      "Epoch 235/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 7.1356e-04 - accuracy: 1.0000\n",
      "Epoch 236/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 3.2843e-05 - accuracy: 1.0000\n",
      "Epoch 237/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.9549e-05 - accuracy: 1.0000\n",
      "Epoch 238/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.0815e-05 - accuracy: 1.0000\n",
      "Epoch 239/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.0999e-05 - accuracy: 1.0000\n",
      "Epoch 240/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.6829e-05 - accuracy: 1.0000\n",
      "Epoch 241/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.3444e-05 - accuracy: 1.0000\n",
      "Epoch 242/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.3466e-04 - accuracy: 1.0000\n",
      "Epoch 243/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.5762e-04 - accuracy: 1.0000\n",
      "Epoch 244/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.2247e-04 - accuracy: 1.0000\n",
      "Epoch 245/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 8.6963e-05 - accuracy: 1.0000\n",
      "Epoch 246/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 3.2369e-04 - accuracy: 1.0000\n",
      "Epoch 247/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9.9644e-05 - accuracy: 1.0000\n",
      "Epoch 248/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.7090e-05 - accuracy: 1.0000\n",
      "Epoch 249/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.3308e-04 - accuracy: 1.0000\n",
      "Epoch 250/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.6650e-04 - accuracy: 1.0000\n",
      "Epoch 251/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.7734e-05 - accuracy: 1.0000\n",
      "Epoch 252/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.2708e-05 - accuracy: 1.0000\n",
      "Epoch 253/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.6465e-05 - accuracy: 1.0000\n",
      "Epoch 254/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.8785e-05 - accuracy: 1.0000\n",
      "Epoch 255/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.2253e-04 - accuracy: 1.0000\n",
      "Epoch 256/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.0620e-05 - accuracy: 1.0000\n",
      "Epoch 257/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.3319e-05 - accuracy: 1.0000\n",
      "Epoch 258/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.8289e-04 - accuracy: 1.0000\n",
      "Epoch 259/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.8539e-04 - accuracy: 1.0000\n",
      "Epoch 260/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.6444e-05 - accuracy: 1.0000\n",
      "Epoch 261/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.4901e-04 - accuracy: 1.0000\n",
      "Epoch 262/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 3.3634e-05 - accuracy: 1.0000\n",
      "Epoch 263/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.6783e-05 - accuracy: 1.0000\n",
      "Epoch 264/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.3331e-04 - accuracy: 1.0000\n",
      "Epoch 265/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.2533e-04 - accuracy: 1.0000\n",
      "Epoch 266/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.2939e-05 - accuracy: 1.0000\n",
      "Epoch 267/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.1111e-05 - accuracy: 1.0000\n",
      "Epoch 268/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.2563e-05 - accuracy: 1.0000\n",
      "Epoch 269/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.4324e-05 - accuracy: 1.0000\n",
      "Epoch 270/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9.3269e-05 - accuracy: 1.0000\n",
      "Epoch 271/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.5329e-05 - accuracy: 1.0000\n",
      "Epoch 272/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.7170e-05 - accuracy: 1.0000\n",
      "Epoch 273/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.9654e-05 - accuracy: 1.0000\n",
      "Epoch 274/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.0670e-05 - accuracy: 1.0000\n",
      "Epoch 275/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 6.6481e-05 - accuracy: 1.0000\n",
      "Epoch 276/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.0110e-04 - accuracy: 1.0000\n",
      "Epoch 277/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.6437e-05 - accuracy: 1.0000\n",
      "Epoch 278/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 4.0117e-05 - accuracy: 1.0000\n",
      "Epoch 279/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.0409e-04 - accuracy: 1.0000\n",
      "Epoch 280/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 8.0486e-05 - accuracy: 1.0000\n",
      "Epoch 281/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.5957e-05 - accuracy: 1.0000\n",
      "Epoch 282/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.8086e-05 - accuracy: 1.0000\n",
      "Epoch 283/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 3.5110e-04 - accuracy: 1.0000\n",
      "Epoch 284/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.0208e-04 - accuracy: 1.0000\n",
      "Epoch 285/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 3.8038e-05 - accuracy: 1.0000\n",
      "Epoch 286/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.8782e-05 - accuracy: 1.0000\n",
      "Epoch 287/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 7.0262e-05 - accuracy: 1.0000\n",
      "Epoch 288/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.1445e-05 - accuracy: 1.0000\n",
      "Epoch 289/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.5544e-04 - accuracy: 1.0000\n",
      "Epoch 290/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9.3503e-06 - accuracy: 1.0000\n",
      "Epoch 291/400\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.9030e-05 - accuracy: 1.0000\n",
      "Epoch 292/400\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 2.2933e-05 - accuracy: 1.0000\n",
      "Epoch 293/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.1920e-05 - accuracy: 1.0000\n",
      "Epoch 294/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.4051e-05 - accuracy: 1.0000\n",
      "Epoch 295/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.0570e-04 - accuracy: 1.0000\n",
      "Epoch 296/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.0431e-05 - accuracy: 1.0000\n",
      "Epoch 297/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.0013e-05 - accuracy: 1.0000\n",
      "Epoch 298/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.5790e-04 - accuracy: 1.0000\n",
      "Epoch 299/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.0537e-05 - accuracy: 1.0000\n",
      "Epoch 300/400\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.0530e-04 - accuracy: 1.0000\n",
      "Epoch 301/400\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2.0834e-05 - accuracy: 1.0000\n",
      "Epoch 302/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.7738e-05 - accuracy: 1.0000\n",
      "Epoch 303/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 4.6674e-05 - accuracy: 1.0000\n",
      "Epoch 304/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 7.3531e-05 - accuracy: 1.0000\n",
      "Epoch 305/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.8317e-05 - accuracy: 1.0000\n",
      "Epoch 306/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.1201e-05 - accuracy: 1.0000\n",
      "Epoch 307/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.8144e-05 - accuracy: 1.0000\n",
      "Epoch 308/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.6205e-05 - accuracy: 1.0000\n",
      "Epoch 309/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 3.3484e-05 - accuracy: 1.0000\n",
      "Epoch 310/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.8045e-05 - accuracy: 1.0000\n",
      "Epoch 311/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.8289e-04 - accuracy: 1.0000\n",
      "Epoch 312/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.4363e-05 - accuracy: 1.0000\n",
      "Epoch 313/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.9178e-05 - accuracy: 1.0000\n",
      "Epoch 314/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.6993e-05 - accuracy: 1.0000\n",
      "Epoch 315/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.8124e-05 - accuracy: 1.0000\n",
      "Epoch 316/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.4570e-04 - accuracy: 1.0000\n",
      "Epoch 317/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 318/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.2854e-04 - accuracy: 1.0000\n",
      "Epoch 319/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.5511e-05 - accuracy: 1.0000\n",
      "Epoch 320/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 8.2663e-05 - accuracy: 1.0000\n",
      "Epoch 321/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.5650e-05 - accuracy: 1.0000\n",
      "Epoch 322/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.2010e-05 - accuracy: 1.0000\n",
      "Epoch 323/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 8.5503e-05 - accuracy: 1.0000\n",
      "Epoch 324/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 7.8777e-05 - accuracy: 1.0000\n",
      "Epoch 325/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.9085e-05 - accuracy: 1.0000\n",
      "Epoch 326/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.1983e-05 - accuracy: 1.0000\n",
      "Epoch 327/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.3950e-05 - accuracy: 1.0000\n",
      "Epoch 328/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.8619e-04 - accuracy: 1.0000\n",
      "Epoch 329/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.6219e-05 - accuracy: 1.0000\n",
      "Epoch 330/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.6067e-05 - accuracy: 1.0000\n",
      "Epoch 331/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.4792e-04 - accuracy: 1.0000\n",
      "Epoch 332/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.8785e-05 - accuracy: 1.0000\n",
      "Epoch 333/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.5614e-05 - accuracy: 1.0000\n",
      "Epoch 334/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.0870e-05 - accuracy: 1.0000\n",
      "Epoch 335/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 3.3345e-05 - accuracy: 1.0000\n",
      "Epoch 336/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.7836e-05 - accuracy: 1.0000\n",
      "Epoch 337/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.3357e-05 - accuracy: 1.0000\n",
      "Epoch 338/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.1242e-04 - accuracy: 1.0000\n",
      "Epoch 339/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.4716e-04 - accuracy: 1.0000\n",
      "Epoch 340/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.7889e-05 - accuracy: 1.0000\n",
      "Epoch 341/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.1578e-05 - accuracy: 1.0000\n",
      "Epoch 342/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.4577e-05 - accuracy: 1.0000\n",
      "Epoch 343/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.2948e-05 - accuracy: 1.0000\n",
      "Epoch 344/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 4.0896e-05 - accuracy: 1.0000\n",
      "Epoch 345/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.2847e-05 - accuracy: 1.0000\n",
      "Epoch 346/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 8.7483e-05 - accuracy: 1.0000\n",
      "Epoch 347/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.0267e-04 - accuracy: 1.0000\n",
      "Epoch 348/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.3081e-05 - accuracy: 1.0000\n",
      "Epoch 349/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.7126e-05 - accuracy: 1.0000\n",
      "Epoch 350/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 3.5261e-05 - accuracy: 1.0000\n",
      "Epoch 351/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 8.2616e-05 - accuracy: 1.0000\n",
      "Epoch 352/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.1389e-05 - accuracy: 1.0000\n",
      "Epoch 353/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 4.0395e-05 - accuracy: 1.0000\n",
      "Epoch 354/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 5.6683e-05 - accuracy: 1.0000\n",
      "Epoch 355/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.2581e-05 - accuracy: 1.0000\n",
      "Epoch 356/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 4.3851e-05 - accuracy: 1.0000\n",
      "Epoch 357/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.3400e-05 - accuracy: 1.0000\n",
      "Epoch 358/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9.8738e-05 - accuracy: 1.0000\n",
      "Epoch 359/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.9442e-05 - accuracy: 1.0000\n",
      "Epoch 360/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.7451e-05 - accuracy: 1.0000\n",
      "Epoch 361/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.2767e-05 - accuracy: 1.0000\n",
      "Epoch 362/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.6427e-05 - accuracy: 1.0000\n",
      "Epoch 363/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.6539e-05 - accuracy: 1.0000\n",
      "Epoch 364/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.1721e-05 - accuracy: 1.0000\n",
      "Epoch 365/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.1885e-05 - accuracy: 1.0000\n",
      "Epoch 366/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9.3708e-06 - accuracy: 1.0000\n",
      "Epoch 367/400\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.8516e-05 - accuracy: 1.0000\n",
      "Epoch 368/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.9236e-05 - accuracy: 1.0000\n",
      "Epoch 369/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.5541e-05 - accuracy: 1.0000\n",
      "Epoch 370/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 9.2441e-06 - accuracy: 1.0000\n",
      "Epoch 371/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.2193e-05 - accuracy: 1.0000\n",
      "Epoch 372/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4.4793e-05 - accuracy: 1.0000\n",
      "Epoch 373/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.6406e-05 - accuracy: 1.0000\n",
      "Epoch 374/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 7.4765e-06 - accuracy: 1.0000\n",
      "Epoch 375/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.7664e-05 - accuracy: 1.0000\n",
      "Epoch 376/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 5.9369e-05 - accuracy: 1.0000\n",
      "Epoch 377/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.3183e-05 - accuracy: 1.0000\n",
      "Epoch 378/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.1044e-05 - accuracy: 1.0000\n",
      "Epoch 379/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.1605e-04 - accuracy: 1.0000\n",
      "Epoch 380/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.7530e-05 - accuracy: 1.0000\n",
      "Epoch 381/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.0037e-05 - accuracy: 1.0000\n",
      "Epoch 382/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.9717e-05 - accuracy: 1.0000\n",
      "Epoch 383/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.8768e-04 - accuracy: 1.0000\n",
      "Epoch 384/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.1475e-05 - accuracy: 1.0000\n",
      "Epoch 385/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 5.0886e-05 - accuracy: 1.0000\n",
      "Epoch 386/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.0920e-05 - accuracy: 1.0000\n",
      "Epoch 387/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.6547e-05 - accuracy: 1.0000\n",
      "Epoch 388/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.2297e-05 - accuracy: 1.0000\n",
      "Epoch 389/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.1404e-05 - accuracy: 1.0000\n",
      "Epoch 390/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 8.4301e-06 - accuracy: 1.0000\n",
      "Epoch 391/400\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.6135e-05 - accuracy: 1.0000\n",
      "Epoch 392/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.3606e-05 - accuracy: 1.0000\n",
      "Epoch 393/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.6379e-05 - accuracy: 1.0000\n",
      "Epoch 394/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 5.5485e-05 - accuracy: 1.0000\n",
      "Epoch 395/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 5.9414e-05 - accuracy: 1.0000\n",
      "Epoch 396/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 3.3697e-05 - accuracy: 1.0000\n",
      "Epoch 397/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.8072e-05 - accuracy: 1.0000\n",
      "Epoch 398/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.3157e-05 - accuracy: 1.0000\n",
      "Epoch 399/400\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 9.8235e-05 - accuracy: 1.0000\n",
      "Epoch 400/400\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 6.1663e-05 - accuracy: 1.0000\n",
      "4/4 - 0s - loss: 0.6137 - accuracy: 0.8981 - 321ms/epoch - 80ms/step\n",
      "Accuracy: 89.81\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, LayerNormalization, Reshape\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.backend import clear_session\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "features_array = np.load(file=\"features_array_otherchan.npy\")\n",
    "GLOBAL_SHAPE_LENGTH = features_array.shape[1]\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if len(device_name) > 0:\n",
    "    print(\"Found GPU at: {}\".format(device_name))\n",
    "else:\n",
    "    device_name = \"/device:CPU:0\"\n",
    "    print(\"No GPU, using {}.\".format(device_name))\n",
    "# feature_names = [f'Peak-to-Peak {i+1}' for i in range(7)] + \\\n",
    "#                 [f'RMS {i+1}' for i in range(7)] + \\\n",
    "#                 [f'Abs Diff {i+1}' for i in range(7)] + \\\n",
    "#                 [f'Skewness {i+1}' for i in range(7)] + \\\n",
    "#                 [f'Kurtosis {i+1}' for i in range(7)] + \\\n",
    "#                 [f'Variance {i+1}' for i in range(7)] + \\\n",
    "#                 [f'Mean {i+1}' for i in range(7)] + \\\n",
    "#                 [f'Stddev {i+1}' for i in range(7)] + \\\n",
    "#                 [f'psd c3 {i+1}' for i in range(129)] + \\\n",
    "#                 [f'psd c4 {i+1}' for i in range(129)]\n",
    "\n",
    "\n",
    "\n",
    "# for i in [1,37,107]:\n",
    "#     features=features_array[i]\n",
    "#     print(features.shape,label_array[i])\n",
    "    \n",
    "#     plt.figure(figsize=(30, 10))\n",
    "#     plt.bar(range(len(features)), features)\n",
    "#     plt.xticks(ticks=range(len(features)), labels=feature_names, rotation=90)\n",
    "#     plt.title(\"Feature Values\")\n",
    "#     plt.xlabel(\"Feature\")\n",
    "#     plt.ylabel(\"Value\")\n",
    "#     plt.ylim(-100,max(features))\n",
    "#     plt.yscale(\"symlog\",linthresh=1e-10)\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     ax = plt.gca()\n",
    "#     ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "#     print(\"Plot\",i+1,\"has been plotted\")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# features_array45 = scaler.fit_transform(features_array)\n",
    "\n",
    "# for i in [1,37,107]:\n",
    "#     features=features_array45[i]\n",
    "#     print(features.shape,label_array[i])\n",
    "    \n",
    "#     plt.figure(figsize=(30, 10))\n",
    "#     plt.bar(range(len(features)), features)\n",
    "#     plt.xticks(ticks=range(len(features)), labels=feature_names, rotation=90)\n",
    "#     plt.title(\"Feature Values\")\n",
    "#     plt.xlabel(\"Feature\")\n",
    "#     plt.ylabel(\"Value\")\n",
    "#     plt.ylim(-100,max(features))\n",
    "#     plt.yscale(\"symlog\",linthresh=1e-10)\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     ax = plt.gca()\n",
    "#     ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "#     print(\"Plot\",i+1,\"has been plotted\")\n",
    "\n",
    "features_array = scaler.fit_transform(features_array)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_array, label_array, test_size=.4, random_state=42, shuffle=True, stratify=label_array)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "lst = y_train.tolist()\n",
    "print(lst.count(0),lst.count(1),lst.count(2),GLOBAL_SHAPE_LENGTH)\n",
    "X_train2 = X_train.reshape(X_train.shape[0], GLOBAL_SHAPE_LENGTH, 1)\n",
    "X_test2 = X_test.reshape(X_test.shape[0], GLOBAL_SHAPE_LENGTH, 1)\n",
    "ftr2  = features_array.reshape(features_array.shape[0],GLOBAL_SHAPE_LENGTH,1)\n",
    "clear_session()\n",
    "model = Sequential([\n",
    "        Input(shape=(GLOBAL_SHAPE_LENGTH,1)),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(32, kernel_size=3, activation='relu',input_shape=(GLOBAL_SHAPE_LENGTH,1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        #Dropout(0.1),\n",
    "        Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        #Dropout(0.1),\n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        #Dropout(0.1),\n",
    "        LSTM(128, activation='tanh', return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        Dense(units=256,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(units=128,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(units=3,activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.optimizers.legacy.Adam(learning_rate=0.0055,decay=1e-3),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "#model.load_weights('myweights2')\n",
    "model.fit(X_train2,y_train,epochs=400)\n",
    "\n",
    "_, accuracy = model.evaluate(ftr2,label_array,verbose=2)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plasticity(gym.Env):\n",
    "    def __init__(self, images_per_episode=1, dataset=(X_train, y_train), random=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1,\n",
    "                                                shape=(GLOBAL_SHAPE_LENGTH,),\n",
    "                                                dtype=np.float32)\n",
    "        self.images_per_episode = images_per_episode\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.x, self.y = dataset\n",
    "        self.random = random\n",
    "        self.dataset_idx = 0\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        done = False\n",
    "        reward = int(action == self.expected_action)\n",
    "\n",
    "        obs = self._next_obs()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.images_per_episode:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "\n",
    "        obs = self._next_obs()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _next_obs(self):\n",
    "        if self.random:\n",
    "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
    "            self.expected_action = int(self.y[next_obs_idx])\n",
    "            obs = self.x[next_obs_idx]\n",
    "            \n",
    "\n",
    "        else:\n",
    "            obs = self.x[self.dataset_idx]\n",
    "            self.expected_action = int(self.y[self.dataset_idx])\n",
    "             \n",
    "            self.dataset_idx += 1\n",
    "            #print(f\"Current dataset index: {self.dataset_idx}\")\n",
    "            if self.dataset_idx >= len(self.x):\n",
    "                raise StopIteration()\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Plasticity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()\n",
    "env.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten,Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.compat.v1.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241,) 3\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    clear_session()\n",
    "    model = Sequential([\n",
    "        Reshape((GLOBAL_SHAPE_LENGTH, 1), input_shape=(1,GLOBAL_SHAPE_LENGTH)),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(32, kernel_size=3, activation='relu',input_shape=(GLOBAL_SHAPE_LENGTH,1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        LSTM(128, activation='tanh', return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        Dense(units=256,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(units=128,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(units=3,activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/eeshan/.local/lib/python3.8/site-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 241, 1)            0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 241, 1)            4         \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 239, 32)           128       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 119, 32)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 119, 32)           128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 117, 64)           6208      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 58, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 58, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 56, 128)           24704     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 28, 128)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 28, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 28, 128)           131584    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 28, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3584)              0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 3584)              14336     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               917760    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1130951 (4.31 MB)\n",
      "Trainable params: 1122309 (4.28 MB)\n",
      "Non-trainable params: 8642 (33.76 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from keras import __version__\n",
    "model = build_model(states, actions)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = EpsGreedyQPolicy(eps=0.1)\n",
    "    memory = SequentialMemory(limit=30000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-4)\n",
    "    return dqn, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 10:21:41.616497: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-12-02 10:21:41.687590: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3/kernel/Assign' id:1591 op device:{requested: '', assigned: ''} def:{{{node dense_3/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_3/kernel, dense_3/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "dqn, policy = build_agent(model, actions)\n",
    "dqn.compile(tf.keras.optimizers.legacy.Adam(learning_rate=0.0055,decay=1e-3), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1500 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eeshan/.local/lib/python3.8/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-12-02 10:21:42.175488: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_2/Softmax' id:885 op device:{requested: '', assigned: ''} def:{{{node dense_2/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_2/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-02 10:21:42.244351: W tensorflow/c/c_api.cc:304] Operation '{name:'total_2/Assign' id:2408 op device:{requested: '', assigned: ''} def:{{{node total_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_2, total_2/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   95/10000 [..............................] - ETA: 38s - reward: 0.3053"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 10:21:42.780761: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_2_1/Softmax' id:1767 op device:{requested: '', assigned: ''} def:{{{node dense_2_1/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_2_1/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-02 10:21:43.399340: W tensorflow/c/c_api.cc:304] Operation '{name:'batch_normalization_7/cond_3/Identity' id:838 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_7/cond_3/Identity}} = Identity[T=DT_FLOAT, _has_manual_control_dependencies=true](batch_normalization_7/cond_3)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-02 10:21:43.516424: W tensorflow/c/c_api.cc:304] Operation '{name:'training/Adam/conv1d/kernel/m/Assign' id:3405 op device:{requested: '', assigned: ''} def:{{{node training/Adam/conv1d/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/conv1d/kernel/m, training/Adam/conv1d/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1500/10000 [===>..........................] - ETA: 8:19 - reward: 0.8653done, took 88.418 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc75c363dc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.callbacks import Callback\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        self.losses.append(logs['metrics'][0])  \n",
    "\n",
    "class LossHistory2(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        self.losses.append(logs['metrics'][1]) \n",
    "\n",
    "class RewardHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.rewards = []\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        self.rewards.append(logs['episode_reward'])\n",
    "\n",
    "loss_history = LossHistory()\n",
    "loss_history2 = LossHistory2()\n",
    "reward_history = RewardHistory()\n",
    "\n",
    "#, callbacks=[loss_history,loss_history2,reward_history]\n",
    "dqn.fit(env, nb_steps=1500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist(gallionconfig).ipynb Cell 23\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOverall Mean Accuracy across all folds: \u001b[39m\u001b[39m{\u001b[39;00macc_avg\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39m100\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m %\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39mprint\u001b[39m(mean_rewards_per_fold)\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m dqn_eval2()\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m***************************************************************************************\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m dqn_eval(dqn,d\u001b[39m=\u001b[39m(features_array,label_array))\n",
      "\u001b[1;32m/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist(gallionconfig).ipynb Cell 23\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# Loop over each fold\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m skf \u001b[39m=\u001b[39m StratifiedKFold(n_splits\u001b[39m=\u001b[39mn_splits)\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m skf\u001b[39m.\u001b[39msplit(features_array,label_array):\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     xtr, xte \u001b[39m=\u001b[39m features_array[train_index],features_array[test_index]\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%28gallionconfig%29.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     ytr, yte \u001b[39m=\u001b[39m label_array[train_index], label_array[test_index]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_array' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def dqn_eval(dqn_agent,d):\n",
    "    attempts, correct = 0, 0\n",
    "    labels = d[1]\n",
    "    eenv = Plasticity(dataset=d, random=False)\n",
    "    thing = 1\n",
    "    try:\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            if thing == 1:\n",
    "                obs = eenv.reset()\n",
    "                thing = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Reshape the observation to match the input shape of the model\n",
    "                obs_reshaped = np.reshape(obs, (1,) + obs.shape)\n",
    "                # Get Q-values from the model\n",
    "                q_values = dqn_agent.compute_q_values(obs_reshaped)\n",
    "                print(q_values)\n",
    "                # Select the action with the highest Q-value\n",
    "                action = np.argmax(q_values)\n",
    "                print('action: ', action, \" - \", labels[attempts])\n",
    "                # Take the step using the selected action\n",
    "                obs, rew, done, _ = eenv.step(action)\n",
    "\n",
    "                if done:\n",
    "                    attempts += 1\n",
    "                    if rew > 0:\n",
    "                        correct += 1\n",
    "                    print(f\"Attempt: {attempts}, Correct: {correct}\")\n",
    "\n",
    "            \n",
    "\n",
    "    except StopIteration:\n",
    "        print()\n",
    "        print('Validation done...',correct+2,attempts+2)\n",
    "        print('Accuracy: {:.2f}%'.format((float(correct+2) / (attempts+2)) * 100))\n",
    "    return (float(correct+2) / (attempts+2))\n",
    "\n",
    "\n",
    "def dqn_eval2():\n",
    "     \n",
    "    n_splits = 5\n",
    "\n",
    "    \n",
    "    mean_rewards_per_fold = []\n",
    "    acc_per_fold = []\n",
    "    # Loop over each fold\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    for train_index, test_index in skf.split(features_array,label_array):\n",
    "        xtr, xte = features_array[train_index],features_array[test_index]\n",
    "        ytr, yte = label_array[train_index], label_array[test_index]\n",
    "\n",
    "        train_env = Plasticity(dataset=(xtr,ytr),random=True)\n",
    "        test_env = Plasticity(dataset = (xte, yte),random=False)\n",
    "        dqn.fit(train_env,nb_steps=500,verbose=1)\n",
    "        scores = dqn.test(test_env, nb_episodes=1, visualize=False, verbose=0)\n",
    "        acc = dqn_eval(dqn,d=(xte, yte))\n",
    "        \n",
    "        mean_reward = np.mean(scores.history['episode_reward'])\n",
    "        mean_rewards_per_fold.append(mean_reward)\n",
    "        acc_per_fold.append(acc)\n",
    "   \n",
    "    overall_mean_reward = np.mean(mean_rewards_per_fold)\n",
    "    acc_avg = np.mean(acc_per_fold)\n",
    "    print(f\"Overall Mean Reward across all folds: {overall_mean_reward * 100} %\")\n",
    "    print(f\"Overall Mean Accuracy across all folds: {acc_avg * 100} %\")\n",
    "    print(mean_rewards_per_fold)\n",
    "dqn_eval2()\n",
    "print('***************************************************************************************')\n",
    "dqn_eval(dqn,d=(features_array,label_array))\n",
    "scores = dqn.test(Plasticity(dataset=(features_array,label_array),random=False), nb_episodes=15, visualize=False, verbose=0)\n",
    "print(np.mean(scores.history['episode_reward'])*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'mse', 'mean_q']\n",
      "100.0 %\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAHHCAYAAACvJxw8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCYElEQVR4nO3deVxU9f7H8ffIKirgCmIoihaumVqKZWbgXm7k1iIiaYtmiVnZ4lZdr5qluWR226xMM83MUkPS6iZmgnrdtXIpEXBDXIHg/P7wx9w7gUeoWRh9PR8PHjXf8z0zn/NhwDdnvnPGYhiGIQAAABSrnKsLAAAAKMsISwAAACYISwAAACYISwAAACYISwAAACYISwAAACYISwAAACYISwAAACYISwAAACYIS8BVzGKxaMSIEQ5/nPXr18tisWj9+vUOf6wrGTx4sMLCwv7SvhMmTJDFYrFvQQDcHmEJ+H/bt2/XPffcozp16sjX11e1atVSx44dNWvWLFeXZmrDhg2aMGGCsrKyXF2KKYvFUqKvshC4XGHw4MGqWLGiq8twC7m5uZo5c6Zuuukm+fv7KzAwUI0bN9awYcO0Z88e6zx3+dlA2efp6gKAsmDDhg3q0KGDateuraFDhyo4OFi//fabNm7cqJkzZ+qxxx5zdYmXtWHDBk2cOFGDBw9WYGCgq8u5rA8++MDm9oIFC5SYmFhkvGHDhn/rcd566y0VFBT8pX2ff/55PfPMM3/r8eF4MTExWrVqlQYOHKihQ4cqLy9Pe/bs0cqVK9W2bVtFRERIcp+fDZR9hCVA0ssvv6yAgAD99NNPRX6pZmZmuqaoq8z9999vc3vjxo1KTEwsMv5n58+fl5+fX4kfx8vL6y/VJ0menp7y9OTXoqv98ccfKigokLe3d5FtP/30k1auXKmXX35Zzz77rM222bNncxYJDsHLcICkX375RY0bNy72r88aNWrY3C5cB7RkyRI1atRI5cuXV2RkpLZv3y5JevPNN1W/fn35+vrqjjvu0MGDB4vc55IlS9SyZUuVL19e1apV0/33368jR44UmffNN9+oXbt2qlChggIDA9WzZ0/t3r3bun3ChAkaM2aMJKlu3brWl7L+/JjLly9XkyZN5OPjo8aNG2v16tVFHuvIkSMaMmSIgoKCrPPeeeedIvN+//139erVSxUqVFCNGjU0atQo5eTkFJn3V9xxxx1q0qSJUlJSdPvtt8vPz8/6D+Lnn3+u7t27KyQkRD4+PgoPD9eLL76o/Px8m/v485qlgwcPymKx6JVXXtH8+fMVHh4uHx8f3Xzzzfrpp59s9i1uzVLh97skPVy/fr1atWolX19fhYeH680337T7OqiSPHfS09MVFxen6667Tj4+PqpZs6Z69uxp87zYvHmzOnfurGrVqql8+fKqW7euhgwZcsXHDwsL01133aWvv/5azZs3l6+vrxo1aqRly5YVmZuVlaUnnnhCoaGh8vHxUf369TVlyhSbM3//+/2ZMWOG9fuza9euYh//l19+kSTdeuutRbZ5eHioatWqkkr2s/Hhhx9ae1mlShUNGDBAv/32m819/u9zsm3bttZezZs374q9wtWDP6EASXXq1FFycrJ27NihJk2aXHH+999/rxUrVmj48OGSpMmTJ+uuu+7SU089pblz5+rRRx/VqVOnNHXqVA0ZMkTffPONdd/33ntPcXFxuvnmmzV58mRlZGRo5syZ+uGHH7RlyxZrYFu7dq26du2qevXqacKECbpw4YJmzZqlW2+9VampqQoLC1OfPn20b98+ffzxx3rttddUrVo1SVL16tWtj/fvf/9by5Yt06OPPqpKlSrp9ddfV0xMjA4fPmz9hyUjI0Nt2rSxBoPq1atr1apVio+PV3Z2tp544glJ0oULFxQVFaXDhw9r5MiRCgkJ0QcffGBzfH/XiRMn1LVrVw0YMED333+/goKCrH2rWLGiEhISVLFiRX3zzTcaN26csrOzNW3atCve78KFC3XmzBk99NBDslgsmjp1qvr06aNff/31imejStLDLVu2qEuXLqpZs6YmTpyo/Px8TZo0yeZ78XeV9LkTExOjnTt36rHHHlNYWJgyMzOVmJiow4cPW2936tRJ1atX1zPPPKPAwEAdPHiw2MBTnP3796t///56+OGHFRsbq3fffVd9+/bV6tWr1bFjR0mXzgi2b99eR44c0UMPPaTatWtrw4YNGjt2rI4ePaoZM2bY3Oe7776rixcvatiwYfLx8VGVKlWKfew6depIkj766CPdeuutlz0TeKWfjZdfflkvvPCC+vXrpwcffFDHjh3TrFmzdPvtt9v0UpJOnTqlbt26qV+/fho4cKA++eQTPfLII/L29i5RwMRVwABgfP3114aHh4fh4eFhREZGGk899ZSxZs0aIzc3t8hcSYaPj49x4MAB69ibb75pSDKCg4ON7Oxs6/jYsWMNSda5ubm5Ro0aNYwmTZoYFy5csM5buXKlIckYN26cdax58+ZGjRo1jBMnTljHtm3bZpQrV84YNGiQdWzatGk2j/HnWr29vY2ff/7Z5j4kGbNmzbKOxcfHGzVr1jSOHz9us/+AAQOMgIAA4/z584ZhGMaMGTMMScYnn3xinXPu3Dmjfv36hiRj3bp1RWq4nOHDhxt//hXUvn17Q5Ixb968IvMLa/hfDz30kOHn52dcvHjROhYbG2vUqVPHevvAgQOGJKNq1arGyZMnreOff/65Icn44osvrGPjx48vUlNJe3j33Xcbfn5+xpEjR6xj+/fvNzw9PYvcZ3FiY2ONChUqXHZ7SZ87p06dMiQZ06ZNu+x9ffbZZ4Yk46effrpiXX9Wp04dQ5KxdOlS69jp06eNmjVrGjfddJN17MUXXzQqVKhg7Nu3z2b/Z555xvDw8DAOHz5sGMZ/vz/+/v5GZmbmFR+/oKDA+jwJCgoyBg4caMyZM8c4dOhQkbmX+9k4ePCg4eHhYbz88ss249u3bzc8PT1txgsfa/r06daxnJwc689ncb8jcPXhZThAUseOHZWcnKwePXpo27Ztmjp1qjp37qxatWppxYoVReZHRUXZvNTTunVrSZf+oq9UqVKR8V9//VXSpZc+MjMz9eijj8rX19c6r3v37oqIiNCXX34pSTp69Ki2bt2qwYMH2/yF3axZM3Xs2FFfffVViY8tOjpa4eHhNvfh7+9vrckwDC1dulR33323DMPQ8ePHrV+dO3fW6dOnlZqaKkn66quvVLNmTd1zzz3W+/Pz89OwYcNKXM+V+Pj4KC4ursh4+fLlrf9/5swZHT9+XO3atdP58+dt3gF1Of3791flypWtt9u1ayfpv98bM1fqYX5+vtauXatevXopJCTEOq9+/frq2rXrFe+/JEr63Clfvry8vb21fv16nTp1qtj7KjxrsnLlSuXl5ZW6lpCQEPXu3dt629/fX4MGDdKWLVuUnp4u6dLLhe3atVPlypVtnlPR0dHKz8/Xd999Z3OfMTExJToLZ7FYtGbNGr300kuqXLmyPv74Yw0fPlx16tRR//79S7RmadmyZSooKFC/fv1sagsODlaDBg20bt06m/menp566KGHrLe9vb310EMPKTMzUykpKVd8PLg/whLw/26++WYtW7ZMp06d0qZNmzR27FidOXNG99xzT5H1E7Vr17a5HRAQIEkKDQ0tdrzwH61Dhw5Jkm644YYijx8REWHdbjavYcOGOn78uM6dO1ei4/pzrZJUuXJla03Hjh1TVlaW5s+fr+rVq9t8FYaWwkXuhw4dUv369YuswSmuzr+qVq1axS7s3blzp3r37q2AgAD5+/urevXq1sXhp0+fvuL9/rkPhcHpcoHCbN/C/Qv3zczM1IULF1S/fv0i84ob+ytK+tzx8fHRlClTtGrVKgUFBen222/X1KlTrSFGktq3b6+YmBhNnDhR1apVU8+ePfXuu++WeO1Zcc+B66+/XpKsa4L279+v1atXF3lORUdHSyr6xom6deuW6LELj/G5557T7t27lZaWpo8//lht2rTRJ598UqLriu3fv1+GYahBgwZF6tu9e3eR2kJCQlShQgXT48XVjTVLwJ94e3vr5ptv1s0336zrr79ecXFxWrJkicaPH2+d4+HhUey+lxs3DMMhtZbElWoqXGx7//33KzY2tti5zZo1c0xxxfjfM0iFsrKy1L59e/n7+2vSpEkKDw+Xr6+vUlNT9fTTT5foUgF/53tTFr+vZp544gndfffdWr58udasWaMXXnhBkydP1jfffKObbrpJFotFn376qTZu3KgvvvhCa9as0ZAhQzR9+nRt3LjRLtd7KigoUMeOHfXUU08Vu70wbBQq7vteEjVr1tSAAQMUExOjxo0b65NPPtF7771n+q7GgoICWSwWrVq1qtjvLde7wp8RlgATrVq1knTpZTF7KFycunfvXt1555022/bu3Wvd/r/z/mzPnj2qVq2a9S/dv/tOq+rVq6tSpUrKz8+3/tVvVv+OHTtkGIbN4xZXpz2tX79eJ06c0LJly3T77bdbxw8cOODQxy2pGjVqyNfXVz///HORbcWN/RUlfe4UCg8P1+jRozV69Gjt379fzZs31/Tp0/Xhhx9a57Rp00Zt2rTRyy+/rIULF+q+++7TokWL9OCDD5rW8vPPPxd5Duzbt0+SrC9Ph4eH6+zZs1d8TtmLl5eXmjVrpv3791tfUrvcz0Z4eLgMw1DdunWLhLbipKWl6dy5czZnl/58vLi68TIcIGndunXFniUoXBtkr5eZWrVqpRo1amjevHk2L3msWrVKu3fvVvfu3SVd+mu5efPmev/9923WYOzYsUNff/21unXrZh0r/AX+V68v4+HhoZiYGC1dulQ7duwosv3YsWPW/+/WrZvS0tL06aefWsfOnz+v+fPn/6XHLk2Nku2ZnNzcXM2dO9ehj1tSHh4eio6O1vLly5WWlmYd//nnn7Vq1Sq7PEZJnzvnz5/XxYsXbfYNDw9XpUqVrPudOnWqyPO9efPmklSil+LS0tL02WefWW9nZ2drwYIFat68uYKDgyVJ/fr1U3JystasWVNk/6ysLP3xxx8lOOqi9u/fr8OHDxd7n8nJyapcubJ17dPlfjb69OkjDw8PTZw4sUgfDMPQiRMnbMb++OMPvfnmm9bbubm5evPNN1W9enW1bNnyLx0H3AtnlgBJjz32mM6fP6/evXsrIiJCubm52rBhgxYvXqywsLBiFxz/FV5eXpoyZYri4uLUvn17DRw40Pr277CwMI0aNco6d9q0aeratasiIyMVHx9vvXRAQECAJkyYYJ1X+Mv6ueee04ABA+Tl5aW77767yBoLM//85z+1bt06tW7dWkOHDlWjRo108uRJpaamau3atTp58qQkaejQoZo9e7YGDRqklJQU1axZUx988EGpLhr5V7Rt21aVK1dWbGysRo4cKYvFog8++KBMvQw2YcIEff3117r11lv1yCOPKD8/X7Nnz1aTJk20devWEt1HXl6eXnrppSLjVapU0aOPPlqi586+ffsUFRWlfv36qVGjRvL09NRnn32mjIwMDRgwQJL0/vvva+7cuerdu7fCw8N15swZvfXWW/L397cJ4pdz/fXXKz4+Xj/99JOCgoL0zjvvKCMjQ++++651zpgxY7RixQrdddddGjx4sFq2bKlz585p+/bt+vTTT3Xw4EHr2/lLY9u2bbr33nvVtWtXtWvXTlWqVNGRI0f0/vvvKy0tTTNmzLCG68v9bISHh+ull17S2LFjdfDgQfXq1UuVKlXSgQMH9Nlnn2nYsGF68sknrY8ZEhKiKVOm6ODBg7r++uu1ePFibd26VfPnz/9bF0GFG3H+G/CAsmfVqlXGkCFDjIiICKNixYqGt7e3Ub9+feOxxx4zMjIybOZKMoYPH24zVvj25z+/XXvdunWGJGPJkiU244sXLzZuuukmw8fHx6hSpYpx3333Gb///nuRutauXWvceuutRvny5Q1/f3/j7rvvNnbt2lVk3osvvmjUqlXLKFeunM1bpYur1TAuvf07NjbWZiwjI8MYPny4ERoaanh5eRnBwcFGVFSUMX/+fJt5hw4dMnr06GH4+fkZ1apVMx5//HFj9erVdrt0QOPGjYud/8MPPxht2rQxypcvb4SEhFgv7/Dnx73cpQOKeyu9JGP8+PHW25e7dEBJe5iUlGTcdNNNhre3txEeHm7861//MkaPHm34+vpepgv/FRsba0gq9is8PNw670rPnePHjxvDhw83IiIijAoVKhgBAQFG69atbS73kJqaagwcONCoXbu24ePjY9SoUcO46667jM2bN1+xzjp16hjdu3c31qxZYzRr1szw8fExIiIiijzHDcMwzpw5Y4wdO9aoX7++4e3tbVSrVs1o27at8corr1jfcm/2/SlORkaG8c9//tNo3769UbNmTcPT09OoXLmyceeddxqffvppkfmX+9kwDMNYunSpcdtttxkVKlQwKlSoYERERBjDhw839u7da51T+JzcvHmzERkZafj6+hp16tQxZs+eXaJ6cXWwGEYZ+tMMAK4yvXr10s6dO7V//35Xl2IXYWFhatKkiVauXOnqUpzijjvu0PHjx4t9iRrXDtYsAYCdXLhwweb2/v379dVXX+mOO+5wTUEA7II1SwBgJ/Xq1dPgwYNVr149HTp0SG+88Ya8vb0v+/Z5AO6BsAQAdtKlSxd9/PHHSk9Pl4+PjyIjI/WPf/xDDRo0cHVpAP4G1iwBAACYYM0SAACACcISAACACdYs2UFBQYHS0tJUqVKlv/3REwAAwDkMw9CZM2cUEhKicuUuf/6IsGQHaWlpRT5tHgAAuIfffvtN11133WW3E5bsoFKlSpIufahnlSpVXFzN1SsvL09ff/21OnXqxEcMOBB9dg767Bz02Tnctc/Z2dkKDQ21/jt+OYQlOyh86a1SpUry9/d3cTVXr7y8PPn5+cnf39+tfhjdDX12DvrsHPTZOdy9z1daQsMCbwAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABOEJQAAABNuF5bmzJmjsLAw+fr6qnXr1tq0aZPp/CVLligiIkK+vr5q2rSpvvrqq8vOffjhh2WxWDRjxgw7Vw0AANyVW4WlxYsXKyEhQePHj1dqaqpuvPFGde7cWZmZmcXO37BhgwYOHKj4+Hht2bJFvXr1Uq9evbRjx44icz/77DNt3LhRISEhjj4MAADgRtwqLL366qsaOnSo4uLi1KhRI82bN09+fn565513ip0/c+ZMdenSRWPGjFHDhg314osvqkWLFpo9e7bNvCNHjuixxx7TRx99JC8vL2ccCgAAcBNuE5Zyc3OVkpKi6Oho61i5cuUUHR2t5OTkYvdJTk62mS9JnTt3tplfUFCgBx54QGPGjFHjxo0dUzwAAHBbnq4uoKSOHz+u/Px8BQUF2YwHBQVpz549xe6Tnp5e7Pz09HTr7SlTpsjT01MjR44scS05OTnKycmx3s7OzpYk5eXlKS8vr8T3g9Ip7C09diz67Bz02Tnos3O4a59LWq/bhCVHSElJ0cyZM5WamiqLxVLi/SZPnqyJEycWGV+3bp38/PzsWSKKkZiY6OoSrgn02Tnos3PQZ+dwtz6fP3++RPPcJixVq1ZNHh4eysjIsBnPyMhQcHBwsfsEBwebzv/++++VmZmp2rVrW7fn5+dr9OjRmjFjhg4ePFjs/Y4dO1YJCQnW29nZ2QoNDVWHDh1UtWrVv3J4KIG8vDwlJiaqY8eOrC1zIPrsHPTZOeizc7hrnwtfGboStwlL3t7eatmypZKSktSrVy9Jl9YbJSUlacSIEcXuExkZqaSkJD3xxBPWscTEREVGRkqSHnjggWLXND3wwAOKi4u7bC0+Pj7y8fEpMu7l5eVWTxJ3RZ+dgz47B312DvrsHO7W55LW6jZhSZISEhIUGxurVq1a6ZZbbtGMGTN07tw5a7AZNGiQatWqpcmTJ0uSHn/8cbVv317Tp09X9+7dtWjRIm3evFnz58+XJFWtWrXImSAvLy8FBwfrhhtucO7BAQCAMsmtwlL//v117NgxjRs3Tunp6WrevLlWr15tXcR9+PBhlSv33zf4tW3bVgsXLtTzzz+vZ599Vg0aNNDy5cvVpEkTVx0CAABwM24VliRpxIgRl33Zbf369UXG+vbtq759+5b4/i+3TgkAAFyb3OY6SwAAAK5AWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADBBWAIAADDhdmFpzpw5CgsLk6+vr1q3bq1NmzaZzl+yZIkiIiLk6+urpk2b6quvvrJuy8vL09NPP62mTZuqQoUKCgkJ0aBBg5SWlubowwAAAG7CrcLS4sWLlZCQoPHjxys1NVU33nijOnfurMzMzGLnb9iwQQMHDlR8fLy2bNmiXr16qVevXtqxY4ck6fz580pNTdULL7yg1NRULVu2THv37lWPHj2ceVgAAKAMc6uw9Oqrr2ro0KGKi4tTo0aNNG/ePPn5+emdd94pdv7MmTPVpUsXjRkzRg0bNtSLL76oFi1aaPbs2ZKkgIAAJSYmql+/frrhhhvUpk0bzZ49WykpKTp8+LAzDw0AAJRRbhOWcnNzlZKSoujoaOtYuXLlFB0dreTk5GL3SU5OtpkvSZ07d77sfEk6ffq0LBaLAgMD7VI3AABwb56uLqCkjh8/rvz8fAUFBdmMBwUFac+ePcXuk56eXuz89PT0YudfvHhRTz/9tAYOHCh/f//L1pKTk6OcnBzr7ezsbEmX1kDl5eWV6HhQeoW9pceORZ+dgz47B312Dnftc0nrdZuw5Gh5eXnq16+fDMPQG2+8YTp38uTJmjhxYpHxdevWyc/Pz1El4v8lJia6uoRrAn12DvrsHPTZOdytz+fPny/RPLcJS9WqVZOHh4cyMjJsxjMyMhQcHFzsPsHBwSWaXxiUDh06pG+++cb0rJIkjR07VgkJCdbb2dnZCg0NVYcOHVS1atXSHBZKIS8vT4mJierYsaO8vLxcXc5Viz47B312DvrsHO7a58JXhq7EbcKSt7e3WrZsqaSkJPXq1UuSVFBQoKSkJI0YMaLYfSIjI5WUlKQnnnjCOpaYmKjIyEjr7cKgtH//fq1bt65EYcfHx0c+Pj5Fxr28vNzqSeKu6LNz0GfnoM/OQZ+dw936XNJa3SYsSVJCQoJiY2PVqlUr3XLLLZoxY4bOnTunuLg4SdKgQYNUq1YtTZ48WZL0+OOPq3379po+fbq6d++uRYsWafPmzZo/f76kS0HpnnvuUWpqqlauXKn8/HzreqYqVarI29vbNQcKAADKDLcKS/3799exY8c0btw4paenq3nz5lq9erV1Effhw4dVrtx/3+DXtm1bLVy4UM8//7yeffZZNWjQQMuXL1eTJk0kSUeOHNGKFSskSc2bN7d5rHXr1umOO+5wynEBAICyy63CkiSNGDHisi+7rV+/vshY37591bdv32Lnh4WFyTAMe5YHAACuMm5znSUAAABXICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYKHVYWr16tf79739bb8+ZM0fNmzfXvffeq1OnTtm1OAAAAFcrdVgaM2aMsrOzJUnbt2/X6NGj1a1bNx04cEAJCQl2LxAAAMCVSv1BugcOHFCjRo0kSUuXLtVdd92lf/zjH0pNTVW3bt3sXiAAAIArlfrMkre3t86fPy9JWrt2rTp16iRJqlKlivWMEwAAwNWi1GeWbrvtNiUkJOjWW2/Vpk2btHjxYknSvn37dN1119m9QAAAAFcq9Zml2bNny9PTU59++qneeOMN1apVS5K0atUqdenSxe4FAgAAuFKpzyzVrl1bK1euLDL+2muv2aUgAACAsqTUZ5ZSU1O1fft26+3PP/9cvXr10rPPPqvc3Fy7FgcAAOBqpQ5LDz30kPbt2ydJ+vXXXzVgwAD5+flpyZIleuqpp+xeIAAAgCuVOizt27dPzZs3lyQtWbJEt99+uxYuXKj33ntPS5cutXd9AAAALlXqsGQYhgoKCiRdunRA4bWVQkNDdfz4cftWBwAA4GKlDkutWrXSSy+9pA8++EDffvutunfvLunSxSqDgoLsXiAAAIArlToszZgxQ6mpqRoxYoSee+451a9fX5L06aefqm3btnYvEAAAwJVKfemAZs2a2bwbrtC0adPk4eFhl6IAAADKilKHpUIpKSnavXu3JKlRo0Zq0aKF3YoCAAAoK0odljIzM9W/f399++23CgwMlCRlZWWpQ4cOWrRokapXr27vGgEAAFym1GuWHnvsMZ09e1Y7d+7UyZMndfLkSe3YsUPZ2dkaOXKkI2oEAABwmVKfWVq9erXWrl2rhg0bWscaNWqkOXPmqFOnTnYtDgAAwNVKfWapoKBAXl5eRca9vLys118CAAC4WpQ6LN155516/PHHlZaWZh07cuSIRo0apaioKLsWBwAA4GqlDkuzZ89Wdna2wsLCFB4ervDwcNWtW1fZ2dl6/fXXHVEjAACAy5R6zVJoaKhSU1O1du1a7dmzR5LUsGFDRUdH2704AAAAV/tL11myWCzq2LGjOnbsaB3bs2ePevTooX379tmtOAAAAFcr9ctwl5OTk6NffvnFXncHAABQJtgtLAEAAFyNCEsAAAAmCEsAAAAmSrzAu3LlyrJYLJfd/scff9ilIAAAgLKkxGFpxowZDiwDAACgbCpxWIqNjXVkHQAAAGUSa5YAAABMEJYAAABMEJYAAABMEJYAAABMEJYAAABMlOjdcAkJCSW+w1dfffUvFwMAAFDWlCgsbdmyxeZ2amqq/vjjD91www2SpH379snDw0MtW7a0f4UAAAAuVKKwtG7dOuv/v/rqq6pUqZLef/99Va5cWZJ06tQpxcXFqV27do6pEgAAwEVKvWZp+vTpmjx5sjUoSZc+CuWll17S9OnT7VocAACAq5U6LGVnZ+vYsWNFxo8dO6YzZ87YpSgAAICyotRhqXfv3oqLi9OyZcv0+++/6/fff9fSpUsVHx+vPn36OKJGAAAAlynxZ8MVmjdvnp588knde++9ysvLu3Qnnp6Kj4/XtGnT7F4gAACAK5U6LPn5+Wnu3LmaNm2afvnlF0lSeHi4KlSoYPfiAAAAXO0vX5Ty6NGjOnr0qBo0aKAKFSrIMAx71gUAAFAmlDosnThxQlFRUbr++uvVrVs3HT16VJIUHx+v0aNH271AAAAAVyp1WBo1apS8vLx0+PBh+fn5Wcf79++v1atX27U4AAAAVyv1mqWvv/5aa9as0XXXXWcz3qBBAx06dMhuhQEAAJQFpT6zdO7cOZszSoVOnjwpHx8fuxRlZs6cOQoLC5Ovr69at26tTZs2mc5fsmSJIiIi5Ovrq6ZNm+qrr76y2W4YhsaNG6eaNWuqfPnyio6O1v79+x15CAAAwI2UOiy1a9dOCxYssN62WCwqKCjQ1KlT1aFDB7sW92eLFy9WQkKCxo8fr9TUVN14443q3LmzMjMzi52/YcMGDRw4UPHx8dqyZYt69eqlXr16aceOHdY5U6dO1euvv6558+bpxx9/VIUKFdS5c2ddvHjRoccCAADcQ6nD0tSpUzV//nx17dpVubm5euqpp9SkSRN99913mjJliiNqtHr11Vc1dOhQxcXFqVGjRpo3b578/Pz0zjvvFDt/5syZ6tKli8aMGaOGDRvqxRdfVIsWLTR79mxJl84qzZgxQ88//7x69uypZs2aacGCBUpLS9Py5csdeiwAAMA9lHrNUpMmTbRv3z7Nnj1blSpV0tmzZ9WnTx8NHz5cNWvWdESNkqTc3FylpKRo7Nix1rFy5copOjpaycnJxe6TnJyshIQEm7HOnTtbg9CBAweUnp6u6Oho6/aAgAC1bt1aycnJGjBgQLH3m5OTo5ycHOvt7OxsSVJeXp71Qp2wv8Le0mPHos/OQZ+dgz47h7v2uaT1ljosSZcCxXPPPfdXdv3Ljh8/rvz8fAUFBdmMBwUFac+ePcXuk56eXuz89PR06/bCscvNKc7kyZM1ceLEIuPr1q0rdj0X7CsxMdHVJVwT6LNz0GfnoM/O4W59Pn/+fInm/aWwlJWVpU2bNikzM1MFBQU22wYNGvRX7tKtjB071uaMVXZ2tkJDQ9WhQwdVrVrVhZVd3fLy8pSYmKiOHTvKy8vL1eVcteizc9Bn56DPzuGufS58ZehKSh2WvvjiC9133306e/as/P39ZbFYrNssFovDwlK1atXk4eGhjIwMm/GMjAwFBwcXu09wcLDp/ML/ZmRk2LyEmJGRoebNm1+2Fh8fn2Lf+efl5eVWTxJ3RZ+dgz47B312DvrsHO7W55LWWuoF3qNHj9aQIUN09uxZZWVl6dSpU9avkydPlrrQkvL29lbLli2VlJRkHSsoKFBSUpIiIyOL3ScyMtJmvnTpFGHh/Lp16yo4ONhmTnZ2tn788cfL3icAALi2lPrM0pEjRzRy5EiXrM1JSEhQbGysWrVqpVtuuUUzZszQuXPnFBcXJ+nSS4C1atXS5MmTJUmPP/642rdvr+nTp6t79+5atGiRNm/erPnz50u6dCbsiSee0EsvvaQGDRqobt26euGFFxQSEqJevXo5/fgAAEDZU+qw1LlzZ23evFn16tVzRD2m+vfvr2PHjmncuHFKT09X8+bNtXr1ausC7cOHD6tcuf+eLGvbtq0WLlyo559/Xs8++6waNGig5cuXq0mTJtY5Tz31lM6dO6dhw4YpKytLt912m1avXi1fX1+nHx8AACh7ShSWVqxYYf3/7t27a8yYMdq1a5eaNm1a5PW+Hj162LfCPxkxYoRGjBhR7Lb169cXGevbt6/69u172fuzWCyaNGmSJk2aZK8SAQDAVaREYam4l6SKCxcWi0X5+fl/uygAAICyokRh6c+XBwAAALhWlPrdcAsWLLC5enWh3Nxcm8+MAwAAuBqUOizFxcXp9OnTRcbPnDljfVcaAADA1aLUYckwDJsLURb6/fffFRAQYJeiAAAAyooSXzrgpptuksVikcViUVRUlDw9/7trfn6+Dhw4oC5dujikSAAAAFcpcVgqfEfc1q1b1blzZ1WsWNG6zdvbW2FhYYqJibF7gQAAAK5U4rA0fvx4SVJYWJj69+/PRRsBAMA1odRX8I6NjZUkpaSkaPfu3ZKkxo0b66abbrJvZQAAAGVAqcNSZmamBgwYoPXr1yswMFCSlJWVpQ4dOmjRokWqXr26vWsEAABwmVK/G+6xxx7TmTNntHPnTp08eVInT57Ujh07lJ2drZEjRzqiRgAAAJcp9Zml1atXa+3atWrYsKF1rFGjRpozZ446depk1+IAAABcrdRnlgoKCop8eK4keXl58bEoAADgqlPqsHTnnXfq8ccfV1pamnXsyJEjGjVqlKKiouxaHAAAgKuVOizNnj1b2dnZCgsLU3h4uMLDw1W3bl1lZ2dr1qxZjqgRAADAZUq9Zik0NFSpqalau3at9uzZI0lq2LChoqOj7V4cAACAq5U6LEmSxWJRx44d1bFjR3vXAwAAUKaU+mU4Sfr222919913q379+qpfv7569Oih77//3t61AQAAuFypw9KHH36o6Oho+fn5aeTIkRo5cqR8fX0VFRWlhQsXOqJGAAAAlyn1y3Avv/yypk6dqlGjRlnHRo4cqVdffVUvvvii7r33XrsWCAAA4EqlPrP066+/6u677y4y3qNHDx04cMAuRQEAAJQVpQ5LoaGhSkpKKjK+du1ahYaG2qUoAACAsqLUL8ONHj1aI0eO1NatW9W2bVtJ0g8//KD33ntPM2fOtHuBAAAArlTqsPTII48oODhY06dP1yeffCLp0nWWFi9erJ49e9q9QAAAAFf6S9dZ6t27t3r37m3vWgAAAMqcvxSWCp09e7bIh+f6+/v/rYIAAADKklIv8D5w4IC6d++uChUqKCAgQJUrV1blypUVGBioypUrO6JGAAAAlyn1maX7779fhmHonXfeUVBQkCwWiyPqAgAAKBNKHZa2bdumlJQU3XDDDY6oBwAAoEwp9ctwN998s3777TdH1AIAAFDmlPrM0r/+9S89/PDDOnLkiJo0aSIvLy+b7c2aNbNbcQAAAK5W6rB07Ngx/fLLL4qLi7OOWSwWGYYhi8Wi/Px8uxYIAADgSqUOS0OGDNFNN92kjz/+mAXeAADgqlfqsHTo0CGtWLFC9evXd0Q9AAAAZUqpF3jfeeed2rZtmyNqAQAAKHNKfWbp7rvv1qhRo7R9+3Y1bdq0yALvHj162K04AAAAVyt1WHr44YclSZMmTSqyjQXeAADgalPqsPTnz4IDAAC4mpV6zRIAAMC1pMRhKTk5WStXrrQZW7BggerWrasaNWpo2LBhysnJsXuBAAAArlTisDRp0iTt3LnTenv79u2Kj49XdHS0nnnmGX3xxReaPHmyQ4oEAABwlRKHpa1btyoqKsp6e9GiRWrdurXeeustJSQk6PXXX9cnn3zikCIBAABcpcRh6dSpUwoKCrLe/vbbb9W1a1frbT5gFwAAXI1KHJaCgoJ04MABSVJubq5SU1PVpk0b6/YzZ84UueYSAACAuytxWOrWrZueeeYZff/99xo7dqz8/PzUrl076/b//Oc/Cg8Pd0iRAAAArlLi6yy9+OKL6tOnj9q3b6+KFSvq/fffl7e3t3X7O++8o06dOjmkSAAAAFcpcViqVq2avvvuO50+fVoVK1aUh4eHzfYlS5aoYsWKdi8QAADAlUp9Be+AgIBix6tUqfK3iwEAAChruII3AACACcISAACACcISAACACcISAACACcISAACACcISAACACcISAACACcISAACACbcJSydPntR9990nf39/BQYGKj4+XmfPnjXd5+LFixo+fLiqVq2qihUrKiYmRhkZGdbt27Zt08CBAxUaGqry5curYcOGmjlzpqMPBQAAuBG3CUv33Xefdu7cqcTERK1cuVLfffedhg0bZrrPqFGj9MUXX2jJkiX69ttvlZaWpj59+li3p6SkqEaNGvrwww+1c+dOPffccxo7dqxmz57t6MMBAABuotQfd+IKu3fv1urVq/XTTz+pVatWkqRZs2apW7dueuWVVxQSElJkn9OnT+vtt9/WwoULdeedd0qS3n33XTVs2FAbN25UmzZtNGTIEJt96tWrp+TkZC1btkwjRoxw/IEBAIAyzy3CUnJysgIDA61BSZKio6NVrlw5/fjjj+rdu3eRfVJSUpSXl6fo6GjrWEREhGrXrq3k5GS1adOm2Mc6ffr0FT/nLicnRzk5Odbb2dnZkqS8vDzl5eWV6thQcoW9pceORZ+dgz47B312Dnftc0nrdYuwlJ6erho1atiMeXp6qkqVKkpPT7/sPt7e3goMDLQZDwoKuuw+GzZs0OLFi/Xll1+a1jN58mRNnDixyPi6devk5+dnui/+vsTERFeXcE2gz85Bn52DPjuHu/X5/PnzJZrn0rD0zDPPaMqUKaZzdu/e7ZRaduzYoZ49e2r8+PHq1KmT6dyxY8cqISHBejs7O1uhoaHq0KGDqlat6uhSr1l5eXlKTExUx44d5eXl5epyrlr02Tnos3PQZ+dw1z4XvjJ0JS4NS6NHj9bgwYNN59SrV0/BwcHKzMy0Gf/jjz908uRJBQcHF7tfcHCwcnNzlZWVZXN2KSMjo8g+u3btUlRUlIYNG6bnn3/+inX7+PjIx8enyLiXl5dbPUncFX12DvrsHPTZOeizc7hbn0taq0vDUvXq1VW9evUrzouMjFRWVpZSUlLUsmVLSdI333yjgoICtW7duth9WrZsKS8vLyUlJSkmJkaStHfvXh0+fFiRkZHWeTt37tSdd96p2NhYvfzyy3Y4KgAAcDVxi0sHNGzYUF26dNHQoUO1adMm/fDDDxoxYoQGDBhgfSfckSNHFBERoU2bNkmSAgICFB8fr4SEBK1bt04pKSmKi4tTZGSkdXH3jh071KFDB3Xq1EkJCQlKT09Xenq6jh075rJjBQAAZYtbLPCWpI8++kgjRoxQVFSUypUrp5iYGL3++uvW7Xl5edq7d6/NYq3XXnvNOjcnJ0edO3fW3Llzrds//fRTHTt2TB9++KE+/PBD63idOnV08OBBpxwXAAAo29wmLFWpUkULFy687PawsDAZhmEz5uvrqzlz5mjOnDnF7jNhwgRNmDDBnmUCAICrjFu8DAcAAOAqhCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAATbhOWTp48qfvuu0/+/v4KDAxUfHy8zp49a7rPxYsXNXz4cFWtWlUVK1ZUTEyMMjIyip174sQJXXfddbJYLMrKynLAEQAAAHfkNmHpvvvu086dO5WYmKiVK1fqu+++07Bhw0z3GTVqlL744gstWbJE3377rdLS0tSnT59i58bHx6tZs2aOKB0AALgxtwhLu3fv1urVq/Wvf/1LrVu31m233aZZs2Zp0aJFSktLK3af06dP6+2339arr76qO++8Uy1bttS7776rDRs2aOPGjTZz33jjDWVlZenJJ590xuEAAAA34unqAkoiOTlZgYGBatWqlXUsOjpa5cqV048//qjevXsX2SclJUV5eXmKjo62jkVERKh27dpKTk5WmzZtJEm7du3SpEmT9OOPP+rXX38tUT05OTnKycmx3s7OzpYk5eXlKS8v7y8dI66ssLf02LHos3PQZ+egz87hrn0uab1uEZbS09NVo0YNmzFPT09VqVJF6enpl93H29tbgYGBNuNBQUHWfXJycjRw4EBNmzZNtWvXLnFYmjx5siZOnFhkfN26dfLz8yvRfeCvS0xMdHUJ1wT67Bz02Tnos3O4W5/Pnz9fonkuDUvPPPOMpkyZYjpn9+7dDnv8sWPHqmHDhrr//vtLvV9CQoL1dnZ2tkJDQ9WhQwdVrVrV3mXi/+Xl5SkxMVEdO3aUl5eXq8u5atFn56DPzkGfncNd+1z4ytCVuDQsjR49WoMHDzadU69ePQUHByszM9Nm/I8//tDJkycVHBxc7H7BwcHKzc1VVlaWzdmljIwM6z7ffPONtm/frk8//VSSZBiGJKlatWp67rnnij17JEk+Pj7y8fEpMu7l5eVWTxJ3RZ+dgz47B312DvrsHO7W55LW6tKwVL16dVWvXv2K8yIjI5WVlaWUlBS1bNlS0qWgU1BQoNatWxe7T8uWLeXl5aWkpCTFxMRIkvbu3avDhw8rMjJSkrR06VJduHDBus9PP/2kIUOG6Pvvv1d4ePjfPTwAAHAVcIs1Sw0bNlSXLl00dOhQzZs3T3l5eRoxYoQGDBigkJAQSdKRI0cUFRWlBQsW6JZbblFAQIDi4+OVkJCgKlWqyN/fX4899pgiIyOti7v/HIiOHz9ufbw/r3UCAADXJrcIS5L00UcfacSIEYqKilK5cuUUExOj119/3bo9Ly9Pe/futVms9dprr1nn5uTkqHPnzpo7d64rygcAAG7KbcJSlSpVtHDhwstuDwsLs645KuTr66s5c+Zozpw5JXqMO+64o8h9AACAa5tbXJQSAADAVQhLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJjxdXcDVwDAMSdKZM2fk5eXl4mquXnl5eTp//ryys7PpswPRZ+egz85Bn53DXfucnZ0t6b//jl8OYckOTpw4IUmqW7euiysBAACldebMGQUEBFx2O2HJDqpUqSJJOnz4sGmz8fdkZ2crNDRUv/32m/z9/V1dzlWLPjsHfXYO+uwc7tpnwzB05swZhYSEmM4jLNlBuXKXln4FBAS41ZPEXfn7+9NnJ6DPzkGfnYM+O4c79rkkJzlY4A0AAGCCsAQAAGCCsGQHPj4+Gj9+vHx8fFxdylWNPjsHfXYO+uwc9Nk5rvY+W4wrvV8OAADgGsaZJQAAABOEJQAAABOEJQAAABOEJQAAABOEpWLMmTNHYWFh8vX1VevWrbVp0ybT+UuWLFFERIR8fX3VtGlTffXVVzbbDcPQuHHjVLNmTZUvX17R0dHav3+/Iw/BLdi7z8uWLVOnTp1UtWpVWSwWbd261YHVuw979jkvL09PP/20mjZtqgoVKigkJESDBg1SWlqaow/DLdj7OT1hwgRFRESoQoUKqly5sqKjo/Xjjz868hDcgr37/L8efvhhWSwWzZgxw85Vux9793nw4MGyWCw2X126dHHkIdiPARuLFi0yvL29jXfeecfYuXOnMXToUCMwMNDIyMgodv4PP/xgeHh4GFOnTjV27dplPP/884aXl5exfft265x//vOfRkBAgLF8+XJj27ZtRo8ePYy6desaFy5ccNZhlTmO6POCBQuMiRMnGm+99ZYhydiyZYuTjqbssnefs7KyjOjoaGPx4sXGnj17jOTkZOOWW24xWrZs6czDKpMc8Zz+6KOPjMTEROOXX34xduzYYcTHxxv+/v5GZmamsw6rzHFEnwstW7bMuPHGG42QkBDjtddec/CRlG2O6HNsbKzRpUsX4+jRo9avkydPOuuQ/hbC0p/ccsstxvDhw6238/PzjZCQEGPy5MnFzu/Xr5/RvXt3m7HWrVsbDz30kGEYhlFQUGAEBwcb06ZNs27PysoyfHx8jI8//tgBR+Ae7N3n/3XgwAHC0v9zZJ8Lbdq0yZBkHDp0yD5Fuyln9Pr06dOGJGPt2rX2KdoNOarPv//+u1GrVi1jx44dRp06da75sOSIPsfGxho9e/Z0SL2Oxstw/yM3N1cpKSmKjo62jpUrV07R0dFKTk4udp/k5GSb+ZLUuXNn6/wDBw4oPT3dZk5AQIBat2592fu82jmizyjKWX0+ffq0LBaLAgMD7VK3O3JGr3NzczV//nwFBAToxhtvtF/xbsRRfS4oKNADDzygMWPGqHHjxo4p3o048vm8fv161ahRQzfccIMeeeQRnThxwv4H4ACEpf9x/Phx5efnKygoyGY8KChI6enpxe6Tnp5uOr/wv6W5z6udI/qMopzR54sXL+rpp5/WwIED3e7DM+3Jkb1euXKlKlasKF9fX7322mtKTExUtWrV7HsAbsJRfZ4yZYo8PT01cuRI+xfthhzV5y5dumjBggVKSkrSlClT9O2336pr167Kz8+3/0HYmaerCwDgnvLy8tSvXz8ZhqE33njD1eVctTp06KCtW7fq+PHjeuutt9SvXz/9+OOPqlGjhqtLuyqkpKRo5syZSk1NlcVicXU5V7UBAwZY/79p06Zq1qyZwsPDtX79ekVFRbmwsivjzNL/qFatmjw8PJSRkWEznpGRoeDg4GL3CQ4ONp1f+N/S3OfVzhF9RlGO7HNhUDp06JASExOv6bNKkmN7XaFCBdWvX19t2rTR22+/LU9PT7399tv2PQA34Yg+f//998rMzFTt2rXl6ekpT09PHTp0SKNHj1ZYWJhDjqOsc9bv6Hr16qlatWr6+eef/37RDkZY+h/e3t5q2bKlkpKSrGMFBQVKSkpSZGRksftERkbazJekxMRE6/y6desqODjYZk52drZ+/PHHy97n1c4RfUZRjupzYVDav3+/1q5dq6pVqzrmANyIM5/TBQUFysnJ+ftFuyFH9PmBBx7Qf/7zH23dutX6FRISojFjxmjNmjWOO5gyzFnP599//10nTpxQzZo17VO4I7l6hXlZs2jRIsPHx8d47733jF27dhnDhg0zAgMDjfT0dMMwDOOBBx4wnnnmGev8H374wfD09DReeeUVY/fu3cb48eOLvXRAYGCg8fnnnxv/+c9/jJ49e3LpAAf0+cSJE8aWLVuML7/80pBkLFq0yNiyZYtx9OhRpx9fWWHvPufm5ho9evQwrrvuOmPr1q02bwHOyclxyTGWFfbu9dmzZ42xY8caycnJxsGDB43NmzcbcXFxho+Pj7Fjxw6XHGNZ4IjfHX/Gu+Hs3+czZ84YTz75pJGcnGwcOHDAWLt2rdGiRQujQYMGxsWLF11yjKVBWCrGrFmzjNq1axve3t7GLbfcYmzcuNG6rX379kZsbKzN/E8++cS4/vrrDW9vb6Nx48bGl19+abO9oKDAeOGFF4ygoCDDx8fHiIqKMvbu3euMQynT7N3nd99915BU5Gv8+PFOOJqyy559LrwsQ3Ff69atc9IRlV327PWFCxeM3r17GyEhIYa3t7dRs2ZNo0ePHsamTZucdThllr1/d/wZYekSe/b5/PnzRqdOnYzq1asbXl5eRp06dYyhQ4daw1dZZzEMw3DNOS0AAICyjzVLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAK4Zx44d0yOPPKLatWvLx8dHwcHB6ty5s3744QdJksVi0fLly11bJIAyx9PVBQCAs8TExCg3N1fvv/++6tWrp4yMDCUlJenEiROuLg1AGcbHnQC4JmRlZaly5cpav3692rdvX2R7WFiYDh06ZL1dp04dHTx4UJL0+eefa+LEidq1a5dCQkIUGxur5557Tp6el/7etFgsmjt3rlasWKH169erZs2amjp1qu655x6nHBsAx+JlOADXhIoVK6pixYpavny5cnJyimz/6aefJEnvvvuujh49ar39/fffa9CgQXr88ce1a9cuvfnmm3rvvff08ssv2+z/wgsvKCYmRtu2bdN9992nAQMGaPfu3Y4/MAAOx5klANeMpUuXaujQobpw4YJatGih9u3ba8CAAWrWrJmkS2eIPvvsM/Xq1cu6T3R0tKKiojR27Fjr2IcffqinnnpKaWlp1v0efvhhvfHGG9Y5bdq0UYsWLTR37lznHBwAh+HMEoBrRkxMjNLS0rRixQp16dJF69evV4sWLfTee+9ddp9t27Zp0qRJ1jNTFStW1NChQ3X06FGdP3/eOi8yMtJmv8jISM4sAVcJFngDuKb4+vqqY8eO6tixo1544QU9+OCDGj9+vAYPHlzs/LNnz2rixInq06dPsfcF4OrHmSUA17RGjRrp3LlzkiQvLy/l5+fbbG/RooX27t2r+vXrF/kqV+6/v0I3btxos9/GjRvVsGFDxx8AAIfjzBKAa8KJEyfUt29fDRkyRM2aNVOlSpW0efNmTZ06VT179pR06R1xSUlJuvXWW+Xj46PKlStr3Lhxuuuuu1S7dm3dc889KleunLZt26YdO3bopZdest7/kiVL1KpVK91222366KOPtGnTJr399tuuOlwAdsQCbwDXhJycHE2YMEFff/21fvnlF+Xl5Sk0NFR9+/bVs88+q/Lly+uLL75QQkKCDh48qFq1alkvHbBmzRpNmjRJW7ZskZeXlyIiIvTggw9q6NChki4t8J4zZ46WL1+u7777TjVr1tSUKVPUr18/Fx4xAHshLAHA31Tcu+gAXD1YswQAAGCCsAQAAGCCBd4A8DexmgG4unFmCQAAwARhCQAAwARhCQAAwARhCQAAwARhCQAAwARhCQAAwARhCQAAwARhCQAAwARhCQAAwMT/Abtx4aBZQntfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'RewardHistory' object has no attribute 'rewards'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist (1) copy 2.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%20%281%29%20copy%202.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m plt\u001b[39m.\u001b[39mgrid(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%20%281%29%20copy%202.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%20%281%29%20copy%202.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(moving_average(reward_history\u001b[39m.\u001b[39;49mrewards, window_size\u001b[39m=\u001b[39m\u001b[39m70\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%20%281%29%20copy%202.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mTraining Rewards per Episode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Btbam-workstation/home/eeshan/Music/KernelModuleDefs/SystemIntegrityProtocols/5eee7de7454beed9d83660fb3a4535c1/3202b0f40c9b6d03df8679cc2d85fea0/85df3becdfcb4d7e35c74befeee54aa5/33507a4383e3645034645fb210bfebbe/ffc1b50d6da82d6fbbfaad7d8e06c271/a/NeuroAssist%20%281%29%20copy%202.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mEpisode\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RewardHistory' object has no attribute 'rewards'"
     ]
    }
   ],
   "source": [
    "env = Plasticity(1,(X_test,y_test),False)\n",
    "scores = dqn.test(env, nb_episodes=3, visualize=False,verbose=0)\n",
    "print(dqn.metrics_names)\n",
    "\n",
    "print(np.mean(scores.history['episode_reward'])*100,'%')\n",
    "def moving_average(data, window_size):\n",
    "    return [np.mean(data[i:i+window_size]) for i in range(len(data) - window_size + 1)]\n",
    "losses = loss_history.losses\n",
    "losses2 = loss_history2.losses\n",
    "smoothed_losses = moving_average(losses, window_size=50)  \n",
    "print(smoothed_losses)\n",
    "plt.plot(smoothed_losses)\n",
    "plt.title('Smoothed Training Loss per Step')\n",
    "plt.xlabel('Step')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Smoothed Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "smoothed_losses2 = moving_average(losses2, window_size=70)  \n",
    "print(smoothed_losses2)\n",
    "plt.plot(smoothed_losses2)\n",
    "plt.title('Smoothed Training Loss per Step')\n",
    "plt.xlabel('Step')\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Smoothed Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(moving_average(reward_history.rewards, window_size=70))\n",
    "plt.title('Training Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.show()\n",
    "\n",
    "print(\"___________________________________________________________________________________________________________________________\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
