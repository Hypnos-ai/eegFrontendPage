{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import scipy.signal\n",
    "from scipy import stats\n",
    "def mean(x):\n",
    "    return np.mean(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def stddev(x):\n",
    "    return np.std(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def peaktopeak(x):\n",
    "    return np.ptp(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def variance(x):\n",
    "    return np.var(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def mini(x):\n",
    "    return np.min(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def maxi(x):\n",
    "    return np.max(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def argmini(x):\n",
    "    return np.argmin(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def argmaxi(x):\n",
    "    return np.argmax(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def rms(x):\n",
    "    return np.sqrt(np.mean(x**2, axis=-1)).reshape(-1, 1)\n",
    "\n",
    "def abs_diff_signal(x):\n",
    "    return np.sum(np.abs(np.diff(x, axis=-1)), axis=-1).reshape(-1, 1)\n",
    "\n",
    "def skewness(x):\n",
    "    return stats.skew(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def kurtosis(x):\n",
    "    return stats.kurtosis(x, axis=-1).reshape(-1, 1)\n",
    "\n",
    "def concat_features(x):\n",
    "    features = np.concatenate(\n",
    "        (\n",
    "            peaktopeak(x),\n",
    "            rms(x),\n",
    "            abs_diff_signal(x),\n",
    "            skewness(x),\n",
    "            kurtosis(x),\n",
    "            variance(x),\n",
    "            mean(x),\n",
    "            stddev(x)\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    return features\n",
    "\n",
    "def apply_cwt(data, scales, wavelet_name='morl'):\n",
    "    \"\"\"\n",
    "    Apply Continuous Wavelet Transform (CWT) to EEG data.\n",
    "\n",
    "    :param data: EEG data in CSP space with shape (components, timepoints)\n",
    "    :param scales: Scales for CWT\n",
    "    :param wavelet_name: Name of the mother wavelet for CWT\n",
    "    :return: CWT coefficients\n",
    "    \"\"\"\n",
    "    cwt_coeffs = np.array([pywt.cwt(data[i, :], scales, wavelet_name)[0] for i in range(data.shape[0])])\n",
    "    return cwt_coeffs\n",
    "\n",
    "    \n",
    "def featuresarray_load(data_array):\n",
    "    features = []\n",
    "    fs = 250\n",
    "    for d in data_array:\n",
    "        \n",
    "       \n",
    "        alpha = mne.filter.filter_data(d, sfreq=fs, l_freq=8, h_freq=12,verbose=False)\n",
    "        beta = mne.filter.filter_data(d, sfreq=fs, l_freq=12, h_freq=30,verbose=False)\n",
    "        \n",
    "        alph_ftrs = concat_features(alpha)\n",
    "        beta_ftrs = concat_features(beta)\n",
    "        \n",
    "        #nperseg = 256\n",
    "        \n",
    "        \n",
    "        _,p=scipy.signal.welch(beta, fs=fs,average='median',nfft = 512)\n",
    "        _,p2=scipy.signal.welch(alpha, fs=fs,average='median',nfft = 512)\n",
    "        \n",
    "\n",
    "        res = np.mean([alph_ftrs,beta_ftrs],axis=0)\n",
    "        #print('p',p.shape,res.shape)\n",
    "        res = np.concatenate((res,p,p2),axis=1)\n",
    "        #print(res.shape)\n",
    "        features.append(res)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 2, 2: 3, 3: 4, 4: 5}\n",
      "Opening raw data file c:/dev/eeg/eegfrontendpage/eeg_tool/src/data/formatted/S03.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 11248 =      0.000 ...    44.992 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncvn\\AppData\\Local\\Temp\\ipykernel_18448\\300687801.py:23: RuntimeWarning: This filename (c:/dev/eeg/eegfrontendpage/eeg_tool/src/data/formatted/S03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif(path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 11248  =      0.000 ...    44.992 secs...\n",
      "Used Annotations descriptions: ['blinking', 'jaw', 'left', 'relax', 'right']\n",
      "eventids {'blinking': 1, 'jaw': 2, 'left': 3, 'relax': 4, 'right': 5}\n",
      "Fitting ICA to data using 3 channels (please be patient, this may take a while)\n",
      "Selecting by number: 3 components\n",
      "Fitting ICA took 0.0s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (3 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 3 PCA components\n",
      "Used Annotations descriptions: ['blinking', 'jaw', 'left', 'relax', 'right']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncvn\\AppData\\Local\\Temp\\ipykernel_18448\\300687801.py:26: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  ica.fit(raw)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from mne.decoding import CSP\n",
    "import mne\n",
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "event_ids = [1,2,3,4, 5]  \n",
    "event_id_to_label = {}\n",
    "for i in range(len(event_ids)):\n",
    "    event_id_to_label[i] = event_ids[i]\n",
    "print(event_id_to_label)\n",
    "path = 'c:/dev/eeg/eegfrontendpage/eeg_tool/src/data/formatted/S03.fif'\n",
    "raw = mne.io.read_raw_fif(path, preload=True)\n",
    "print('eventids',mne.events_from_annotations(raw)[1])\n",
    "ica = mne.preprocessing.ICA(n_components=len(raw.info['ch_names']), random_state=42, max_iter=1000)\n",
    "ica.fit(raw)\n",
    "ica.apply(raw)\n",
    "csp_filters = {} \n",
    "events = mne.events_from_annotations(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "    const toggleVisibility = (className) => {\n",
       "\n",
       "  const elements = document.querySelectorAll(`.${className}`)\n",
       "\n",
       "  elements.forEach(element => {\n",
       "    if (element.classList.contains('repr-section-header')) {\n",
       "      // Don't collapse the section header row.\n",
       "       return\n",
       "    }\n",
       "    if (element.classList.contains('repr-element-collapsed')) {\n",
       "      // Force a reflow to ensure the display change takes effect before removing the class\n",
       "      element.classList.remove('repr-element-collapsed')\n",
       "      element.offsetHeight // This forces the browser to recalculate layout\n",
       "      element.classList.remove('repr-element-faded')\n",
       "    } else {\n",
       "      // Start transition to hide the element\n",
       "      element.classList.add('repr-element-faded')\n",
       "      element.addEventListener('transitionend', handler = (e) => {\n",
       "        if (e.propertyName === 'opacity' && getComputedStyle(element).opacity === '0.2') {\n",
       "          element.classList.add('repr-element-collapsed')\n",
       "          element.removeEventListener('transitionend', handler)\n",
       "        }\n",
       "      });\n",
       "    }\n",
       "  });\n",
       "\n",
       "  // Take care of button (adjust caret)\n",
       "  const button = document.querySelectorAll(`.repr-section-header.${className} > th.repr-section-toggle-col > button`)[0]\n",
       "  button.classList.toggle('collapsed')\n",
       "\n",
       "  // Take care of the tooltip of the section header row\n",
       "  const sectionHeaderRow = document.querySelectorAll(`tr.repr-section-header.${className}`)[0]\n",
       "  sectionHeaderRow.classList.toggle('collapsed')\n",
       "  sectionHeaderRow.title = sectionHeaderRow.title === 'Hide section' ? 'Show section' : 'Hide section'\n",
       "}\n",
       "</script>\n",
       "\n",
       "<style type=\"text/css\">\n",
       "    table.repr.table.table-hover.table-striped.table-sm.table-responsive.small {\n",
       "  /* Don't make rows wider than they need to be. */\n",
       "  display: inline;\n",
       "}\n",
       "\n",
       "table > tbody > tr.repr-element > td {\n",
       "  /* Apply a tighter layout to the table cells. */\n",
       "  padding-top: 0.1rem;\n",
       "  padding-bottom: 0.1rem;\n",
       "  padding-right: 1rem;\n",
       "}\n",
       "\n",
       "table > tbody > tr > td.repr-section-toggle-col {\n",
       "  /* Remove background and border of the first cell in every row\n",
       "     (this row is only used for the collapse / uncollapse caret)\n",
       "\n",
       "     TODO: Need to find a good solution for VS Code that works in both\n",
       "           light and dark mode. */\n",
       "  border-color: transparent;\n",
       "  --bs-table-accent-bg: transparent;\n",
       "}\n",
       "\n",
       "tr.repr-section-header {\n",
       "  /* Remove stripes from section header rows */\n",
       "  background-color: transparent;\n",
       "  border-color: transparent;\n",
       "  --bs-table-striped-bg: transparent;\n",
       "  cursor: pointer;\n",
       "}\n",
       "\n",
       "tr.repr-section-header > th {\n",
       "  text-align: left !important;\n",
       "  vertical-align: middle;\n",
       "}\n",
       "\n",
       ".repr-element, tr.repr-element > td {\n",
       "  opacity: 1;\n",
       "  text-align: left !important;\n",
       "}\n",
       "\n",
       ".repr-element-faded {\n",
       "  transition: 0.3s ease;\n",
       "  opacity: 0.2;\n",
       "}\n",
       "\n",
       ".repr-element-collapsed {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "/* Collapse / uncollapse button and the caret it contains. */\n",
       ".repr-section-toggle-col button {\n",
       "  cursor: pointer;\n",
       "  width: 1rem;\n",
       "  background-color: transparent;\n",
       "  border-color: transparent;\n",
       "}\n",
       "\n",
       "span.collapse-uncollapse-caret {\n",
       "  width: 1rem;\n",
       "  height: 1rem;\n",
       "  display: block;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: left;\n",
       "  background-size: contain;\n",
       "}\n",
       "\n",
       "/* The collapse / uncollapse carets were copied from the free Font Awesome collection and adjusted. */\n",
       "\n",
       "/* Default to black carets for light mode */\n",
       ".repr-section-toggle-col > button.collapsed > span.collapse-uncollapse-caret {\n",
       "  background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"black\" d=\"M246.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-9.2-9.2-22.9-11.9-34.9-6.9s-19.8 16.6-19.8 29.6l0 256c0 12.9 7.8 24.6 19.8 29.6s25.7 2.2 34.9-6.9l128-128z\"/></svg>');\n",
       "}\n",
       "\n",
       ".repr-section-toggle-col\n",
       "  > button:not(.collapsed)\n",
       "  > span.collapse-uncollapse-caret {\n",
       "  background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 320 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"black\" d=\"M137.4 374.6c12.5 12.5 32.8 12.5 45.3 0l128-128c9.2-9.2 11.9-22.9 6.9-34.9s-16.6-19.8-29.6-19.8L32 192c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9l128 128z\"/></svg>');\n",
       "}\n",
       "\n",
       "/* Use white carets for dark mode */\n",
       "@media (prefers-color-scheme: dark) {\n",
       "  .repr-section-toggle-col > button.collapsed > span.collapse-uncollapse-caret {\n",
       "    background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"white\" d=\"M246.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-9.2-9.2-22.9-11.9-34.9-6.9s-19.8 16.6-19.8 29.6l0 256c0 12.9 7.8 24.6 19.8 29.6s25.7 2.2 34.9-6.9l128-128z\"/></svg>');\n",
       "  }\n",
       "\n",
       "  .repr-section-toggle-col\n",
       "    > button:not(.collapsed)\n",
       "    > span.collapse-uncollapse-caret {\n",
       "    background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 320 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"white\" d=\"M137.4 374.6c12.5 12.5 32.8 12.5 45.3 0l128-128c9.2-9.2 11.9-22.9 6.9-34.9s-16.6-19.8-29.6-19.8L32 192c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9l128 128z\"/></svg>');\n",
       "  }\n",
       "}\n",
       "\n",
       ".channel-names-btn {\n",
       "  padding: 0;\n",
       "  border: none;\n",
       "  background: none;\n",
       "  text-decoration: underline;\n",
       "  text-decoration-style: dashed;\n",
       "  cursor: pointer;\n",
       "  color: #0d6efd;\n",
       "}\n",
       "\n",
       ".channel-names-btn:hover {\n",
       "  color: #0a58ca;\n",
       "}\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "<table class=\"repr table table-hover table-striped table-sm table-responsive small\">\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header general-c9245d16-8589-4f28-b1af-b982ba8c8e74\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('general-c9245d16-8589-4f28-b1af-b982ba8c8e74')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>General</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element general-c9245d16-8589-4f28-b1af-b982ba8c8e74 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Filename(s)</td>\n",
       "    <td>\n",
       "        \n",
       "        S03.fif\n",
       "        \n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element general-c9245d16-8589-4f28-b1af-b982ba8c8e74 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>MNE object type</td>\n",
       "    <td>Raw</td>\n",
       "</tr>\n",
       "<tr class=\"repr-element general-c9245d16-8589-4f28-b1af-b982ba8c8e74 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Measurement date</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-c9245d16-8589-4f28-b1af-b982ba8c8e74 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Participant</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-c9245d16-8589-4f28-b1af-b982ba8c8e74 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Experimenter</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header acquisition-644bd089-4102-45ab-8b31-14698333d0da\" \n",
       "    title=\"Hide section\"  onclick=\"toggleVisibility('acquisition-644bd089-4102-45ab-8b31-14698333d0da')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Acquisition</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element acquisition-644bd089-4102-45ab-8b31-14698333d0da \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Duration</td>\n",
       "    <td>00:00:45 (HH:MM:SS)</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-644bd089-4102-45ab-8b31-14698333d0da \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Sampling frequency</td>\n",
       "    <td>250.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-644bd089-4102-45ab-8b31-14698333d0da \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Time points</td>\n",
       "    <td>11,249</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header channels-74307faa-5a4c-434c-a58d-5e546eeacd39\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('channels-74307faa-5a4c-434c-a58d-5e546eeacd39')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Channels</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-74307faa-5a4c-434c-a58d-5e546eeacd39 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>EEG</td>\n",
       "    <td>\n",
       "        <button class=\"channel-names-btn\" onclick=\"alert('Good EEG:\\n\\nC3, CPz, C4')\" title=\"(Click to open in popup)&#13;&#13;C3, CPz, C4\">\n",
       "            3\n",
       "        </button>\n",
       "\n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-74307faa-5a4c-434c-a58d-5e546eeacd39 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Head & sensor digitization</td>\n",
       "    \n",
       "    <td>Not available</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header filters-b3b48140-0db8-4a1b-b3a9-8d4756d44ba9\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('filters-b3b48140-0db8-4a1b-b3a9-8d4756d44ba9')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Filters</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element filters-b3b48140-0db8-4a1b-b3a9-8d4756d44ba9 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Highpass</td>\n",
       "    <td>0.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element filters-b3b48140-0db8-4a1b-b3a9-8d4756d44ba9 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Lowpass</td>\n",
       "    <td>125.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "</table>"
      ],
      "text/plain": [
       "<Raw | S03.fif, 3 x 11249 (45.0 s), ~272 kB, data loaded>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "    const toggleVisibility = (className) => {\n",
       "\n",
       "  const elements = document.querySelectorAll(`.${className}`)\n",
       "\n",
       "  elements.forEach(element => {\n",
       "    if (element.classList.contains('repr-section-header')) {\n",
       "      // Don't collapse the section header row.\n",
       "       return\n",
       "    }\n",
       "    if (element.classList.contains('repr-element-collapsed')) {\n",
       "      // Force a reflow to ensure the display change takes effect before removing the class\n",
       "      element.classList.remove('repr-element-collapsed')\n",
       "      element.offsetHeight // This forces the browser to recalculate layout\n",
       "      element.classList.remove('repr-element-faded')\n",
       "    } else {\n",
       "      // Start transition to hide the element\n",
       "      element.classList.add('repr-element-faded')\n",
       "      element.addEventListener('transitionend', handler = (e) => {\n",
       "        if (e.propertyName === 'opacity' && getComputedStyle(element).opacity === '0.2') {\n",
       "          element.classList.add('repr-element-collapsed')\n",
       "          element.removeEventListener('transitionend', handler)\n",
       "        }\n",
       "      });\n",
       "    }\n",
       "  });\n",
       "\n",
       "  // Take care of button (adjust caret)\n",
       "  const button = document.querySelectorAll(`.repr-section-header.${className} > th.repr-section-toggle-col > button`)[0]\n",
       "  button.classList.toggle('collapsed')\n",
       "\n",
       "  // Take care of the tooltip of the section header row\n",
       "  const sectionHeaderRow = document.querySelectorAll(`tr.repr-section-header.${className}`)[0]\n",
       "  sectionHeaderRow.classList.toggle('collapsed')\n",
       "  sectionHeaderRow.title = sectionHeaderRow.title === 'Hide section' ? 'Show section' : 'Hide section'\n",
       "}\n",
       "</script>\n",
       "\n",
       "<style type=\"text/css\">\n",
       "    table.repr.table.table-hover.table-striped.table-sm.table-responsive.small {\n",
       "  /* Don't make rows wider than they need to be. */\n",
       "  display: inline;\n",
       "}\n",
       "\n",
       "table > tbody > tr.repr-element > td {\n",
       "  /* Apply a tighter layout to the table cells. */\n",
       "  padding-top: 0.1rem;\n",
       "  padding-bottom: 0.1rem;\n",
       "  padding-right: 1rem;\n",
       "}\n",
       "\n",
       "table > tbody > tr > td.repr-section-toggle-col {\n",
       "  /* Remove background and border of the first cell in every row\n",
       "     (this row is only used for the collapse / uncollapse caret)\n",
       "\n",
       "     TODO: Need to find a good solution for VS Code that works in both\n",
       "           light and dark mode. */\n",
       "  border-color: transparent;\n",
       "  --bs-table-accent-bg: transparent;\n",
       "}\n",
       "\n",
       "tr.repr-section-header {\n",
       "  /* Remove stripes from section header rows */\n",
       "  background-color: transparent;\n",
       "  border-color: transparent;\n",
       "  --bs-table-striped-bg: transparent;\n",
       "  cursor: pointer;\n",
       "}\n",
       "\n",
       "tr.repr-section-header > th {\n",
       "  text-align: left !important;\n",
       "  vertical-align: middle;\n",
       "}\n",
       "\n",
       ".repr-element, tr.repr-element > td {\n",
       "  opacity: 1;\n",
       "  text-align: left !important;\n",
       "}\n",
       "\n",
       ".repr-element-faded {\n",
       "  transition: 0.3s ease;\n",
       "  opacity: 0.2;\n",
       "}\n",
       "\n",
       ".repr-element-collapsed {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "/* Collapse / uncollapse button and the caret it contains. */\n",
       ".repr-section-toggle-col button {\n",
       "  cursor: pointer;\n",
       "  width: 1rem;\n",
       "  background-color: transparent;\n",
       "  border-color: transparent;\n",
       "}\n",
       "\n",
       "span.collapse-uncollapse-caret {\n",
       "  width: 1rem;\n",
       "  height: 1rem;\n",
       "  display: block;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: left;\n",
       "  background-size: contain;\n",
       "}\n",
       "\n",
       "/* The collapse / uncollapse carets were copied from the free Font Awesome collection and adjusted. */\n",
       "\n",
       "/* Default to black carets for light mode */\n",
       ".repr-section-toggle-col > button.collapsed > span.collapse-uncollapse-caret {\n",
       "  background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"black\" d=\"M246.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-9.2-9.2-22.9-11.9-34.9-6.9s-19.8 16.6-19.8 29.6l0 256c0 12.9 7.8 24.6 19.8 29.6s25.7 2.2 34.9-6.9l128-128z\"/></svg>');\n",
       "}\n",
       "\n",
       ".repr-section-toggle-col\n",
       "  > button:not(.collapsed)\n",
       "  > span.collapse-uncollapse-caret {\n",
       "  background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 320 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"black\" d=\"M137.4 374.6c12.5 12.5 32.8 12.5 45.3 0l128-128c9.2-9.2 11.9-22.9 6.9-34.9s-16.6-19.8-29.6-19.8L32 192c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9l128 128z\"/></svg>');\n",
       "}\n",
       "\n",
       "/* Use white carets for dark mode */\n",
       "@media (prefers-color-scheme: dark) {\n",
       "  .repr-section-toggle-col > button.collapsed > span.collapse-uncollapse-caret {\n",
       "    background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"white\" d=\"M246.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-9.2-9.2-22.9-11.9-34.9-6.9s-19.8 16.6-19.8 29.6l0 256c0 12.9 7.8 24.6 19.8 29.6s25.7 2.2 34.9-6.9l128-128z\"/></svg>');\n",
       "  }\n",
       "\n",
       "  .repr-section-toggle-col\n",
       "    > button:not(.collapsed)\n",
       "    > span.collapse-uncollapse-caret {\n",
       "    background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 320 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"white\" d=\"M137.4 374.6c12.5 12.5 32.8 12.5 45.3 0l128-128c9.2-9.2 11.9-22.9 6.9-34.9s-16.6-19.8-29.6-19.8L32 192c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9l128 128z\"/></svg>');\n",
       "  }\n",
       "}\n",
       "\n",
       ".channel-names-btn {\n",
       "  padding: 0;\n",
       "  border: none;\n",
       "  background: none;\n",
       "  text-decoration: underline;\n",
       "  text-decoration-style: dashed;\n",
       "  cursor: pointer;\n",
       "  color: #0d6efd;\n",
       "}\n",
       "\n",
       ".channel-names-btn:hover {\n",
       "  color: #0a58ca;\n",
       "}\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "<table class=\"repr table table-hover table-striped table-sm table-responsive small\">\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header general-317af9d4-d07d-4464-98ed-db1c78c25959\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('general-317af9d4-d07d-4464-98ed-db1c78c25959')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>General</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element general-317af9d4-d07d-4464-98ed-db1c78c25959 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Filename(s)</td>\n",
       "    <td>\n",
       "        \n",
       "        S03.fif\n",
       "        \n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element general-317af9d4-d07d-4464-98ed-db1c78c25959 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>MNE object type</td>\n",
       "    <td>Raw</td>\n",
       "</tr>\n",
       "<tr class=\"repr-element general-317af9d4-d07d-4464-98ed-db1c78c25959 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Measurement date</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-317af9d4-d07d-4464-98ed-db1c78c25959 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Participant</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-317af9d4-d07d-4464-98ed-db1c78c25959 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Experimenter</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header acquisition-afa9e7f3-3537-4247-bca8-dbfa35ccfed6\" \n",
       "    title=\"Hide section\"  onclick=\"toggleVisibility('acquisition-afa9e7f3-3537-4247-bca8-dbfa35ccfed6')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Acquisition</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element acquisition-afa9e7f3-3537-4247-bca8-dbfa35ccfed6 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Duration</td>\n",
       "    <td>00:00:45 (HH:MM:SS)</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-afa9e7f3-3537-4247-bca8-dbfa35ccfed6 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Sampling frequency</td>\n",
       "    <td>250.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-afa9e7f3-3537-4247-bca8-dbfa35ccfed6 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Time points</td>\n",
       "    <td>11,249</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header channels-b91854b6-ee97-4764-b9db-7f92ac458bda\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('channels-b91854b6-ee97-4764-b9db-7f92ac458bda')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Channels</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-b91854b6-ee97-4764-b9db-7f92ac458bda \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>EEG</td>\n",
       "    <td>\n",
       "        <button class=\"channel-names-btn\" onclick=\"alert('Good EEG:\\n\\nC3, CPz, C4')\" title=\"(Click to open in popup)&#13;&#13;C3, CPz, C4\">\n",
       "            3\n",
       "        </button>\n",
       "\n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-b91854b6-ee97-4764-b9db-7f92ac458bda \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Head & sensor digitization</td>\n",
       "    \n",
       "    <td>6 points</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header filters-7e42c938-1f5d-4576-aa53-a8f8b48aa4f2\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('filters-7e42c938-1f5d-4576-aa53-a8f8b48aa4f2')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Filters</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element filters-7e42c938-1f5d-4576-aa53-a8f8b48aa4f2 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Highpass</td>\n",
       "    <td>0.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element filters-7e42c938-1f5d-4576-aa53-a8f8b48aa4f2 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Lowpass</td>\n",
       "    <td>125.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "</table>"
      ],
      "text/plain": [
       "<Raw | S03.fif, 3 x 11249 (45.0 s), ~274 kB, data loaded>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping = {\n",
    "#     'EEG 1': 'FCz',\n",
    "#     'EEG 2': 'C3',\n",
    "#     'EEG 3': 'FC1',\n",
    "#     'EEG 4': 'CPz',\n",
    "#     'EEG 5': 'C2',\n",
    "#     'EEG 6': 'C4'\n",
    "# }\n",
    "\n",
    "# raw.rename_channels(mapping)\n",
    "# Load the standard 10-20 montage\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "\n",
    "# Apply the montage to your raw data\n",
    "raw.set_montage(montage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "9 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 9 events and 1501 original time points ...\n",
      "2 bad epochs dropped\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "(7, 3, 1501) (7,)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 25 (2.2e-16 eps * 3 dim * 3.8e+16  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Times should be between 0.0 and 2.0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m csp \u001b[38;5;241m=\u001b[39m CSP(n_components\u001b[38;5;241m=\u001b[39mncomp, norm_trace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform_into\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsp_space\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m csp\u001b[38;5;241m.\u001b[39mfit(X_all, y)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mcsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_patterns\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcomponents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meeg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m csp_transformed_data[event_id] \u001b[38;5;241m=\u001b[39m csp\u001b[38;5;241m.\u001b[39mtransform(X_all)\n\u001b[0;32m     29\u001b[0m csp_filter_objects[event_id] \u001b[38;5;241m=\u001b[39m csp\n",
      "File \u001b[1;32mc:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\Lib\\site-packages\\mne\\decoding\\csp.py:379\u001b[0m, in \u001b[0;36mCSP.plot_patterns\u001b[1;34m(self, info, components, average, ch_type, scalings, sensors, show_names, mask, mask_params, contours, outlines, sphere, image_interp, extrapolate, border, res, size, cmap, vlim, cnorm, colorbar, cbar_fmt, units, axes, name_format, nrows, ncols, show)\u001b[0m\n\u001b[0;32m    377\u001b[0m patterns \u001b[38;5;241m=\u001b[39m EvokedArray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatterns_\u001b[38;5;241m.\u001b[39mT, info, tmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# the call plot_topomap\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mpatterns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_topomap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mch_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscalings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43msensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontours\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43msphere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msphere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_interp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_interp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextrapolate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextrapolate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mborder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mborder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvlim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvlim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolorbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcbar_fmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcbar_fmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mncols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fig\n",
      "File \u001b[1;32mc:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\Lib\\site-packages\\mne\\evoked.py:666\u001b[0m, in \u001b[0;36mEvoked.plot_topomap\u001b[1;34m(self, times, average, ch_type, scalings, proj, sensors, show_names, mask, mask_params, contours, outlines, sphere, image_interp, extrapolate, border, res, size, cmap, vlim, cnorm, colorbar, cbar_fmt, units, axes, time_unit, time_format, nrows, ncols, show)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;129m@copy_function_doc_to_method_doc\u001b[39m(plot_evoked_topomap)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_topomap\u001b[39m(\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m     show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    665\u001b[0m ):\n\u001b[1;32m--> 666\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplot_evoked_topomap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mch_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvlim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvlim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43msensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolorbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcbar_fmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcbar_fmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontours\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_interp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_interp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextrapolate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextrapolate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43msphere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msphere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mborder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mborder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mncols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\Lib\\site-packages\\mne\\viz\\topomap.py:2181\u001b[0m, in \u001b[0;36mplot_evoked_topomap\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   2179\u001b[0m space \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m evoked\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(times) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mmax\u001b[39m(evoked\u001b[38;5;241m.\u001b[39mtimes) \u001b[38;5;241m+\u001b[39m space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(times) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mmin\u001b[39m(evoked\u001b[38;5;241m.\u001b[39mtimes) \u001b[38;5;241m-\u001b[39m space:\n\u001b[1;32m-> 2181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2182\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimes should be between \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevoked\u001b[38;5;241m.\u001b[39mtimes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2183\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevoked\u001b[38;5;241m.\u001b[39mtimes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2184\u001b[0m     )\n\u001b[0;32m   2185\u001b[0m \u001b[38;5;66;03m# create axes\u001b[39;00m\n\u001b[0;32m   2186\u001b[0m want_axes \u001b[38;5;241m=\u001b[39m n_times \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(colorbar)\n",
      "\u001b[1;31mValueError\u001b[0m: Times should be between 0.0 and 2.0."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import stft\n",
    "import tensorflow as tf\n",
    "\n",
    "all_epochs = mne.Epochs(raw, events[0], event_id=[1,2,3,4, 5],  # No specific event_id filtering\n",
    "                        tmin=-0.5, tmax=5.5, baseline=(-0.5,1), preload=True)\n",
    "all_epochs.pick_types(meg=False, eeg=True)\n",
    "\n",
    "X_all = all_epochs.get_data()\n",
    "save_copy = X_all.copy()\n",
    "\n",
    "event_ids_all = all_epochs.events[:, -1]\n",
    "print(X_all.shape,event_ids_all.shape)\n",
    "csp_filter_objects = {}\n",
    "ncomp = 4\n",
    "csp_transformed_data = {}\n",
    "for event_id in [1,2,3,4,5]:\n",
    "    y = (event_ids_all == event_id).astype(int)\n",
    "    if np.unique(y).size < 2:\n",
    "        print(f\"Skipping event_id {event_id}.\")\n",
    "        continue\n",
    "\n",
    "    csp = CSP(n_components=ncomp, norm_trace=False, transform_into='csp_space')\n",
    "    csp.fit(X_all, y)\n",
    "    csp.plot_patterns(info=raw.info,\n",
    "                  components=[0,1,2,3], ch_type='eeg')\n",
    "    csp_transformed_data[event_id] = csp.transform(X_all)\n",
    "    csp_filter_objects[event_id] = csp\n",
    "\n",
    "import joblib\n",
    "joblib.dump(csp_filter_objects, 'csp_filters_ovr.pkl')\n",
    "print(\"CSP filters saved successfully.\")\n",
    "\n",
    "print('CSP FILTERS DICT:',csp_transformed_data)\n",
    "# Combine CSP features for each trial based on its label\n",
    "n_trials = len(X_all)  # Number of trials\n",
    "n_components = ncomp       # Number of CSP components (assuming 3 for this example)\n",
    "n_time_points = csp_transformed_data[1].shape[2]   # Number of time points in the transformed CSP data\n",
    "\n",
    "# Initialize the combined_features array to hold CSP features for all trials\n",
    "combined_features = np.zeros((n_trials, n_components, n_time_points))\n",
    "\n",
    "# Loop through each trial and assign the CSP-transformed data\n",
    "for i, label in enumerate(event_ids_all):\n",
    "    # Fetch the CSP features for the current trial and class label\n",
    "    # Adjust the indexing based on how your labels and csp_transformed_data are structured\n",
    "    csp_features_for_label = csp_transformed_data.get(label, None)\n",
    "\n",
    "    # Check if the label exists in the dictionary and if the index is within bounds\n",
    "    if csp_features_for_label is not None and i < len(csp_features_for_label):\n",
    "        combined_features[i, :, :] = csp_features_for_label[i]\n",
    "\n",
    "print(combined_features.shape)\n",
    "\n",
    "print(combined_features.shape)\n",
    "y = np.zeros((X_all.shape[0], len(event_ids)))  \n",
    "\n",
    "for i, event_id in enumerate([1,2,3,4,5]):\n",
    "    binary_labels = (event_ids_all == event_id).astype(int)\n",
    "    y[:, i] = binary_labels  \n",
    "print(y)\n",
    "\n",
    "y_flattened = np.argmax(y, axis=1)\n",
    "print(y_flattened)\n",
    "print(y_flattened.tolist().count(0),y_flattened.tolist().count(1),y_flattened.tolist().count(2),y_flattened.tolist().count(3))\n",
    "# clf = Pipeline([('scaler',StandardScaler()),('SVC', SVC())])\n",
    "print('features shape: ',combined_features.shape,y_flattened.shape)\n",
    "# scores = cross_val_score(clf, combined_features, y_flattened, cv=10, scoring='accuracy')\n",
    "# print(\"Multiclass classification accuracy: %f\" % scores.mean())\n",
    "\n",
    "ftrs = featuresarray_load(combined_features)\n",
    "\n",
    "print('features shape: ',ftrs.shape,y_flattened.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(ftrs, y_flattened, train_size=0.9, random_state=42, stratify=y_flattened)\n",
    "\n",
    "\n",
    "from keras.layers import PReLU, Conv1D, Dropout, SpatialDropout1D, MaxPooling1D, GlobalMaxPooling1D, Layer, AveragePooling1D, LSTM, Reshape, BatchNormalization\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "GLOBAL_SHAPE_LENGTH = ftrs.shape[2]\n",
    "# model = tf.keras.models.load_model('1dcnnlstm_model_88p_acc.keras')\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(X_train, y_train, epochs=200, validation_split=0.2, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Scaler saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csp_transformed_data[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# import socket\n",
    "# from stable_baselines3 import DQN\n",
    "# from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n",
    "# from mne.decoding import CSP\n",
    "# import joblib\n",
    "# import tensorflow as tf\n",
    "# # Import your feature extraction function\n",
    "# # Ensure that featuresarray_load is correctly defined or imported above\n",
    "# # from your_feature_module import featuresarray_load  # Adjust the import as necessary\n",
    "\n",
    "# # Load the pre-trained DQN model\n",
    "# model = tf.keras.models.load_model('1dcnnlstm_model_88p_acc.keras')\n",
    "# #DQN.load(\"dqn_plasticity_final\")  # Ensure the path is correct\n",
    "\n",
    "# # Load the pre-trained CSP filters\n",
    "# csp_filters = joblib.load('csp_filters_ovr.pkl')  # Ensure this path is correct\n",
    "# print(\"CSP filters loaded for real-time use.\",csp_filters)\n",
    "\n",
    "# # Load the scaler used during training\n",
    "# scaler = joblib.load('scaler.pkl')  # Ensure this path is correct\n",
    "# print(\"Scaler loaded for real-time use.\")\n",
    "\n",
    "# # Circular Buffer for handling 6 EEG channels\n",
    "# class MultiChannelCircularBuffer:\n",
    "#     def __init__(self, num_channels, buffer_size):\n",
    "#         self.buffer = np.zeros((num_channels, buffer_size))\n",
    "#         self.buffer_size = buffer_size\n",
    "#         self.index = 0\n",
    "\n",
    "#     def add_data(self, data):\n",
    "#         num_samples = data.shape[1]\n",
    "#         if self.index + num_samples <= self.buffer_size:\n",
    "#             self.buffer[:, self.index:self.index + num_samples] = data\n",
    "#         else:\n",
    "#             end_index = (self.index + num_samples) % self.buffer_size\n",
    "#             self.buffer[:, self.index:] = data[:, :self.buffer_size - self.index]\n",
    "#             self.buffer[:, :end_index] = data[:, self.buffer_size - self.index:]\n",
    "#         self.index = (self.index + num_samples) % self.buffer_size\n",
    "\n",
    "#     def get_window(self, window_size):\n",
    "#         if self.index < window_size:\n",
    "#             return np.concatenate((self.buffer[:, -window_size + self.index:], self.buffer[:, :self.index]), axis=1)\n",
    "#         else:\n",
    "#             return self.buffer[:, self.index - window_size:self.index]\n",
    "\n",
    "# # Initialize buffer and window parameters\n",
    "# num_channels = 6\n",
    "# buffer_size = 1250 * 10\n",
    "# window_size = 1250\n",
    "# num_samples = 1250  # 5 seconds of data at 250 Hz\n",
    "\n",
    "# # Initialize the multi-channel buffer\n",
    "# eeg_buffer = MultiChannelCircularBuffer(num_channels, buffer_size)\n",
    "\n",
    "# # Set up BrainFlow for OpenBCI (Cyton board as an example)\n",
    "# params = BrainFlowInputParams()\n",
    "# params.serial_port = 'COM9'  # Replace with your actual COM port for OpenBCI\n",
    "# board = BoardShim(BoardIds.CYTON_BOARD.value, params)\n",
    "# board.prepare_session()\n",
    "# board.start_stream()\n",
    "\n",
    "# def map_action_to_command(action):\n",
    "#     action_mapping = {\n",
    "#         0: 'L',\n",
    "#         1: 'R',\n",
    "#         2: 'F',\n",
    "#         3: 'B'\n",
    "#     }\n",
    "#     return action_mapping.get(action, 'STOP')\n",
    "\n",
    "# # Set up socket connection to Arduino\n",
    "\n",
    "# sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "# def send_command_to_arduino(command):\n",
    "#     try:\n",
    "#         arduino_ip = '192.168.1.168'  # Replace with your Arduino's IP\n",
    "#         arduino_port = 80         # Replace with your Arduino's listening port\n",
    "        \n",
    "#         try:\n",
    "#             sock.connect((arduino_ip, arduino_port))\n",
    "#             print(\"Connected to Arduino\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to connect to Arduino: {e}\")\n",
    "        \n",
    "#         # HTTP GET request\n",
    "#         request = f\"GET /{command} HTTP/1.1\\r\\nHost: {arduino_ip}\\r\\n\\r\\n\"\n",
    "#         sock.send(request.encode())\n",
    "        \n",
    "#         # Close the connection\n",
    "#         sock.close()\n",
    "#         print(f\"Sent command: {command}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error sending command: {e}\")\n",
    "\n",
    "# # Real-time EEG data acquisition and prediction loop\n",
    "# try:\n",
    "#     while True:\n",
    "#         data_count = 0\n",
    "#         data_count = board.get_board_data_count()\n",
    "#         print('data count',data_count)\n",
    "#         if data_count >= num_samples:\n",
    "#             # Retrieve the latest num_samples data points\n",
    "#             data = board.get_current_board_data(num_samples)\n",
    "#             print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "#             # Extract EEG data from the first 6 channels\n",
    "#             eeg_channels = BoardShim.get_eeg_channels(BoardIds.CYTON_BOARD.value)\n",
    "#             eeg_data = data[eeg_channels[:6], :]\n",
    "#             eeg_buffer.add_data(eeg_data)  # Add to buffer\n",
    "\n",
    "#             # Check if buffer has enough data for a prediction window\n",
    "#             if eeg_buffer.index >= window_size:\n",
    "#                 window_data = eeg_buffer.get_window(window_size).reshape(1, num_channels, window_size)\n",
    "\n",
    "#                 # Initialize an empty list to collect CSP features for all classes\n",
    "#                 csp_features_list = []\n",
    "\n",
    "#                 # Apply CSP transform for each class (One-vs-Rest)\n",
    "#                 for class_id, csp in csp_filter_objects.items():\n",
    "#                     # CSP expects data in shape (n_trials, n_channels, n_times)\n",
    "#                     transformed = csp.transform(window_data)  # Shape: (n_trials, n_components)\n",
    "#                     csp_features_list.append(transformed)\n",
    "\n",
    "#                 # Concatenate CSP features from all classes\n",
    "#                 # Resulting shape: (n_trials, n_classes * n_components)\n",
    "#                 csp_ftrs = np.concatenate(csp_features_list, axis=1)\n",
    "                \n",
    "#                 print('cspftrs shape:',csp_ftrs.shape)\n",
    "\n",
    "#                 # Reshape csp_ftrs to match featuresarray_load expected input\n",
    "#                 # Assuming featuresarray_load expects (n_samples, n_features)\n",
    "#                 #csp_ftrs_reshaped = csp_ftrs.reshape(1, -1)  # Shape: (1, n_features)\n",
    "\n",
    "#                 # Extract features using featuresarray_load\n",
    "#                 ftrs = featuresarray_load(csp_ftrs)  # Adjust if necessary\n",
    "\n",
    "#                 # Apply the pre-fitted scaler\n",
    "#                 ftrs = scaler.transform(ftrs.reshape(-1, ftrs.shape[-1])).reshape(ftrs.shape)\n",
    "\n",
    "#                 print(ftrs.shape)\n",
    "#                 # Predict action with the DQN model\n",
    "#                 action, _states = model.predict(ftrs[:, 0, :], deterministic=True)\n",
    "#                 print(action)\n",
    "#                 # Map action to command and send to Arduino\n",
    "#                 command = map_action_to_command(int(action))\n",
    "#                 send_command_to_arduino(command)\n",
    "\n",
    "#             time.sleep(0.1)  # Control loop frequency (e.g., 10 Hz)\n",
    "#         else:\n",
    "#             pass\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Stopped by user.\")\n",
    "# finally:\n",
    "#     sock.close()\n",
    "#     board.stop_stream()\n",
    "#     board.release_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "class KerasDQNPolicy(DQNPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(KerasDQNPolicy, self).__init__(*args, **kwargs)\n",
    "        self.keras_model = Sequential([\n",
    "            Reshape((GLOBAL_SHAPE_LENGTH,ncomp)),\n",
    "            BatchNormalization(),\n",
    "\n",
    "            Conv1D(32, kernel_size=3),\n",
    "            BatchNormalization(),\n",
    "            PReLU(),\n",
    "            \n",
    "\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            SpatialDropout1D(0.1),\n",
    "\n",
    "            Conv1D(64, kernel_size=3),\n",
    "            BatchNormalization(),\n",
    "            PReLU(),\n",
    "            AveragePooling1D(pool_size=2),\n",
    "            SpatialDropout1D(0.1),\n",
    "\n",
    "            LSTM(64, activation='tanh', recurrent_regularizer=l1_l2(l1=0.01, l2=0.01),return_sequences=True),\n",
    "            BatchNormalization(),\n",
    "            GlobalMaxPooling1D(),\n",
    "            BatchNormalization(),\n",
    "            Dense(units=64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.1),\n",
    "            Dense(units=32, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.1),\n",
    "            Dense(units=5, activation='linear')\n",
    "        ])\n",
    "    def q_values(self, obs):\n",
    "        return self.keras_model.predict(np.array(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Plasticity(gym.Env):\n",
    "    def __init__(self, images_per_episode=1, dataset=(X_train, y_train), random=True):\n",
    "        super(Plasticity, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(5)  # 4 actions\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                                shape=(ncomp, GLOBAL_SHAPE_LENGTH),\n",
    "                                                dtype=np.float32)\n",
    "        self.images_per_episode = images_per_episode\n",
    "        self.step_count = 0\n",
    "        self.x, self.y = dataset\n",
    "        self.random = random\n",
    "        self.dataset_idx = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = self.calculate_reward(action)\n",
    "        obs = self._next_obs()\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.images_per_episode:\n",
    "            done = True\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "        return self._next_obs()\n",
    "\n",
    "    def _next_obs(self):\n",
    "        if self.random:\n",
    "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
    "            self.expected_action = int(self.y[next_obs_idx])\n",
    "            obs = self.x[next_obs_idx]\n",
    "        else:\n",
    "            obs = self.x[self.dataset_idx]\n",
    "            self.expected_action = int(self.y[self.dataset_idx])\n",
    "            self.dataset_idx = (self.dataset_idx) % (len(X_train))\n",
    "            # if self.dataset_idx >= len(self.x):\n",
    "            #     raise StopIteration()\n",
    "        return obs\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        if action == self.expected_action:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return -1.0  # Negative reward for incorrect actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = Plasticity(images_per_episode=1, dataset=(X_train, y_train), random=True)\n",
    "\n",
    "# Initialize the DQN agent with custom policy\n",
    "model = DQN(\n",
    "    KerasDQNPolicy,  # Use custom policy instead of \"MlpPolicy\"\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=5e-4,\n",
    "    buffer_size=50000,\n",
    "    learning_starts=100,\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=200,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    tensorboard_log=\"./dqn_plasticity_tensorboard/\"\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=1000,\n",
    "    save_path='./models/',\n",
    "    name_prefix='dqn_plasticity'\n",
    ")\n",
    "\n",
    "eval_env = Plasticity(images_per_episode=1, dataset=(X_test, y_test), random=False)\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path='./logs/',\n",
    "    log_path='./logs/',\n",
    "    eval_freq=1000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % 1000 == 0:\n",
    "            print(f\"Step: {self.n_calls}\")\n",
    "        return True\n",
    "\n",
    "custom_callback = CustomCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=2500,\n",
    "    callback=[checkpoint_callback, eval_callback, custom_callback]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"dqn_plasticity_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model (if needed)\n",
    "model = DQN.load(\"dqn_plasticity_final\", env=env)\n",
    "eval_env = Plasticity(images_per_episode=1, dataset=(X_test, y_test), random=False)\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100,deterministic=True)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# import joblib\n",
    "# import tensorflow as tf\n",
    "# from mne.decoding import CSP\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import (\n",
    "#     PReLU, Conv1D, Dropout, SpatialDropout1D, MaxPooling1D, \n",
    "#     GlobalMaxPooling1D, BatchNormalization, LSTM, Dense, Reshape\n",
    "# )\n",
    "# from keras.regularizers import l1_l2\n",
    "\n",
    "# # Import your feature extraction function\n",
    "# # Ensure that featuresarray_load is correctly defined or imported above\n",
    "# # from your_feature_module import featuresarray_load  # Adjust the import as necessary\n",
    "\n",
    "\n",
    "# # ==========================\n",
    "# # Load Pre-trained Models and Scalers\n",
    "# # ==========================\n",
    "\n",
    "# # Load the pre-trained Keras model\n",
    "# cnnlsmt_model = tf.keras.models.load_model('1dcnnlstm_model_88p_acc.keras')\n",
    "\n",
    "# # Load the pre-trained CSP filters (assuming One-vs-Rest CSP)\n",
    "# csp_filters = joblib.load('csp_filters_ovr.pkl')  # Ensure this path is correct\n",
    "# print(\"CSP filters loaded for simulated use.\",csp_filter_objects)\n",
    "\n",
    "# # Load the scaler used during training\n",
    "# scaler = joblib.load('scaler.pkl')  # Ensure this path is correct\n",
    "# print(\"Scaler loaded for simulated use.\")\n",
    "\n",
    "# # ==========================\n",
    "# # Define the Circular Buffer\n",
    "# # ==========================\n",
    "\n",
    "# class MultiChannelCircularBuffer:\n",
    "#     def __init__(self, num_channels, buffer_size):\n",
    "#         self.buffer = np.zeros((num_channels, buffer_size))\n",
    "#         self.buffer_size = buffer_size\n",
    "#         self.index = 0\n",
    "\n",
    "#     def add_data(self, data):\n",
    "#         num_samples = data.shape[1]\n",
    "#         if self.index + num_samples <= self.buffer_size:\n",
    "#             self.buffer[:, self.index:self.index + num_samples] = data\n",
    "#         else:\n",
    "#             end_index = (self.index + num_samples) % self.buffer_size\n",
    "#             self.buffer[:, self.index:] = data[:, :self.buffer_size - self.index]\n",
    "#             self.buffer[:, :end_index] = data[:, self.buffer_size - self.index:]\n",
    "#         self.index = (self.index + num_samples) % self.buffer_size\n",
    "\n",
    "#     def get_window(self, window_size):\n",
    "#         if self.index < window_size:\n",
    "#             return np.concatenate((self.buffer[:, -window_size + self.index:], self.buffer[:, :self.index]), axis=1)\n",
    "#         else:\n",
    "#             return self.buffer[:, self.index - window_size:self.index]\n",
    "\n",
    "# # ==========================\n",
    "# # Simulate EEG Data Generation\n",
    "# # ==========================\n",
    "\n",
    "# def generate_simulated_eeg(num_channels, num_samples, fs=250):\n",
    "#     \"\"\"\n",
    "#     Generate simulated EEG data with alpha and beta rhythms.\n",
    "    \n",
    "#     :param num_channels: Number of EEG channels\n",
    "#     :param num_samples: Number of samples per channel\n",
    "#     :param fs: Sampling frequency in Hz\n",
    "#     :return: Simulated EEG data as a numpy array of shape (num_channels, num_samples)\n",
    "#     \"\"\"\n",
    "#     t = np.linspace(0, num_samples / fs, num_samples, endpoint=False)\n",
    "#     eeg_data = np.zeros((num_channels, num_samples))\n",
    "    \n",
    "#     # Define frequency components for each channel\n",
    "#     for ch in range(num_channels):\n",
    "#         # Simulate alpha (10 Hz) and beta (20 Hz) rhythms with some random noise\n",
    "#         alpha_freq = 10 + np.random.uniform(-1, 1)\n",
    "#         beta_freq = 20 + np.random.uniform(-1, 1)\n",
    "#         eeg_data[ch, :] = (\n",
    "#             np.sin(2 * np.pi * alpha_freq * t) +\n",
    "#             0.5 * np.sin(2 * np.pi * beta_freq * t) +\n",
    "#             0.3 * np.random.randn(num_samples)  # Additive Gaussian noise\n",
    "#         )\n",
    "#     return eeg_data\n",
    "\n",
    "# # ==========================\n",
    "# # Define Action Mapping (No Socket)\n",
    "# # ==========================\n",
    "\n",
    "# def map_action_to_command(action):\n",
    "#     \"\"\"\n",
    "#     Map the model's predicted action to a command.\n",
    "    \n",
    "#     :param action: Integer representing the action\n",
    "#     :return: Command string\n",
    "#     \"\"\"\n",
    "#     action_mapping = {\n",
    "#         0: 'L',    # Left\n",
    "#         1: 'R',    # Right\n",
    "#         2: 'F',    # Forward\n",
    "#         3: 'B'     # Backward\n",
    "#     }\n",
    "#     return action_mapping.get(action, 'STOP')\n",
    "\n",
    "# # ==========================\n",
    "# # Initialize Buffer and Parameters\n",
    "# # ==========================\n",
    "\n",
    "# num_channels = 6\n",
    "# fs = 250  # Sampling frequency\n",
    "# buffer_size = fs * 10  # 10 seconds buffer\n",
    "# window_size = fs * 5   # 5 seconds window\n",
    "# num_samples = fs * 5    # 5 seconds of data per iteration\n",
    "\n",
    "# # Initialize the multi-channel buffer\n",
    "# eeg_buffer = MultiChannelCircularBuffer(num_channels, buffer_size)\n",
    "\n",
    "# # ==========================\n",
    "# # Initialize CSP Filters (One-vs-Rest)\n",
    "# # ==========================\n",
    "\n",
    "# # Assuming 'csp_filters' is a dictionary with class IDs as keys and CSP objects as values\n",
    "# csp_filter_objects = csp_filters  # Rename for clarity\n",
    "\n",
    "# # ==========================\n",
    "# # Simulated Prediction Loop\n",
    "# # ==========================\n",
    "\n",
    "# try:\n",
    "#     while True:\n",
    "#         # Simulate data acquisition delay\n",
    "#         time.sleep(0.1)  # Control loop frequency (e.g., 10 Hz)\n",
    "        \n",
    "#         # Generate simulated EEG data\n",
    "#         simulated_data = generate_simulated_eeg(num_channels, num_samples, fs=fs)\n",
    "#         print(f\"Simulated Data Shape: {simulated_data.shape}\")\n",
    "        \n",
    "#         # Add simulated data to buffer\n",
    "#         eeg_buffer.add_data(simulated_data)\n",
    "        \n",
    "#         # Check if buffer has enough data for a prediction window\n",
    "#         if eeg_buffer.index >= window_size:\n",
    "#             # Retrieve the latest window_size data points\n",
    "#             window_data = eeg_buffer.get_window(window_size).reshape(1, num_channels, window_size)\n",
    "#             print(f\"Window Data Shape: {window_data.shape}\")\n",
    "            \n",
    "#             # Initialize an empty list to collect CSP features for all classes\n",
    "#             csp_features_list = []\n",
    "            \n",
    "#             # Apply CSP transform for each class (One-vs-Rest)\n",
    "#             for class_id, csp in csp_filter_objects.items():\n",
    "#                 # CSP expects data in shape (n_trials, n_channels, n_times)\n",
    "#                 transformed = csp.transform(window_data)  # Shape: (n_trials, n_components)\n",
    "#                 csp.plot_patterns(info=raw.info,\n",
    "#                   components=list(range(ncomp)), ch_type='eeg')\n",
    "#                 csp_features_list.append(transformed)\n",
    "            \n",
    "#             # Concatenate CSP features from all classes\n",
    "#             # Resulting shape: (n_trials, n_classes * n_components)\n",
    "#             csp_ftrs = np.concatenate(csp_features_list, axis=1)\n",
    "#             print(f\"CSP Features Shape: {csp_ftrs.shape}\")\n",
    "            \n",
    "#             # Extract features using featuresarray_load\n",
    "#             ftrs = featuresarray_load(csp_ftrs)\n",
    "#             print(f\"Extracted Features Shape: {ftrs.shape}\")\n",
    "#             predictions = []\n",
    "#             # Apply the pre-fitted scaler\n",
    "#             print('Entering ensenble loop...')\n",
    "#             for i in range(0,16,4):\n",
    "#                 ftrs_scaled = (scaler.transform(ftrs.reshape(-1, ftrs.shape[-1])).reshape(ftrs.shape))\n",
    "#                 ftrs_scaled = ftrs_scaled[:,i:i+4,:]\n",
    "                \n",
    "#                 print(ftrs_scaled.shape)\n",
    "#                 # Predict action with the pre-trained model\n",
    "#                 # Assuming the model expects input shape compatible with ftrs_scaled\n",
    "#                 # Here, adjust the input shape as per your model's requirement\n",
    "#                 # For example, if model expects (batch_size, time_steps, features), reshape accordingly\n",
    "#                 # Here, assuming ftrs_scaled is already in the correct shape\n",
    "#                 a,b = model.predict(ftrs_scaled)\n",
    "#                 print('a',a,'b',b)\n",
    "#                 print('prediction',a[0])\n",
    "#                 predictions.append(a[0])\n",
    "                \n",
    "#                 # Map action to command\n",
    "#             predictions = np.array(predictions)\n",
    "#             print('prediction array', predictions)\n",
    "#             action = np.bincount(predictions).argmax()\n",
    "#             print(f\"Predicted Action: {action}\")\n",
    "#             command = map_action_to_command(int(action))\n",
    "#             print(f\"Mapped Command: {command}\")\n",
    "            \n",
    "#             # Here, instead of sending the command to Arduino, we simply print it\n",
    "#             # You can add additional handling if needed\n",
    "#             # send_command_to_arduino(command)  # Removed socket communication\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Stopped by user.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# import socket\n",
    "# from stable_baselines3 import DQN\n",
    "# from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n",
    "# from mne.decoding import CSP\n",
    "# import joblib\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Constants\n",
    "# OVERLAP_RATIO = 0.8\n",
    "# SMOOTHING_WINDOW = 5\n",
    "# MIN_CONFIDENCE = 0.3\n",
    "# MAX_CONFIDENCE = 0.8\n",
    "# COMMAND_DURATION = 0.5\n",
    "\n",
    "# # Load the pre-trained DQN model\n",
    "# model = DQN.load(\"dqn_plasticity_final\")\n",
    "# print(\"DQN model loaded.\")\n",
    "\n",
    "# # Load the pre-trained CSP filters\n",
    "# csp_filters = joblib.load('csp_filters_ovr.pkl')\n",
    "# print(\"CSP filters loaded for real-time use.\")\n",
    "\n",
    "# # Load the scaler used during training\n",
    "# scaler = joblib.load('scaler.pkl')\n",
    "# print(\"Scaler loaded for real-time use.\")\n",
    "\n",
    "# class MultiChannelCircularBuffer:\n",
    "#     def __init__(self, num_channels, buffer_size, window_size=1250, overlap_ratio=0.8):\n",
    "#         self.buffer = np.zeros((num_channels, buffer_size))\n",
    "#         self.buffer_size = buffer_size\n",
    "#         self.window_size = window_size  # Size of analysis window (e.g., 5s = 1250 samples)\n",
    "#         self.overlap_samples = int(window_size * overlap_ratio)  # Number of samples to overlap\n",
    "#         self.step_size = window_size - self.overlap_samples  # Number of new samples needed\n",
    "#         self.index = 0\n",
    "#         self.last_window_end = 0\n",
    "        \n",
    "#         print(f\"Initialized circular buffer:\")\n",
    "#         print(f\"- Buffer size: {buffer_size} samples\")\n",
    "#         print(f\"- Window size: {window_size} samples ({window_size/250:.1f}s)\")\n",
    "#         print(f\"- Overlap: {overlap_ratio*100:.0f}% ({self.overlap_samples} samples)\")\n",
    "#         print(f\"- Step size: {self.step_size} samples ({self.step_size/250:.3f}s)\")\n",
    "\n",
    "#     def add_data(self, data):\n",
    "#         \"\"\"Add new data to buffer and return True if enough new samples for next window\"\"\"\n",
    "#         num_samples = data.shape[1]\n",
    "        \n",
    "#         # Add data to buffer\n",
    "#         if self.index + num_samples <= self.buffer_size:\n",
    "#             self.buffer[:, self.index:self.index + num_samples] = data\n",
    "#         else:\n",
    "#             # Handle wrap-around\n",
    "#             end_index = (self.index + num_samples) % self.buffer_size\n",
    "#             self.buffer[:, self.index:] = data[:, :self.buffer_size - self.index]\n",
    "#             self.buffer[:, :end_index] = data[:, self.buffer_size - self.index:]\n",
    "            \n",
    "#         self.index = (self.index + num_samples) % self.buffer_size\n",
    "        \n",
    "#         # Check if we have enough new samples since last window\n",
    "#         samples_since_last = (self.index - self.last_window_end) % self.buffer_size\n",
    "#         return samples_since_last >= self.step_size\n",
    "\n",
    "#     def get_window(self):\n",
    "#         \"\"\"Get the latest window of data with overlap\"\"\"\n",
    "#         # Update last window position\n",
    "#         self.last_window_end = self.index\n",
    "        \n",
    "#         # Calculate start position for window\n",
    "#         start_idx = (self.index - self.window_size) % self.buffer_size\n",
    "        \n",
    "#         # Handle wrap-around case\n",
    "#         if start_idx < self.index:\n",
    "#             return self.buffer[:, start_idx:self.index]\n",
    "#         else:\n",
    "#             return np.concatenate((\n",
    "#                 self.buffer[:, start_idx:],\n",
    "#                 self.buffer[:, :self.index]\n",
    "#             ), axis=1)\n",
    "\n",
    "# class ArduinoConnection:\n",
    "#     def __init__(self, host, port):\n",
    "#         self.host = host\n",
    "#         self.port = port\n",
    "#         self.sock = None\n",
    "        \n",
    "#     def connect(self):\n",
    "#         if not self.sock:\n",
    "#             self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "#             self.sock.connect((self.host, self.port))\n",
    "    \n",
    "#     def send_command(self, command):\n",
    "#         if not self.sock:\n",
    "#             self.connect()\n",
    "#         try:\n",
    "#             self.sock.send(f\"GET /{command} HTTP/1.1\\r\\nHost: {self.host}\\r\\n\\r\\n\".encode())\n",
    "#             return self.sock.recv(1024).decode()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Connection error: {e}\")\n",
    "#             self.sock = None\n",
    "#             return None\n",
    "    \n",
    "#     def close(self):\n",
    "#         if self.sock:\n",
    "#             self.sock.close()\n",
    "#             self.sock = None\n",
    "\n",
    "# def map_action_to_command(action):\n",
    "#     action_mapping = {\n",
    "#         0: 'L',\n",
    "#         1: 'R',\n",
    "#         2: 'F',\n",
    "#         3: 'B'\n",
    "#     }\n",
    "#     return action_mapping.get(action, 'S')\n",
    "\n",
    "# # Initialize components\n",
    "# num_channels = 6\n",
    "# buffer_size = 1250 * 10  # 10 seconds buffer\n",
    "# window_size = 1250       # 5 seconds window\n",
    "# samples_per_read = 250   # Read 1 second of data at a time\n",
    "\n",
    "# # Initialize the buffer with overlap parameters\n",
    "# eeg_buffer = MultiChannelCircularBuffer(\n",
    "#     num_channels=num_channels,\n",
    "#     buffer_size=buffer_size,\n",
    "#     window_size=window_size,\n",
    "#     overlap_ratio=OVERLAP_RATIO\n",
    "# )\n",
    "\n",
    "# # Set up BrainFlow\n",
    "# params = BrainFlowInputParams()\n",
    "# params.serial_port = 'COM9'  # Replace with your actual COM port\n",
    "# board = BoardShim(BoardIds.CYTON_BOARD.value, params)\n",
    "# board.prepare_session()\n",
    "# print(\"BrainFlow session prepared.\")\n",
    "# board.start_stream()\n",
    "# print(\"Started data stream from board.\")\n",
    "\n",
    "# try:\n",
    "#     # Initialize Arduino connection\n",
    "#     arduino = ArduinoConnection('192.168.1.168', 80)\n",
    "    \n",
    "#     # Initialize state variables\n",
    "#     prediction_history = []\n",
    "#     last_state = None\n",
    "#     last_command = 'S'\n",
    "#     last_command_time = time.time()\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     while True:\n",
    "#         # Get new data (1 second chunks)\n",
    "#         data = board.get_current_board_data(samples_per_read)\n",
    "#         eeg_channels = BoardShim.get_eeg_channels(BoardIds.CYTON_BOARD.value)\n",
    "#         eeg_data = data[eeg_channels[:6], :]\n",
    "\n",
    "#         # Add data and check if we have enough for next window\n",
    "#         if eeg_buffer.add_data(eeg_data):\n",
    "#             # Get overlapped window and process\n",
    "#             window_data = eeg_buffer.get_window().reshape(1, num_channels, -1)\n",
    "#             print(f\"Processing window of shape: {window_data.shape}\")\n",
    "            \n",
    "#             # Initialize CSP features list\n",
    "#             csp_features_list = []\n",
    "            \n",
    "#             # Apply CSP transform for each class\n",
    "#             for class_id, csp in csp_filters.items():\n",
    "#                 transformed = csp.transform(window_data)\n",
    "#                 # if hasattr(raw, 'info'):  # Only plot if raw.info is available\n",
    "#                 #     csp.plot_patterns(info=raw.info, components=list(range(4)), ch_type='eeg')\n",
    "#                 csp_features_list.append(transformed)\n",
    "            \n",
    "#             # Concatenate CSP features\n",
    "#             csp_ftrs = np.concatenate(csp_features_list, axis=1)\n",
    "#             print(f\"CSP Features Shape: {csp_ftrs.shape}\")\n",
    "            \n",
    "#             # Extract features\n",
    "#             ftrs = featuresarray_load(csp_ftrs)\n",
    "#             print(f\"Extracted Features Shape: {ftrs.shape}\")\n",
    "            \n",
    "#             predictions = []\n",
    "#             # Apply scaling and make predictions\n",
    "#             print('Entering ensemble loop...')\n",
    "#             for i in range(0, 20, 4):\n",
    "#                 ftrs_scaled = scaler.transform(ftrs.reshape(-1, ftrs.shape[-1])).reshape(ftrs.shape)\n",
    "#                 ftrs_scaled = ftrs_scaled[:, i:i+4, :]\n",
    "#                 print(f\"Scaled Features Shape: {ftrs_scaled.shape}\")\n",
    "                \n",
    "#                 # Get prediction\n",
    "#                 action, _ = model.predict(ftrs_scaled, deterministic=True)\n",
    "#                 print(f'Prediction: {action[0]}')\n",
    "#                 predictions.append(action[0])\n",
    "            \n",
    "#             # Aggregate predictions\n",
    "#             predictions = np.array(predictions)\n",
    "#             print('Prediction array:', predictions)\n",
    "#             action = np.bincount(predictions).argmax()\n",
    "            \n",
    "#             # Replace the current confidence calculation with:\n",
    "#             # Replace the current confidence calculation with:\n",
    "#             q_values = model.q_net(model.q_net.obs_to_tensor(ftrs_scaled)[0])[0].detach().numpy()\n",
    "#             # Normalize Q-values to be between 0 and 1\n",
    "#             q_values = (q_values - np.min(q_values)) / (np.max(q_values) - np.min(q_values) + 1e-6)\n",
    "#             max_q = np.max(q_values)\n",
    "#             other_q_values = q_values[q_values != max_q]\n",
    "#             mean_other_q = np.mean(other_q_values) if len(other_q_values) > 0 else 0\n",
    "#             confidence = float(max_q - mean_other_q)  # Will now be between 0 and 1\n",
    "#             print(f\"Confidence {confidence}\")\n",
    "            \n",
    "            \n",
    "#             # Update prediction history\n",
    "#             prediction_history.append((action, confidence))\n",
    "#             if len(prediction_history) > SMOOTHING_WINDOW:\n",
    "#                 prediction_history.pop(0)\n",
    "            \n",
    "#             # State change detection\n",
    "#             # State change detection\n",
    "#             current_time = time.time()\n",
    "#             if len(prediction_history) >= SMOOTHING_WINDOW:\n",
    "#                 recent_predictions = [p[0] for p in prediction_history[-SMOOTHING_WINDOW:]]\n",
    "#                 recent_confidences = [p[1] for p in prediction_history[-SMOOTHING_WINDOW:]]\n",
    "                \n",
    "#                 most_common = max(set(recent_predictions), key=recent_predictions.count)\n",
    "#                 avg_confidence = np.mean(recent_confidences)\n",
    "                \n",
    "#                 if (most_common != last_state and \n",
    "#                     recent_predictions.count(most_common) >= SMOOTHING_WINDOW * 0.6 and  # At least 60% agreement\n",
    "#                     MIN_CONFIDENCE <= avg_confidence):\n",
    "                    \n",
    "#                     last_state = most_common\n",
    "#                     command = map_action_to_command(int(most_common))\n",
    "#                     last_command = command\n",
    "#                     last_command_time = current_time\n",
    "#                     print(f\"[{current_time - start_time:.2f}s] State Change: {most_common}, \"\n",
    "#                         f\"Confidence: {avg_confidence:.2f}, Command: {command}\")\n",
    "#                     arduino.send_command(command)\n",
    "                    \n",
    "#         time.sleep(0.02)  # 20ms sleep\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\nStopped by user.\")\n",
    "#     arduino.send_command('S')\n",
    "    \n",
    "# finally:\n",
    "#     arduino.close()\n",
    "#     board.stop_stream()\n",
    "#     board.release_session()\n",
    "#     arduino.send_command('S')\n",
    "    \n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     iterations = len(prediction_history)\n",
    "#     print(f\"\\nRun complete:\")\n",
    "#     print(f\"- Total time: {elapsed_time:.1f} seconds\")\n",
    "#     print(f\"- Iterations: {iterations}\")\n",
    "#     print(f\"- Average rate: {iterations/elapsed_time:.1f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW CODE TESTING WITH SIMULATED SIGNALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN model loaded.\n",
      "CSP filters loaded for real-time use.\n",
      "Scaler loaded for real-time use.\n",
      "Initialized circular buffer:\n",
      "- Buffer size: 12500 samples (50.0s)\n",
      "- Window size: 1250 samples (5.0s)\n",
      "- Overlap: 65% (812 samples)\n",
      "- Step size: 438 samples (1.752s)\n",
      "BrainFlow session prepared.\n",
      "Started data stream from board.\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: B [1, 3, 3, 3, 2][0.9008080363273621, 0.6786321401596069, 0.890689492225647, 0.6679656505584717, 0.8210822343826294]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: B\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: R [2, 4, 1, 3][0.9080088138580322, 0.8935555219650269, 0.6532490253448486, 0.9372038245201111]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: R\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: B [0, 2, 3, 4, 2][0.7449016571044922, 0.8029767274856567, 0.6020006537437439, 0.7023704051971436, 0.7261606454849243]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: B\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: B [2, 4, 0, 2, 3][0.8002949357032776, 0.7584376931190491, 0.7009528875350952, 0.6416471004486084, 0.6364317536354065]\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: S [2, 0, 4, 1, 0][0.7472045421600342, 0.7237077951431274, 0.6298294067382812, 0.6927086114883423, 0.638891339302063]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: S\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: L [4, 2, 0][0.7169185876846313, 0.7361514568328857, 0.6162987947463989]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: L\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: S [4, 2, 4, 0][0.6491369009017944, 0.7886979579925537, 0.7060648202896118, 0.662956714630127]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: S\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: F [1, 2, 2][0.7558062672615051, 0.6906523108482361, 0.6258904933929443]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: F\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: L [0, 1, 0, 0][0.6452878713607788, 0.7651331424713135, 0.7096046209335327, 0.6186007261276245]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: L\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: F [2, 1, 3, 2][0.637076735496521, 0.7505974173545837, 0.6956907510757446, 0.628385603427887]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: F\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: S [4, 3, 1][0.6001037955284119, 0.682364821434021, 0.6352534890174866]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: S\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: R [3, 0, 1][0.6860920190811157, 0.7563314437866211, 0.6347578763961792]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: R\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: S [2, 2, 2, 4][0.6922863721847534, 0.7511255145072937, 0.7524460554122925, 0.6189796924591064]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: S\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: F [2, 3, 0, 0, 3][0.6050239205360413, 0.8272736072540283, 0.7754048109054565, 0.6503388285636902, 0.6223107576370239]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: F\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: R [0, 4, 3, 1][0.6524127721786499, 0.691303014755249, 0.7877652049064636, 0.6243489384651184]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: R\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: L [4, 1, 4, 0][0.7400450706481934, 0.6206245422363281, 0.7290728092193604, 0.6116776466369629]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: L\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: L [0, 2, 3, 0][0.7656468152999878, 0.7909751534461975, 0.713638961315155, 0.6437740325927734]\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: L [3, 3, 0, 3, 4][0.776016891002655, 0.7644132971763611, 0.6057291626930237, 0.7114435434341431, 0.6365562081336975]\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: R [2, 4, 4, 0, 1][0.7808387279510498, 0.7828247547149658, 0.6357758045196533, 0.682682991027832, 0.6242810487747192]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: R\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: L [2, 0, 1, 2, 0][0.8310971260070801, 0.6915891170501709, 0.6860488057136536, 0.6649834513664246, 0.6271945238113403]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: L\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: F [1, 3, 3, 2][0.7498396039009094, 0.8042521476745605, 0.7197936773300171, 0.6770046949386597]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: F\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: R [0, 2, 3, 1][0.7411633133888245, 0.8031989932060242, 0.7260718941688538, 0.7017073035240173]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: R\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: F [4, 4, 2, 1][0.7629351615905762, 0.79195237159729, 0.7029294371604919, 0.7177534699440002]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: F\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: B [4, 1, 3, 3][0.7083538770675659, 0.651293158531189, 0.7184045314788818, 0.6451781392097473]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: B\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: R [1, 1, 3][0.6032655835151672, 0.6578867435455322, 0.8937358260154724]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: R\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: S [3, 4, 0][0.6315822601318359, 0.6047475934028625, 0.674934446811676]\n",
      "Stopping for 1 second...\n",
      "Changed to new command: S\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: S [4, 4, 4][0.6315822601318359, 0.6047475934028625, 0.674934446811676]\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: F [4, 2, 2][0.6315822601318359, 0.6047475934028625, 0.674934446811676]\n",
      "Stopping for 1 second...\n",
      "HTTP request error: HTTPConnectionPool(host='192.168.1.168', port=80): Max retries exceeded with url: /S (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002D71D5BC6E0>, 'Connection to 192.168.1.168 timed out. (connect timeout=1)'))\n",
      "HTTP request error: HTTPConnectionPool(host='192.168.1.168', port=80): Max retries exceeded with url: /F (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002D71D5BD550>, 'Connection to 192.168.1.168 timed out. (connect timeout=1)'))\n",
      "Changed to new command: F\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: F [2, 2, 4][0.6315822601318359, 0.6047475934028625, 0.674934446811676]\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: S [2, 4, 1][0.6315822601318359, 0.6047475934028625, 0.674934446811676]\n",
      "Stopping for 1 second...\n",
      "HTTP request error: HTTPConnectionPool(host='192.168.1.168', port=80): Max retries exceeded with url: /S (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002D71D5BC230>, 'Connection to 192.168.1.168 timed out. (connect timeout=1)'))\n",
      "HTTP request error: HTTPConnectionPool(host='192.168.1.168', port=80): Max retries exceeded with url: /S (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002D71D5BE6F0>, 'Connection to 192.168.1.168 timed out. (connect timeout=1)'))\n",
      "Changed to new command: S\n",
      "Processing window of shape: (1, 6, 1250)\n",
      "CSP Features Shape: (1, 20, 1250)\n",
      "Extracted Features Shape: (1, 20, 522)\n",
      "Mapped Command: B [1, 3, 0][0.6315822601318359, 0.6047475934028625, 0.674934446811676]\n",
      "Stopping for 1 second...\n",
      "HTTP request error: HTTPConnectionPool(host='192.168.1.168', port=80): Max retries exceeded with url: /S (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002D71D5175F0>, 'Connection to 192.168.1.168 timed out. (connect timeout=1)'))\n",
      "\n",
      "Stopped by user.\n"
     ]
    },
    {
     "ename": "BrainFlowError",
     "evalue": "BOARD_WRITE_ERROR:4 unable to stop streaming session",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrainFlowError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 328\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     simulation\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 328\u001b[0m     \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m     board\u001b[38;5;241m.\u001b[39mrelease_session()\n\u001b[0;32m    330\u001b[0m     simulation\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\Lib\\site-packages\\brainflow\\board_shim.py:1261\u001b[0m, in \u001b[0;36mBoardShim.stop_stream\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1259\u001b[0m res \u001b[38;5;241m=\u001b[39m BoardControllerDLL\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mstop_stream(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_json)\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m!=\u001b[39m BrainFlowExitCodes\u001b[38;5;241m.\u001b[39mSTATUS_OK\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m-> 1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BrainFlowError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munable to stop streaming session\u001b[39m\u001b[38;5;124m'\u001b[39m, res)\n",
      "\u001b[1;31mBrainFlowError\u001b[0m: BOARD_WRITE_ERROR:4 unable to stop streaming session"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import socket\n",
    "from stable_baselines3 import DQN\n",
    "from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n",
    "from mne.decoding import CSP\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import mne\n",
    "import torch\n",
    "\n",
    "MIN_CONFIDENCE = 0.3\n",
    "MAX_CONFIDENCE = 0.8\n",
    "COMMAND_DURATION = 0.5\n",
    "\n",
    "# Load the pre-trained DQN model\n",
    "model = DQN.load(\"dqn_plasticity_final\")\n",
    "print(\"DQN model loaded.\")\n",
    "\n",
    "# Load the pre-trained CSP filters\n",
    "csp_filters = joblib.load('csp_filters_ovr.pkl')\n",
    "print(\"CSP filters loaded for real-time use.\")\n",
    "\n",
    "# Load the scaler used during training\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "print(\"Scaler loaded for real-time use.\")\n",
    "\n",
    "class MultiChannelCircularBuffer:\n",
    "    def __init__(self, num_channels, buffer_size, window_size=1250, overlap_ratio=0.8):\n",
    "        self.buffer = np.zeros((num_channels, buffer_size))\n",
    "        self.buffer_size = buffer_size\n",
    "        self.window_size = window_size\n",
    "        self.overlap_samples = int(window_size * overlap_ratio)\n",
    "        self.step_size = window_size - self.overlap_samples\n",
    "        self.index = 0\n",
    "        self.last_window_end = 0\n",
    "        \n",
    "        print(f\"Initialized circular buffer:\")\n",
    "        print(f\"- Buffer size: {buffer_size} samples ({buffer_size/250:.1f}s)\")\n",
    "        print(f\"- Window size: {window_size} samples ({window_size/250:.1f}s)\")\n",
    "        print(f\"- Overlap: {overlap_ratio*100:.0f}% ({self.overlap_samples} samples)\")\n",
    "        print(f\"- Step size: {self.step_size} samples ({self.step_size/250:.3f}s)\")\n",
    "\n",
    "    def add_data(self, data):\n",
    "        \"\"\"Add new data and return True if enough new samples for next window\"\"\"\n",
    "        num_samples = data.shape[1]\n",
    "        \n",
    "        # Add data to buffer\n",
    "        if self.index + num_samples <= self.buffer_size:\n",
    "            self.buffer[:, self.index:self.index + num_samples] = data\n",
    "        else:\n",
    "            # Handle wrap-around\n",
    "            end_index = (self.index + num_samples) % self.buffer_size\n",
    "            self.buffer[:, self.index:] = data[:, :self.buffer_size - self.index]\n",
    "            self.buffer[:, :end_index] = data[:, self.buffer_size - self.index:]\n",
    "            \n",
    "        self.index = (self.index + num_samples) % self.buffer_size\n",
    "        \n",
    "        # Check if we have enough new samples since last window\n",
    "        samples_since_last = (self.index - self.last_window_end) % self.buffer_size\n",
    "        return samples_since_last >= self.step_size\n",
    "\n",
    "    def get_window(self):\n",
    "        \"\"\"Get the latest overlapping window of data\"\"\"\n",
    "        # Update last window position\n",
    "        self.last_window_end = self.index\n",
    "        \n",
    "        # Calculate start position for window\n",
    "        start_idx = (self.index - self.window_size) % self.buffer_size\n",
    "        \n",
    "        if start_idx < self.index:\n",
    "            return self.buffer[:, start_idx:self.index]\n",
    "        else:\n",
    "            return np.concatenate((\n",
    "                self.buffer[:, start_idx:],\n",
    "                self.buffer[:, :self.index]\n",
    "            ), axis=1)\n",
    "\n",
    "import keyboard\n",
    "\n",
    "class FeedbackHandler:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.current_features = None\n",
    "        \n",
    "        # Keyboard mapping\n",
    "        self.key_to_action = {\n",
    "            'left': 0,    # L\n",
    "            'right': 1,   # R\n",
    "            'up': 2,      # F\n",
    "            'down': 3     # B\n",
    "        }\n",
    "    \n",
    "    def check_feedback(self):\n",
    "        \"\"\"Check for keyboard feedback\"\"\"\n",
    "        for key, action in self.key_to_action.items():\n",
    "            if keyboard.is_pressed(key):\n",
    "                if self.current_features is not None:\n",
    "                    self.update_model(action)\n",
    "                    time.sleep(0.2)  # Debounce\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def update_model(self, correct_action):\n",
    "        \"\"\"Update model with correct action\"\"\"\n",
    "        if self.current_features is not None:\n",
    "            # Convert action to numpy array\n",
    "            action_array = np.array([correct_action])\n",
    "            \n",
    "            # Add experience to replay buffer\n",
    "            self.model.replay_buffer.add(\n",
    "                obs=self.current_features,\n",
    "                next_obs=self.current_features,  # Same state since this is immediate feedback\n",
    "                action=action_array,\n",
    "                reward=1.0,  # Positive reward for correct action\n",
    "                done=False,\n",
    "                infos={}\n",
    "            )\n",
    "            \n",
    "            # Train for one gradient step\n",
    "            if self.model.replay_buffer.size() > self.model.batch_size:\n",
    "                self.model.train(gradient_steps=1, batch_size=self.model.batch_size)\n",
    "            \n",
    "            print(f\"Model updated - Correct action was: {map_action_to_command(correct_action)}\")\n",
    "    \n",
    "    def set_current_features(self, features):\n",
    "        \"\"\"Store current features for potential update\"\"\"\n",
    "        self.current_features = features.copy()  # Make a copy to ensure we don't modify the original\n",
    "# In your main loop:\n",
    "feedback_handler = FeedbackHandler(model)\n",
    "\n",
    "\n",
    "\n",
    "class SimulationConnection:\n",
    "    def __init__(self, host='http://localhost:5000'):\n",
    "        self.host = host\n",
    "    \n",
    "    def send_command(self, command):\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(f\"{self.host}/command/{command}\")\n",
    "            return response.json()['status'] == 'ok'\n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "simulation = SimulationConnection()\n",
    "\n",
    "class ArduinoConnection:\n",
    "    def __init__(self, host, port):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.current_command = 'S'\n",
    "        self.STOP_DURATION = 0.5\n",
    "        \n",
    "    def send_command(self, new_command):\n",
    "        if new_command != self.current_command:\n",
    "            try:\n",
    "                # Send stop command\n",
    "                print(\"Stopping for 1 second...\")\n",
    "                self._send_http_command('S')\n",
    "                time.sleep(self.STOP_DURATION)\n",
    "                \n",
    "                # Send new command\n",
    "                self._send_http_command(new_command)\n",
    "                self.current_command = new_command\n",
    "                print(f\"Changed to new command: {new_command}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Connection error: {e}\")\n",
    "    \n",
    "    def _send_http_command(self, command):\n",
    "        import requests\n",
    "        try:\n",
    "            url = f\"http://{self.host}:{self.port}/{command}\"\n",
    "            requests.get(url, timeout=1)\n",
    "        except Exception as e:\n",
    "            print(f\"HTTP request error: {e}\")\n",
    "    \n",
    "    def close(self):\n",
    "        self._send_http_command('S')\n",
    "arduino = ArduinoConnection('192.168.1.168', 80)\n",
    "\n",
    "def map_action_to_command(action):\n",
    "    action_mapping = {\n",
    "        0: 'L',\n",
    "        1: 'R',\n",
    "        2: 'F',\n",
    "        3: 'B'\n",
    "    }\n",
    "    return action_mapping.get(action, 'S')\n",
    "\n",
    "# Initialize components\n",
    "num_channels = 6\n",
    "buffer_size = 1250 * 10  # 10 seconds buffer\n",
    "window_size = 1250       # 5 seconds window\n",
    "samples_per_read = 1250  # Read 5 seconds of data at a time\n",
    "\n",
    "# Initialize the buffer\n",
    "eeg_buffer = MultiChannelCircularBuffer(\n",
    "    num_channels=num_channels,\n",
    "    buffer_size=buffer_size,\n",
    "    window_size=window_size,\n",
    "    overlap_ratio=0.65\n",
    ")\n",
    "predictions = []\n",
    "# Set up BrainFlow\n",
    "params = BrainFlowInputParams()\n",
    "params.serial_port = 'COM9'  # Replace with your actual COM port\n",
    "board = BoardShim(BoardIds.CYTON_BOARD.value, params)\n",
    "board.prepare_session()\n",
    "print(\"BrainFlow session prepared.\")\n",
    "board.start_stream()\n",
    "print(\"Started data stream from board.\")\n",
    "\n",
    "def get_conservative_prediction(predictions, confidences, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    When confidences are close (within threshold), pick the lower confidence prediction\n",
    "    \"\"\"\n",
    "    if not confidences:  # Empty list check\n",
    "        return None, None\n",
    "    \n",
    "    # Get the highest confidence and its index\n",
    "    max_conf = max(confidences)\n",
    "    max_idx = confidences.index(max_conf)\n",
    "    \n",
    "    # Look for confidences that are close to the maximum\n",
    "    close_indices = [i for i, conf in enumerate(confidences) \n",
    "                    if (max_conf - conf) <= similarity_threshold]\n",
    "    \n",
    "    if len(close_indices) > 1:\n",
    "        # If we have close confidences, pick the lower one\n",
    "        min_conf_idx = min(close_indices, key=lambda i: confidences[i])\n",
    "        return predictions[min_conf_idx], confidences[min_conf_idx]\n",
    "    \n",
    "    # If no close confidences, return the highest one\n",
    "    return predictions[max_idx], confidences[max_idx]\n",
    "\n",
    "\n",
    "try:\n",
    "    # Initialize state variables\n",
    "    prediction_history = []\n",
    "    last_state = None\n",
    "    last_command = 'S'\n",
    "    last_command_time = time.time()\n",
    "    start_time = time.time()\n",
    "    model.exploration_rate = 2\n",
    "    model.exploration_final_eps = 2\n",
    "    while True:\n",
    "        # Get new data (5 second chunks)\n",
    "        data = board.get_current_board_data(samples_per_read)\n",
    "        eeg_channels = BoardShim.get_eeg_channels(BoardIds.CYTON_BOARD.value)\n",
    "        eeg_data = data[eeg_channels[:6], :]\n",
    "        \n",
    "\n",
    "        # Add data and check if we have enough for a window\n",
    "        if eeg_buffer.add_data(eeg_data):\n",
    "            # Get window and process\n",
    "            window_data = eeg_buffer.get_window().reshape(1, num_channels, -1)\n",
    "            print(f\"Processing window of shape: {window_data.shape}\")\n",
    "            \n",
    "            # Filter data\n",
    "            # data_downsampled = mne.filter.resample(window_data, up=2)  # Actually convert to 125Hz\n",
    "            # filtered_data = mne.filter.filter_data(data_downsampled, sfreq=250, l_freq=0.5, h_freq=45, verbose=False)\n",
    "            # Initialize CSP features list\n",
    "            csp_features_list = []\n",
    "            \n",
    "            # Apply CSP transform for each class\n",
    "            for class_id, csp in csp_filters.items():\n",
    "                transformed = csp.transform(window_data)\n",
    "                # if hasattr(raw, 'info'):  # Only plot if raw.info is available\n",
    "                #     csp.plot_patterns(info=raw.info, components=list(range(4)), ch_type='eeg')\n",
    "                csp_features_list.append(transformed)\n",
    "            \n",
    "            # Concatenate CSP features\n",
    "            csp_ftrs = np.concatenate(csp_features_list, axis=1)\n",
    "            print(f\"CSP Features Shape: {csp_ftrs.shape}\")\n",
    "            \n",
    "            # Extract features\n",
    "            ftrs = featuresarray_load(csp_ftrs)\n",
    "            print(f\"Extracted Features Shape: {ftrs.shape}\")\n",
    "            \n",
    "            predictions = []\n",
    "            confidences = []\n",
    "\n",
    "            for i in range(0, 20, 4):\n",
    "                ftrs_scaled = scaler.transform(ftrs.reshape(-1, ftrs.shape[-1])).reshape(ftrs.shape)\n",
    "                ftrs_scaled = ftrs_scaled[:, i:i+4, :]\n",
    "                feedback_handler.set_current_features(ftrs_scaled)\n",
    "                \n",
    "                # Get prediction\n",
    "                action, _ = model.predict(ftrs_scaled, deterministic=False)  # Changed this line\n",
    "                action = action[0]\n",
    "                # command = map_action_to_command(action)\n",
    "                # print(f\"Mapped Command: {command} \")\n",
    "                # simulation.send_command(command)\n",
    "                # Calculate confidence\n",
    "                q_values = model.q_net(model.q_net.obs_to_tensor(ftrs_scaled)[0])[0].detach().numpy()\n",
    "                q_values = (q_values - np.min(q_values)) / (np.max(q_values) - np.min(q_values) + 1e-6)\n",
    "                \n",
    "                max_q = np.max(q_values)\n",
    "                other_q_values = q_values[q_values != max_q]\n",
    "                confidence = float(max_q - np.mean(other_q_values)) if len(other_q_values) > 0 else 0\n",
    "                \n",
    "                # Only add high confidence predictions\n",
    "                if confidence > 0.6:  # You can adjust this threshold\n",
    "                    predictions.append(int(action))\n",
    "                    confidences.append(confidence)\n",
    "\n",
    "            # Get conservative prediction\n",
    "            action, confidence = get_conservative_prediction(predictions, confidences)\n",
    "            if action is not None:\n",
    "                command = map_action_to_command(action)\n",
    "                print(f\"Mapped Command: {command} {predictions}{confidences}\")\n",
    "            else:\n",
    "                command = 'S'  # Default to stop if no confident predictions\n",
    "            simulation.send_command(command)        \n",
    "            arduino.send_command(command)\n",
    "            \n",
    "        time.sleep(0.02)  # 20ms sleep\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopped by user.\")    \n",
    "finally:\n",
    "    simulation.close()\n",
    "    board.stop_stream()\n",
    "    board.release_session()\n",
    "    simulation.send_command('S')\n",
    "    arduino.send_command('S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.release_all_sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting car movement test...\n",
      "Moving Forward\n",
      "Stopping for 1 second...\n",
      "Changed to new command: F\n",
      "Turning Left\n",
      "Stopping for 1 second...\n",
      "Changed to new command: L\n",
      "Turning Right\n",
      "Stopping for 1 second...\n",
      "Changed to new command: R\n",
      "Moving Backward\n",
      "Stopping for 1 second...\n",
      "Changed to new command: B\n",
      "Stopping\n",
      "Stopping for 1 second...\n",
      "Changed to new command: S\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def test_car_movement(arduino):\n",
    "    try:\n",
    "        print(\"Starting car movement test...\")\n",
    "        \n",
    "        # Forward for 3 seconds\n",
    "        print(\"Moving Forward\")\n",
    "        arduino.send_command('F')\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Turn Left for 2 seconds\n",
    "        print(\"Turning Left\")\n",
    "        arduino.send_command('L')\n",
    "        time.sleep(3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Turn Right for 2 seconds\n",
    "        print(\"Turning Right\")\n",
    "        arduino.send_command('R')\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Backward for 3 seconds\n",
    "        print(\"Moving Backward\")\n",
    "        arduino.send_command('B')\n",
    "        time.sleep(3)\n",
    "    \n",
    "        # Stop\n",
    "        print(\"Stopping\")\n",
    "        arduino.send_command('S')\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTest stopped by user\")\n",
    "        arduino.send_command('S')\n",
    "    except Exception as e:\n",
    "        print(f\"Error during test: {e}\")\n",
    "        arduino.send_command('S')\n",
    "\n",
    "# Run the test\n",
    "arduino = ArduinoConnection('192.168.1.168', 80)\n",
    "test_car_movement(arduino)\n",
    "arduino.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_bdf(\"collected_data/session30.bdf\")\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_annotated_fif():\n",
    "    import mne\n",
    "\n",
    "    task_duration = 5\n",
    "    break_duration = 3\n",
    "    session_start_time = 30\n",
    "\n",
    "    # Updated task labels dictionary to include all types\n",
    "    task_labels = {\n",
    "        \"Relax\": \"relax\",\n",
    "        \"Right\": \"right_hand\",\n",
    "        \"Left\": \"left_hand\",\n",
    "        \"Blinking\": \"blinking\",\n",
    "        \"Jaw\": \"jaw_clenching\"\n",
    "    }\n",
    "\n",
    "    # Define the path to your BDF files and the annotations file\n",
    "    bdf_files = [\n",
    "        \"session30.bdf\"\n",
    "    ]\n",
    "    annotations_file = \"collected_data/annotations.txt\"\n",
    "\n",
    "    with open(annotations_file, \"r\") as f:\n",
    "        data = f.read().split(\"_____________________________________________\")\n",
    "    \n",
    "    # Find section30\n",
    "    session30_index = next(i for i, section in enumerate(data) if \"session30\" in section)\n",
    "    data = data[session30_index:]\n",
    "    \n",
    "    print(\"Total sessions available:\", len(data))\n",
    "    print(\"Data content:\", data)\n",
    "\n",
    "    # Updated sessions dictionary\n",
    "    sessions_per_file = {\n",
    "        \"session30.bdf\": 1  # Contains session30 with all task types\n",
    "    }\n",
    "\n",
    "    for i, bdf_file in enumerate(bdf_files):\n",
    "        raw = mne.io.read_raw_bdf(\"collected_data/\"+bdf_file, preload=True)\n",
    "        raw.drop_channels(['EEG 7', 'EEG 8', 'Accel X', 'Accel Y', 'Accel Z'])\n",
    "        \n",
    "        # Get tasks for this file\n",
    "        all_tasks = []\n",
    "        for j in range(sessions_per_file[bdf_file]):\n",
    "            if i + j < len(data):\n",
    "                session_tasks = data[i + j].strip().split(\"\\n\")[1:]\n",
    "                print(f\"Adding {len(session_tasks)} tasks from session30\")\n",
    "                all_tasks.extend(session_tasks)\n",
    "\n",
    "        print(f\"Total tasks for {bdf_file}: {len(all_tasks)}\")\n",
    "\n",
    "        onsets = []\n",
    "        durations = []\n",
    "        descriptions = []\n",
    "\n",
    "        current_time = session_start_time\n",
    "        for task_name in all_tasks:\n",
    "            # Map the task name to its label\n",
    "            if task_name in task_labels:\n",
    "                descriptions.append(task_labels[task_name])\n",
    "                onsets.append(current_time)\n",
    "                durations.append(task_duration)\n",
    "                current_time += task_duration + break_duration\n",
    "            else:\n",
    "                print(f\"Warning: Unknown task type '{task_name}'\")\n",
    "\n",
    "        annotations = mne.Annotations(onset=onsets, duration=durations, description=descriptions)\n",
    "        print(f\"Created {len(annotations)} annotations for {bdf_file}\")\n",
    "        raw.set_annotations(annotations)\n",
    "\n",
    "        annotated_file = bdf_file.replace(\".bdf\", \"_annotated.fif\")\n",
    "        raw.save(\"collected_data/\"+annotated_file, overwrite=True)\n",
    "        print(f\"Saved {annotated_file} with {len(raw.annotations)} annotations\\n\")\n",
    "\n",
    "def combine_fifs():\n",
    "    import mne\n",
    "    import os\n",
    "\n",
    "    data_folder = 'collected_data'\n",
    "    # Modified to only include session30\n",
    "    bdf_files = [os.path.join(data_folder, 'session30_annotated.fif')]\n",
    "    print(\"Files to combine:\", bdf_files)\n",
    "    \n",
    "    raw_combined = None\n",
    "\n",
    "    for bdf_file in bdf_files:\n",
    "        raw = mne.io.read_raw_fif(bdf_file, preload=True)\n",
    "        print(f\"Loaded {bdf_file} with {len(raw.annotations)} annotations\")\n",
    "        \n",
    "        if raw_combined is None:\n",
    "            raw_combined = raw\n",
    "        else:\n",
    "            raw_combined.append(raw)\n",
    "            print(f\"After append: {len(raw_combined.annotations)} total annotations\")\n",
    "\n",
    "    # Save to a new file name to avoid overwriting existing S02.fif\n",
    "    combined_bdf_path = os.path.join(\"formatted_data\", 'S03.fif')\n",
    "    raw_combined.save(combined_bdf_path, overwrite=True)\n",
    "    print(f\"\\nSaved {combined_bdf_path} with {len(raw_combined.annotations)} total annotations\")\n",
    "\n",
    "# Run both functions\n",
    "save_as_annotated_fif()\n",
    "combine_fifs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from mne.decoding import CSP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the combined .fif file\n",
    "raw = mne.io.read_raw_fif(\"formatted_data/S02.fif\", preload=True)\n",
    "\n",
    "# Rename channels\n",
    "mapping = {\n",
    "    'EEG 1': 'FCz',\n",
    "    'EEG 2': 'C3',\n",
    "    'EEG 3': 'Cz',\n",
    "    'EEG 4': 'CPz',\n",
    "    'EEG 5': 'C2',\n",
    "    'EEG 6': 'C4'\n",
    "}\n",
    "raw.rename_channels(mapping)\n",
    "\n",
    "# Set montage\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "raw.set_montage(montage)\n",
    "\n",
    "# Extract events and event IDs\n",
    "events, event_id = mne.events_from_annotations(raw)\n",
    "print('event_id', event_id)\n",
    "\n",
    "# Create epochs\n",
    "tmin, tmax = 0, 5\n",
    "epochs = mne.Epochs(\n",
    "    raw, events, event_id=event_id,\n",
    "    tmin=tmin, tmax=tmax, baseline=None, preload=True\n",
    ")\n",
    "\n",
    "# Get data and labels\n",
    "X = epochs.get_data()\n",
    "y = np.zeros(len(epochs.events))\n",
    "for i, event in enumerate(epochs.events[:, -1]):\n",
    "    if event == event_id['blinking']:        # 1 -> 2\n",
    "        y[i] = 2\n",
    "    elif event == event_id['jaw_clenching']: # 2 -> 3\n",
    "        y[i] = 3\n",
    "    elif event == event_id['left_hand']:     # 3 -> 0\n",
    "        y[i] = 0\n",
    "    elif event == event_id['relax']:         # 4 -> 4\n",
    "        y[i] = 4\n",
    "    elif event == event_id['right_hand']:    # 5 -> 1\n",
    "        y[i] = 1\n",
    "\n",
    "# Create CSP for each class (One-vs-Rest)\n",
    "class_names = ['Left Hand', 'Right Hand', 'Blinking', 'Jaw Clenching', 'Relax']\n",
    "n_components = 6  # Use all components since we have 6 channels\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for class_idx in range(5):\n",
    "    # Create binary labels for current class\n",
    "    binary_y = (y == class_idx).astype(int)\n",
    "    \n",
    "    # Fit CSP\n",
    "    csp = CSP(n_components=n_components, reg=None, log=True, norm_trace=False)\n",
    "    csp.fit(X, binary_y)\n",
    "    \n",
    "    # Plot CSP patterns\n",
    "    plt.subplot(2, 3, class_idx + 1)\n",
    "    mne.viz.plot_topomap(csp.patterns_[:, 0], epochs.info, axes=plt.gca(), \n",
    "                        show=False, contours=0)\n",
    "    plt.title(f'CSP Pattern for {class_names[class_idx]}')\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot CSP patterns with multiple components\n",
    "n_components_to_plot = 3  # Plot top 3 components for each class\n",
    "\n",
    "fig, axes = plt.subplots(5, n_components_to_plot, figsize=(15, 25))\n",
    "fig.suptitle('CSP Patterns: Top 3 Components for Each Class', fontsize=16, y=1.02)\n",
    "\n",
    "for class_idx in range(5):\n",
    "    # Create binary labels for current class\n",
    "    binary_y = (y == class_idx).astype(int)\n",
    "    \n",
    "    # Fit CSP\n",
    "    csp = CSP(n_components=n_components, reg=None, log=True, norm_trace=False)\n",
    "    csp.fit(X, binary_y)\n",
    "    \n",
    "    # Plot top components\n",
    "    for comp_idx in range(n_components_to_plot):\n",
    "        ax = axes[class_idx, comp_idx]\n",
    "        mne.viz.plot_topomap(csp.patterns_[:, comp_idx], epochs.info, axes=ax, \n",
    "                            show=False, contours=0)\n",
    "        if comp_idx == 0:\n",
    "            ax.set_title(f'{class_names[class_idx]}\\nComponent {comp_idx+1}')\n",
    "        else:\n",
    "            ax.set_title(f'Component {comp_idx+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print class counts\n",
    "print(\"\\nNumber of trials per class:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"{name}: {np.sum(y == i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    # Get total number of parameters\n",
    "    param_size = 0\n",
    "    buffer_size = 0\n",
    "    \n",
    "    # For each parameter, calculate size in bytes\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    # Buffer size (for gradients, etc.)\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    # Convert to MB\n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    \n",
    "    return {\n",
    "        'parameters_size_mb': param_size / 1024**2,\n",
    "        'buffer_size_mb': buffer_size / 1024**2,\n",
    "        'total_size_mb': size_mb\n",
    "    }\n",
    "\n",
    "# For your model:\n",
    "model_size = get_model_size(model.q_net)\n",
    "print(f\"Model Memory Usage:\")\n",
    "print(f\"Parameters: {model_size['parameters_size_mb']:.2f} MB\")\n",
    "print(f\"Buffers: {model_size['buffer_size_mb']:.2f} MB\")\n",
    "print(f\"Total: {model_size['total_size_mb']:.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
